<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>&#8474;&#8477; 6 PCA - Análise de Componentes Principais | Caderno de Quimiometria com R</title>
  <meta name="description" content="Este caderno foi criado para ser um guia acessível para todos que desejam explorar e aplicar conceitos de quimiometria utilizando a linguagem de programação R. Foi desenvolvido com suporte do ChatGPT." />
  <meta name="generator" content="bookdown 0.41 and GitBook 2.6.7" />

  <meta property="og:title" content="&#8474;&#8477; 6 PCA - Análise de Componentes Principais | Caderno de Quimiometria com R" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://cleber-n-borges.github.io/quimiometriaR/img/cover.jpg" />
  <meta property="og:description" content="Este caderno foi criado para ser um guia acessível para todos que desejam explorar e aplicar conceitos de quimiometria utilizando a linguagem de programação R. Foi desenvolvido com suporte do ChatGPT." />
  <meta name="github-repo" content="cleber-n-borges/quimiometriaR" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="&#8474;&#8477; 6 PCA - Análise de Componentes Principais | Caderno de Quimiometria com R" />
  
  <meta name="twitter:description" content="Este caderno foi criado para ser um guia acessível para todos que desejam explorar e aplicar conceitos de quimiometria utilizando a linguagem de programação R. Foi desenvolvido com suporte do ChatGPT." />
  <meta name="twitter:image" content="https://cleber-n-borges.github.io/quimiometriaR/img/cover.jpg" />




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="favicon.svg" type="image/x-icon" />
<link rel="prev" href="parte-iii---análise-exploratória-de-dados.html"/>
<link rel="next" href="hca---análise-de-agrupamento-hierárquico.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.33/datatables.js"></script>
<link href="libs/dt-core-1.13.6/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.13.6/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.13.6/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.1/js/crosstalk.min.js"></script>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-YJVXYW1C58"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-YJVXYW1C58');
</script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">&#8474;&#8477; Quimiometria com R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Bem-vindos!</a></li>
<li class="chapter" data-level="" data-path="útil-para-você.html"><a href="útil-para-você.html"><i class="fa fa-check"></i>Útil para Você!</a></li>
<li class="chapter" data-level="" data-path="pergunte-ao-chatgpt.html"><a href="pergunte-ao-chatgpt.html"><i class="fa fa-check"></i>Pergunte ao ChatGPT</a></li>
<li class="chapter" data-level="" data-path="estrutura-do-caderno.html"><a href="estrutura-do-caderno.html"><i class="fa fa-check"></i>Estrutura do Caderno</a></li>
<li class="chapter" data-level="" data-path="parte-i---ponto-de-partida.html"><a href="parte-i---ponto-de-partida.html"><i class="fa fa-check"></i>PARTE I - Ponto de Partida</a>
<ul>
<li class="chapter" data-level="" data-path="parte-i---ponto-de-partida.html"><a href="parte-i---ponto-de-partida.html#primeiro-passo"><i class="fa fa-check"></i>Primeiro Passo</a></li>
<li class="chapter" data-level="" data-path="parte-i---ponto-de-partida.html"><a href="parte-i---ponto-de-partida.html#o-que-é-quimiometria"><i class="fa fa-check"></i>O que é Quimiometria?</a></li>
<li class="chapter" data-level="" data-path="parte-i---ponto-de-partida.html"><a href="parte-i---ponto-de-partida.html#aplicações-do-r-na-quimiometria"><i class="fa fa-check"></i>Aplicações do R na Quimiometria</a></li>
<li class="chapter" data-level="" data-path="parte-i---ponto-de-partida.html"><a href="parte-i---ponto-de-partida.html#contribuições-do-chatgpt"><i class="fa fa-check"></i>Contribuições do ChatGPT</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="comece-a-usar-o-r.html"><a href="comece-a-usar-o-r.html"><i class="fa fa-check"></i><b>1</b> Comece a Usar o R</a>
<ul>
<li class="chapter" data-level="1.1" data-path="comece-a-usar-o-r.html"><a href="comece-a-usar-o-r.html#instale-o-r-e-rstudio"><i class="fa fa-check"></i><b>1.1</b> Instale o R e RStudio</a></li>
<li class="chapter" data-level="1.2" data-path="comece-a-usar-o-r.html"><a href="comece-a-usar-o-r.html#conheça-o-básico-do-r"><i class="fa fa-check"></i><b>1.2</b> Conheça o Básico do R</a></li>
<li class="chapter" data-level="1.3" data-path="comece-a-usar-o-r.html"><a href="comece-a-usar-o-r.html#realize-análises-simples"><i class="fa fa-check"></i><b>1.3</b> Realize Análises Simples</a></li>
<li class="chapter" data-level="1.4" data-path="comece-a-usar-o-r.html"><a href="comece-a-usar-o-r.html#a-conveniência-do-help"><i class="fa fa-check"></i><b>1.4</b> A Conveniência do <code>help()</code></a>
<ul>
<li class="chapter" data-level="" data-path="comece-a-usar-o-r.html"><a href="comece-a-usar-o-r.html#como-usar-o-comando-help"><i class="fa fa-check"></i>Como Usar o Comando <code>help()</code>:</a></li>
<li class="chapter" data-level="" data-path="comece-a-usar-o-r.html"><a href="comece-a-usar-o-r.html#conveniência-da-prática"><i class="fa fa-check"></i>Conveniência da Prática</a></li>
<li class="chapter" data-level="" data-path="comece-a-usar-o-r.html"><a href="comece-a-usar-o-r.html#exemplo-de-uso"><i class="fa fa-check"></i>Exemplo de Uso</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="comece-a-usar-o-r.html"><a href="comece-a-usar-o-r.html#manuseie-arquivos"><i class="fa fa-check"></i><b>1.5</b> Manuseie Arquivos</a></li>
<li class="chapter" data-level="1.6" data-path="comece-a-usar-o-r.html"><a href="comece-a-usar-o-r.html#salve-e-restaure-seu-workspace"><i class="fa fa-check"></i><b>1.6</b> Salve e Restaure seu <i>Workspace</i></a></li>
<li class="chapter" data-level="1.7" data-path="comece-a-usar-o-r.html"><a href="comece-a-usar-o-r.html#instale-pacotes-r"><i class="fa fa-check"></i><b>1.7</b> Instale Pacotes R</a></li>
<li class="chapter" data-level="1.8" data-path="comece-a-usar-o-r.html"><a href="comece-a-usar-o-r.html#aprenda-mais-e-pratique"><i class="fa fa-check"></i><b>1.8</b> Aprenda Mais e Pratique</a></li>
<li class="chapter" data-level="1.9" data-path="comece-a-usar-o-r.html"><a href="comece-a-usar-o-r.html#participar-de-comunidades"><i class="fa fa-check"></i><b>1.9</b> Participar de Comunidades</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="conjunto-de-dados.html"><a href="conjunto-de-dados.html"><i class="fa fa-check"></i><b>2</b> Conjunto de Dados</a>
<ul>
<li class="chapter" data-level="2.1" data-path="conjunto-de-dados.html"><a href="conjunto-de-dados.html#organização-e-notação"><i class="fa fa-check"></i><b>2.1</b> Organização e Notação</a>
<ul>
<li class="chapter" data-level="" data-path="conjunto-de-dados.html"><a href="conjunto-de-dados.html#exemplificando"><i class="fa fa-check"></i>Exemplificando…</a></li>
<li class="chapter" data-level="" data-path="conjunto-de-dados.html"><a href="conjunto-de-dados.html#aproveitando-para-explicar-data.frame-e-matrix-no-r"><i class="fa fa-check"></i>Aproveitando para Explicar <code>data.frame</code> e <code>matrix</code> no R!</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="conjunto-de-dados.html"><a href="conjunto-de-dados.html#valores-faltantes-na"><i class="fa fa-check"></i><b>2.2</b> Valores faltantes: <code>NA</code></a>
<ul>
<li class="chapter" data-level="" data-path="conjunto-de-dados.html"><a href="conjunto-de-dados.html#imputação"><i class="fa fa-check"></i>Imputação</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="conjunto-de-dados.html"><a href="conjunto-de-dados.html#dados-disponíveis"><i class="fa fa-check"></i><b>2.3</b> Dados Disponíveis</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="visualização-de-dados.html"><a href="visualização-de-dados.html"><i class="fa fa-check"></i><b>3</b> Visualização de Dados</a>
<ul>
<li class="chapter" data-level="3.1" data-path="visualização-de-dados.html"><a href="visualização-de-dados.html#gráficos-com-o-r"><i class="fa fa-check"></i><b>3.1</b> Gráficos com o R</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="visualização-de-dados.html"><a href="visualização-de-dados.html#funções-nativas"><i class="fa fa-check"></i><b>3.1.1</b> Funções nativas</a></li>
<li class="chapter" data-level="3.1.2" data-path="visualização-de-dados.html"><a href="visualização-de-dados.html#pacote-animation"><i class="fa fa-check"></i><b>3.1.2</b> Pacote <code>animation</code></a></li>
<li class="chapter" data-level="3.1.3" data-path="visualização-de-dados.html"><a href="visualização-de-dados.html#pacote-ggplot2"><i class="fa fa-check"></i><b>3.1.3</b> Pacote <code>ggplot2</code></a></li>
<li class="chapter" data-level="3.1.4" data-path="visualização-de-dados.html"><a href="visualização-de-dados.html#pacote-plotly"><i class="fa fa-check"></i><b>3.1.4</b> Pacote <code>plotly</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="parte-ii---tratamento-dos-dados.html"><a href="parte-ii---tratamento-dos-dados.html"><i class="fa fa-check"></i>PARTE II - Tratamento dos Dados</a></li>
<li class="chapter" data-level="4" data-path="pré-processamento-de-variáveis.html"><a href="pré-processamento-de-variáveis.html"><i class="fa fa-check"></i><b>4</b> Pré-Processamento de Variáveis</a>
<ul>
<li class="chapter" data-level="4.1" data-path="pré-processamento-de-variáveis.html"><a href="pré-processamento-de-variáveis.html#autoescalamento"><i class="fa fa-check"></i><b>4.1</b> Autoescalamento</a></li>
<li class="chapter" data-level="4.2" data-path="pré-processamento-de-variáveis.html"><a href="pré-processamento-de-variáveis.html#normalização-min-max"><i class="fa fa-check"></i><b>4.2</b> Normalização Min-Max:</a></li>
<li class="chapter" data-level="4.3" data-path="pré-processamento-de-variáveis.html"><a href="pré-processamento-de-variáveis.html#transformações"><i class="fa fa-check"></i><b>4.3</b> Transformações</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="pré-processamento-de-variáveis.html"><a href="pré-processamento-de-variáveis.html#logarítmica-e-raiz-quadrada"><i class="fa fa-check"></i><b>4.3.1</b> Logarítmica e Raiz Quadrada</a></li>
<li class="chapter" data-level="4.3.2" data-path="pré-processamento-de-variáveis.html"><a href="pré-processamento-de-variáveis.html#transformação-box-cox"><i class="fa fa-check"></i><b>4.3.2</b> Transformação Box-Cox</a></li>
<li class="chapter" data-level="4.3.3" data-path="pré-processamento-de-variáveis.html"><a href="pré-processamento-de-variáveis.html#logística-e-logit"><i class="fa fa-check"></i><b>4.3.3</b> Logística e <em>Logit</em></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="pré-processamento-de-amostras.html"><a href="pré-processamento-de-amostras.html"><i class="fa fa-check"></i><b>5</b> Pré-Processamento de Amostras</a>
<ul>
<li class="chapter" data-level="5.1" data-path="pré-processamento-de-amostras.html"><a href="pré-processamento-de-amostras.html#todo"><i class="fa fa-check"></i><b>5.1</b> TODO</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="parte-iii---análise-exploratória-de-dados.html"><a href="parte-iii---análise-exploratória-de-dados.html"><i class="fa fa-check"></i>PARTE III - Análise Exploratória de Dados</a></li>
<li class="chapter" data-level="6" data-path="pca---análise-de-componentes-principais.html"><a href="pca---análise-de-componentes-principais.html"><i class="fa fa-check"></i><b>6</b> PCA - Análise de Componentes Principais</a>
<ul>
<li class="chapter" data-level="6.1" data-path="pca---análise-de-componentes-principais.html"><a href="pca---análise-de-componentes-principais.html#desenvolvimento-histórico"><i class="fa fa-check"></i><b>6.1</b> Desenvolvimento histórico</a></li>
<li class="chapter" data-level="6.2" data-path="pca---análise-de-componentes-principais.html"><a href="pca---análise-de-componentes-principais.html#definição"><i class="fa fa-check"></i><b>6.2</b> Definição</a></li>
<li class="chapter" data-level="6.3" data-path="pca---análise-de-componentes-principais.html"><a href="pca---análise-de-componentes-principais.html#interpretação-geométrica"><i class="fa fa-check"></i><b>6.3</b> Interpretação Geométrica</a></li>
<li class="chapter" data-level="6.4" data-path="pca---análise-de-componentes-principais.html"><a href="pca---análise-de-componentes-principais.html#objetivo"><i class="fa fa-check"></i><b>6.4</b> Objetivo</a></li>
<li class="chapter" data-level="6.5" data-path="pca---análise-de-componentes-principais.html"><a href="pca---análise-de-componentes-principais.html#pca-como-decomposição-matricial"><i class="fa fa-check"></i><b>6.5</b> PCA como Decomposição Matricial</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="pca---análise-de-componentes-principais.html"><a href="pca---análise-de-componentes-principais.html#principais-algoritmos"><i class="fa fa-check"></i><b>6.5.1</b> Principais Algoritmos</a>
<ul>
<li class="chapter" data-level="6.5.1.1" data-path="pca---análise-de-componentes-principais.html"><a href="pca---análise-de-componentes-principais.html#decomposição-espectral"><i class="fa fa-check"></i><b>6.5.1.1</b> Decomposição Espectral</a></li>
<li class="chapter" data-level="6.5.1.2" data-path="pca---análise-de-componentes-principais.html"><a href="pca---análise-de-componentes-principais.html#decomposição-em-valores-singulares-svd"><i class="fa fa-check"></i><b>6.5.1.2</b> Decomposição em Valores Singulares (SVD)</a></li>
<li class="chapter" data-level="6.5.1.3" data-path="pca---análise-de-componentes-principais.html"><a href="pca---análise-de-componentes-principais.html#algoritmo-nipals"><i class="fa fa-check"></i><b>6.5.1.3</b> Algoritmo NIPALS</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="pca---análise-de-componentes-principais.html"><a href="pca---análise-de-componentes-principais.html#variância-por-pc"><i class="fa fa-check"></i><b>6.6</b> Variância por PC</a></li>
<li class="chapter" data-level="6.7" data-path="pca---análise-de-componentes-principais.html"><a href="pca---análise-de-componentes-principais.html#aplicação-típica"><i class="fa fa-check"></i><b>6.7</b> Aplicação Típica</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="hca---análise-de-agrupamento-hierárquico.html"><a href="hca---análise-de-agrupamento-hierárquico.html"><i class="fa fa-check"></i><b>7</b> HCA - Análise de Agrupamento Hierárquico</a></li>
<li class="divider"></li>
<li><a href="https://github.com/cleber-n-borges/quimiometriaR" target="blank">&#8474;&#8477; Repositório GitHub</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Caderno de Quimiometria com R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="pca---análise-de-componentes-principais" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">&#8474;&#8477; 6</span> PCA - Análise de Componentes Principais<a href="pca---análise-de-componentes-principais.html#pca---análise-de-componentes-principais" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>A <strong>Análise de Componentes Principais (PCA)</strong><a href="#fn46" class="footnote-ref" id="fnref46"><sup>46</sup></a> é uma técnica estatística multivariada amplamente utilizada para reduzir a dimensionalidade de conjuntos de dados, preservando a maior quantidade possível de variabilidade presente nos dados originais. Essa abordagem transforma um grande número de variáveis correlacionadas em um conjunto menor de variáveis não correlacionadas, chamadas de componentes principais. Os componentes principais são combinações lineares das variáveis originais e são ordenados de forma que o primeiro componente captura a maior parte da variabilidade dos dados, seguido pelo segundo, e assim por diante.</p>
<div id="desenvolvimento-histórico" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Desenvolvimento histórico<a href="pca---análise-de-componentes-principais.html#desenvolvimento-histórico" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A Análise de Componentes Principais tem seu histórico que remonta ao início do século XX. Aqui estão os principais marcos na evolução dessa técnica:</p>
<ol style="list-style-type: decimal">
<li><strong>Princípios Iniciais</strong>:
<ul>
<li>Os princípios da PCA podem ser traçados até o trabalho de <strong>Karl Pearson</strong><a href="#fn47" class="footnote-ref" id="fnref47"><sup>47</sup></a>, um estatístico britânico. Em 1901, ele publicou um artigo intitulado “On Lines and Planes of Closest Fit to Systems of Points in Space”<a href="#fn48" class="footnote-ref" id="fnref48"><sup>48</sup></a>, onde apresentou o conceito de “análise de fatores” e a ideia de encontrar uma linha de melhor ajuste para um conjunto de pontos em um espaço multidimensional. Este trabalho é considerado uma das primeiras referências à ideia subjacente da PCA.</li>
</ul></li>
</ol>
<ol start="2" style="list-style-type: decimal">
<li><strong>Desenvolvimentos Posteriores</strong>:
<ul>
<li>Em 1933, <strong>Harold Hotelling</strong><a href="#fn49" class="footnote-ref" id="fnref49"><sup>49</sup></a>, um estatístico americano, expandiu as ideias de Pearson e formalizou a técnica da PCA em seu trabalho <em>“Analysis of a Complex of Statistical Variables into Principal Components”</em><a href="#fn50" class="footnote-ref" id="fnref50"><sup>50</sup></a>. Neste artigo, Hotelling descreveu um método para reduzir a dimensionalidade de dados multivariados, mantendo a maior parte da variabilidade nos dados originais. Ele introduziu a terminologia e as fórmulas que são fundamentais para a PCA moderna.</li>
</ul></li>
</ol>
<ol start="3" style="list-style-type: decimal">
<li><strong>Adoção e Popularização</strong>:
<ul>
<li>Ao longo do século XX, a PCA começou a ser adotada em várias disciplinas, incluindo psicologia, biologia, e, mais tarde, em áreas como ciências sociais e, mais recentemente, em ciências de dados e quimiometria. O conceito foi utilizado para análises de dados complexos, onde a visualização e a interpretação de múltiplas variáveis se tornaram essenciais.</li>
</ul></li>
<li><strong>Avanços Computacionais</strong>:
<ul>
<li>Com o advento dos computadores e o aumento do poder computacional nas décadas de 1970 e 1980, a PCA tornou-se ainda mais acessível e prática. Softwares estatísticos começaram a incorporar a PCA como uma ferramenta padrão para análise de dados, permitindo que pesquisadores e profissionais realizassem análises multivariadas com maior facilidade.</li>
</ul></li>
<li><strong>Aplicações Modernas</strong>:
<ul>
<li>Nos anos 2000 e além, a PCA passou a ser uma técnica fundamental na análise de grandes volumes de dados, especialmente na era do <em>Big Data</em>. Sua aplicação se estendeu para áreas como aprendizado de máquina <em>etc</em>, onde a redução de dimensionalidade e a identificação de padrões são cruciais.</li>
</ul></li>
</ol>
<p>Em síntese, o método PCA evoluiu de um conceito inicial apresentado por Pearson para uma técnica robusta e amplamente utilizada, graças ao trabalho de Hotelling e ao avanço da computacão. Sua relevância na análise de dados contemporâneos atesta sua eficácia na extração de informações significativas, contexto do qual a quimiometria está inserida.</p>
</div>
<div id="definição" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Definição<a href="pca---análise-de-componentes-principais.html#definição" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A explicação apresentada anteriormente pode ainda não ser satisfatória e suficiente para um entendimento completo do PCA, especialmente para aqueles que estão iniciando seus estudos. Assim, vamos retomar e destacar alguns pontos-chave de forma mais direta para esclarecer o conceito de maneira mais eficaz. Vamos analisar cada ponto com atenção e com melhores definições:</p>
<p> </p>
<ol style="list-style-type: decimal">
<li>O que são essas <strong>Componentes Principais</strong>?</li>
</ol>
<ul>
<li>São <strong>novas variáveis</strong> produzidas a partir das variáveis originais</li>
</ul>
<p> </p>
<ol start="2" style="list-style-type: decimal">
<li>Como são criadas essas <strong>novas variáveis</strong> chamadas de Componentes Principais?</li>
</ol>
<ul>
<li>São <strong>combinações lineares</strong> das variáveis originais.</li>
</ul>
<p> </p>
<p>Considerando que as combinações lineares desempenham um papel crucial para a definição do PCA, vamos explorar esse aspecto em mais detalhes e compreender como elas geram as componentes principais.</p>
<p><strong>Combinações Lineares</strong></p>
<p>Considere que temos nossos dados numa matrix <span class="math inline">\(\mathbf{X}\)</span> com <span class="math inline">\(n\)</span> amostras (linhas) e <span class="math inline">\(m\)</span> variáveis (colunas). Podemos combinar linearmente as <span class="math inline">\(m\)</span> variáveis para formar <span class="math inline">\(k\)</span> componentes principais da seguinte forma:</p>
<p><span class="math display">\[
\mathbf{t}_1 = p_{1,1} \cdot \mathbf{x}_{\cdot,1} + p_{1,2} \cdot \mathbf{x}_{\cdot,2} + p_{1,3} \cdot \mathbf{x}_{\cdot,3} + \ldots + p_{1,m} \cdot \mathbf{x}_{\cdot, m} \\
\\
\mathbf{t}_2 = p_{2,1} \cdot \mathbf{x}_{\cdot,1} + p_{2,2} \cdot \mathbf{x}_{\cdot,2} + p_{2,3} \cdot \mathbf{x}_{\cdot,3} + \ldots + p_{2,m} \cdot \mathbf{x}_{\cdot, m} \\
\\
\mathbf{t}_3 = p_{3,1} \cdot \mathbf{x}_{\cdot,1} + p_{3,2} \cdot \mathbf{x}_{\cdot,2} + p_{3,3} \cdot \mathbf{x}_{\cdot,3} + \ldots + p_{3,m} \cdot \mathbf{x}_{\cdot, m} \\
\\
\vdots \\
\\
\mathbf{t}_k = p_{k,1} \cdot \mathbf{x}_{\cdot,1} + p_{k,2} \cdot \mathbf{x}_{\cdot,2} + p_{k,3} \cdot \mathbf{x}_{\cdot,3} + \ldots + p_{k,m} \cdot \mathbf{x}_{\cdot, m}
\]</span></p>
<p>       onde:</p>
<ul>
<ul>
<li>
<p><span class="math inline">\(\mathbf{t}_1\)</span>, <span class="math inline">\(\mathbf{t}_2\)</span>, <span class="math inline">\(\mathbf{t}_3\)</span>, …, <span class="math inline">\(\mathbf{t}_k\)</span> são vetores. São chamados de <em>scores</em>.</p>
<li>
<p><span class="math inline">\(p_{1,1}\)</span>, <span class="math inline">\(p_{1,2}\)</span>, <span class="math inline">\(p_{1,3}\)</span>, …, <span class="math inline">\(p_{k,m}\)</span> são valores escalares e são chamados de pesos para cada variável. São chamados de <em>loadings</em>.</p>
<li>
<p><span class="math inline">\(\mathbf{x}_{\cdot,1}\)</span>, <span class="math inline">\(\mathbf{x}_{\cdot,2}\)</span>, <span class="math inline">\(\mathbf{x}_{\cdot,3}\)</span>, …, <span class="math inline">\(\mathbf{x}_{\cdot,m}\)</span> são as colunas da matriz <span class="math inline">\(\mathbf{X}\)</span>.</p>
<li>
<p><span class="math inline">\(k\)</span> pode variar de 1 até <span class="math inline">\(w=min(n,m)\)</span> (o menor valor entre <span class="math inline">\(n\)</span> e <span class="math inline">\(m\)</span>).</p>
</ul>
</ul>
<p>Uma notação mais compacta para as equações pode ser expressa como:</p>
<p><span class="math display">\[
\mathbf{t}_k = \sum_{k=1}^{w} \sum_{j=1}^{m} p_{k,j} \cdot \mathbf{x}_{\cdot,j}
\]</span></p>
<p>Essa notação resume e deixa explicíto as combinações lineares que resultam em cada vetor <span class="math inline">\(\mathbf{t}_k\)</span> em termos dos coeficientes <span class="math inline">\(p_{k,j}\)</span> e dos vetores <span class="math inline">\(\mathbf{x}_{\cdot,j}\)</span>.</p>
<p> </p>
<p>É fundamental destacar que as combinações lineares apresentadas até agora permitem infinitas possibilidades, pois ainda não impusemos restrições quanto à variabilidade dos dados que elas devem descrever. Para que as combinações lineares se tornem as componentes principais, como mencionamos no início, é essencial e necessário definir um critério que as caracterize. Esse critério se refere à forma como será a representação da variabilidade dos dados. O próximo ponto abordará essa condição específica.</p>
<p> </p>
<ol start="3" style="list-style-type: decimal">
<li>Então como são feitas essas <strong>combinações lineares</strong> considerando a descrição da variabilidade dos dados?</li>
</ol>
<ul>
<li><p>A primeira componente principal captura, obrigatoriamente, a maior parte de informação possível contida dos dados.</p></li>
<li><p>Cada outra nova componente principal, ao ser definida, deve ser obrigatoriamente ortogonal às anteriores e capturar a maior variação possível dos dados que ainda não foi explicada.</p></li>
</ul>
<p> </p>
<p>Por ortogonalidade entenda-se que não há correlação entre eles, ou seja, cada componente principal representa uma direção distinta na variabilidade dos dados. Em termos estritamente matemáticos, duas componentes principais (vetores, direções <em>etc</em>) são ortogonais se o produto interno entre elas é zero.</p>
<p>No sentido sobre capturar informações dos dados, diz-se que as componentes são independentes entre si. Essa independência é crucial no PCA, pois permite que cada componente capture informações únicas sobre a variação nos dados, sem redundância. Em outras palavras, a primeira componente principal explica a maior parte da variância dos dados, a segunda componente, que é ortogonal à primeira, explica a maior parte da variância restante (que necessariamente não foi capturada na primeira), e assim por diante.</p>
<p>A intenção dos três pontos abordados é esclarecer as condições necessárias para a definição completa de PCA. Compreendendo esses aspectos, teremos uma base sólida sobre o que é PCA, em termos formais. No entanto, é igualmente importante explorar mais dois outros pontos, que são, a interpretação geométrica do PCA e os objetivos para os quais o PCA é utilizado.</p>
</div>
<div id="interpretação-geométrica" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> Interpretação Geométrica<a href="pca---análise-de-componentes-principais.html#interpretação-geométrica" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A explicação anterior tratou do PCA sob uma perspectiva algébrica. No entanto, também podemos compreendê-lo geometricamente, destacando que a transformação resultante da combinação linear pode ser vista como uma rotação dos eixos originais. Essa rotação é realizada de forma a maximizar a captura da variabilidade dos dados, mantendo a ortogonalidade das novas direções, conforme já definido. No caso de duas variáveis, essa rotação se torna facilmente visualizável e imediatamente reconhecível, permitindo uma compreensão intuitiva do processo.</p>
<p>Para facilitar a ilustração e a visualização, consideraremos uma matriz <span class="math inline">\(\mathbf{X}_{100,2}\)</span>​ composta por duas variáveis, <span class="math inline">\(\mathbf{x}_1\)</span>​ e <span class="math inline">\(\mathbf{x}_{2}\)</span>​, contendo 100 linhas de valores aleatórios. O processo de transformar as variáveis originais em duas novas componentes principais pode ser ilustrado no gráfico a seguir:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pca-rot"></span>
<img src="img/parte03/pca_interpreta_Geo_Rot.png" alt="PCA como rotação dos dados" width="95%" />
<p class="caption">
Figure 6.1: PCA como rotação dos dados
</p>
</div>
<details>
<summary>
😈 + Detalhes
</summary>
<p><br><strong>Suplemento: Mais detalhes sobre a matriz X[100,2]</strong><br><br></p>
<p><em>Outpu</em> do R para:</p>
<ol style="list-style-type: decimal">
<li>Correlação de <span class="math inline">\(\mathbf{X}\)</span></li>
<li>Dados originais</li>
<li>Resumo sobre PCA, de <span class="math inline">\(\mathbf{X}\)</span></li>
</ol>
<pre><code># correlação
&gt; cov( X )
          x1        x2
x1 1.0000000 0.8303746
x2 0.8303746 1.0000000
&gt;
# Dados Originais
&gt; round(as.numeric( X[,1]),3)
  [1]  0.305  0.416 -0.318 -0.627  0.324 -1.851  0.345 -0.468 -0.475 -1.045
 [11] -0.324  1.410  0.113  0.247 -0.893  0.513 -1.015 -0.517  0.785  0.048
 [21]  0.479  1.086 -0.798 -1.613 -1.653  1.399 -0.652  0.337  0.329 -0.366
 [31]  0.508  1.751  1.618  1.244  0.008  0.221 -0.511 -1.711  1.366 -0.197
 [41]  0.792 -2.355 -1.171  0.621  0.547  1.090 -1.488  0.289  0.303 -1.392
 [51] -0.705  1.527 -0.172  0.096 -0.822  0.029  0.400  0.519  1.704 -2.325
 [61] -0.086 -1.424  0.276  1.206 -0.746 -1.864  0.577  1.210  0.244 -1.382
 [71] -0.171 -0.924 -1.161  1.871  1.038  0.626  0.521 -0.948  0.207  0.696
 [81]  0.359  0.716 -0.493  2.002  0.651  1.455  0.383 -0.496  0.261  0.520
 [91] -1.085 -0.987  1.473 -0.571 -1.100  0.397 -0.673  1.716 -0.758 -0.843
&gt;
&gt; round(as.numeric( X[,2]),3)
  [1]  0.335 -0.344 -0.037 -1.243  0.303 -1.751  0.069  0.476 -0.640 -0.654
 [11] -0.957  0.330  0.761  0.896 -0.759  0.091 -0.838 -0.076  1.789 -0.026
 [21]  1.201  1.545 -0.463 -0.422 -1.535  0.413 -1.551  0.876 -0.947 -0.970
 [31]  1.277  1.816  1.515  0.279  1.453 -0.435 -0.980 -0.975  1.101 -0.411
 [41] -0.045 -2.560 -1.259 -0.377  0.406  1.116 -2.031  0.669  0.705 -1.023
 [51] -1.466  1.458 -0.050 -0.398 -0.720 -0.141  1.225  0.185  1.328 -2.181
 [61] -0.361 -0.957  0.054  0.639 -1.151 -1.619  0.771  0.222  1.141 -1.105
 [71] -0.046  0.008 -0.902  1.876 -0.167  0.724  1.753 -0.535  0.254 -0.494
 [81] -0.253  0.500 -0.630  2.147  0.930  1.141  1.076 -0.201  0.082  0.098
 [91] -0.561 -1.067  1.548 -0.299 -0.148  0.723 -0.073  1.054 -0.858 -0.668
&gt;
# PCA
&gt; summary( prcomp( X ) )
Importance of components:
                          PC1     PC2
Standard deviation     1.3529 0.41186
Proportion of Variance 0.9152 0.08481
Cumulative Proportion  0.9152 1.00000
&gt;</code></pre>
<p> </p>
<p><strong>Gráfico interativo</strong></p>
<center>
<iframe src="img/parte03/plotly/pca_rot.html" width="80%" height="400px" data-external="1">
</iframe>
</center>
<center>
△
</center>
</details>
<p> </p>
<p>A figura <a href="pca---análise-de-componentes-principais.html#fig:pca-rot">6.1</a> mostra 4 gráficos, com o primeiro mostrando apenas a dispersão dos pontos nas variáveis originais. No segundo, a reta em azul representa a direção da primeira componente principal capturando a maior variabilidade dos dados. Ou seja, está na direção onde há a maior dispersão da informação contida nos dados. A reta vermelha no terceiro gráfico indica a segunda componente principal, que é perpendicular à primeira e por sua vez é a direção que captura a maior parte da dispersão que a primeira componente não consegue representar. O quarto gráfico usa as duas primeiras componentes principais como sistema de coordenadas e assim é possível visualizar e entender que essa transformação consistiu na rotação do sistema dos eixos originais.</p>
<p>É importante destacar que a relação entre os pontos permanece inalterada, independentemente do sistema de eixos utilizado como mostrado pela figura. Em outras palavras, a distância relativa entre os pontos não muda com a rotação dos eixos. Essa propriedade é fundamental no contexto do PCA, pois significa que, ao aplicar a transformação para encontrar as componentes principais, a estrutura dos dados em termos de proximidade e dispersão é preservada. Assim, mesmo após a rotação, as relações espaciais entre os pontos continuam a refletir a mesma informação sobre a variabilidade dos dados, permitindo que a análise se concentre nas direções de maior variância sem distorcer a interpretação dos dados originais.</p>
</div>
<div id="objetivo" class="section level2 hasAnchor" number="6.4">
<h2><span class="header-section-number">6.4</span> Objetivo<a href="pca---análise-de-componentes-principais.html#objetivo" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Após uma apresentação formal da definição de PCA, a pergunta que se levanta é: qual é a motivação para aplicar essa técnica aos dados? O ponto crucial para compreender suas aplicações reside intrinsecamente ligada na interpretação dos dados nas novas variáveis geradas. O objetivo central do PCA é permitir a redução a dimensionalidade de um conjunto de dados, preservando ao máximo a variabilidade e a informação contida neles.</p>
<p>E como se dá essa redução? A chave está na interpretação: o pesquisador assume que, a partir de certo ponto de acúmulo de informação, o que excede pode ser considerado apenas resíduo ou erro estatístico inerente aos dados. Ou seja, parte da informação pode ser descartada sem comprometer a descrição que os dados propõem.</p>
<p>Para ser mais claro, a técnica de PCA não reduz a dimensionalidade por si só necessariamente; é o pesquisador que, na maior parte das vezes, por meio de uma interpretação cuidadosa das componentes principais, que decide descartar as de ordem mais alta, levando à redução efetiva da dimensionalidade dos dados.</p>
<p>A técnica de PCA reduz a dimensionalidade somente em situações de dependência linear entre variáveis. Quando uma variável pode ser expressa como uma combinação linear exata de outras, isso resulta em uma correlação perfeita, com coeficiente igual a 1. Nesses casos, o número de componentes principais gerados será menor que o menor valor entre o número de variáveis (<span class="math inline">\(n\)</span>) e o número de observações (<span class="math inline">\(m\)</span>), pois a redundância das informações é representada em uma única componente. Essa característica permite que o PCA retenha apenas as combinações de variáveis totalmente correlacionadas, simplificando a representação dos dados em termos de número de variáveis. Por exemplo didático, crie uma imagem mental da figura <a href="pca---análise-de-componentes-principais.html#fig:pca-rot">6.1</a> e reflita sobre a aplicação da PCA nos dados <span class="math inline">\(\mathbf{x}_1 = \{1;2;3\}\)</span> e <span class="math inline">\(\mathbf{x}_2 = \{1;2;3\}\)</span>.</p>
<p>Nos casos em que a correlação entre as variáveis é inferior a 1, mas ainda assim altamente significativa, a técnica de PCA resulta em <span class="math inline">\(k\)</span> componentes principais, sendo <span class="math inline">\(k\)</span> igual ao menor valor entre o número de variáveis (<span class="math inline">\(n\)</span>) e o número de observações (<span class="math inline">\(m\)</span>). Nesses cenários, algumas das componentes principais podem explicar muito pouca variabilidade, tornando-se candidatas a serem descartadas. Isso acontece porque as informações redundantes são capturadas em uma componente e apenas a informação restante que é independente, é representada em outra componentes mas com baixa descrição de variabilidade. Dessa forma, essas componentes não terá peso substancial para a descrição da variabilidade dos dados. Por exemplo, construa uma imagem mental da figura <a href="pca---análise-de-componentes-principais.html#fig:pca-rot">6.1</a> e pense na PCA dos dados <span class="math inline">\(\mathbf{x}_1 = \{1;2;3\}\)</span> e <span class="math inline">\(\mathbf{x}_2 = \{1;2;3,3\}\)</span>.</p>
<p>Para o caso em que as variáveis são inerentemente não correlacionadas, todas as componentes principais terão magnitudes semelhantes, tornando a aplicação do PCA desnecessária. Por exemplo, visualize a figura <a href="pca---análise-de-componentes-principais.html#fig:pca-rot">6.1</a> e considere a PCA aplicada aos dados <span class="math inline">\(\mathbf{x}_1 = \{1, 0\}\)</span> e <span class="math inline">\(\mathbf{x}_2 = \{0, 1\}\)</span>. Nessa situação, a falta de correlação entre as variáveis implica que a técnica não trará benefícios, já que as informações já estão distribuídas de forma independente.</p>
<p>É nesses termos que se afirma sobre a possibilidade de <strong>redução de dimensionalidade</strong>: A PCA permite simplificar conjuntos de dados de alta dimensão, ou seja, eliminar colinearidades quando estas estiverem presentes. Assim, permite que analistas visualizem e interpretem as informações de forma mais clara. Ao reduzir a dimensionalidade dos dados, a técnica preserva a maior parte da variabilidade, facilitando a identificação de padrões (caso exista) e a tomada de decisões. Essa representação mais compacta dos dados torna a análise mais acessível e eficiente, podendo revelar tendências que potencialmente passariam despercebidos em um espaço dimensional maior.</p>
<p>Aproveitando o exemplo já mostrado pela figura <a href="pca---análise-de-componentes-principais.html#fig:pca-rot">6.1</a> com as variáveis <span class="math inline">\(x_1\)</span> e <span class="math inline">\(x_2\)</span>, observamos que a correlação é <span class="math inline">\(\sim 83\%\)</span>. A primeira componente principal retém <span class="math inline">\(\sim 91,5\%\)</span>, da variância dos dados, enquanto a segunda componente retém apenas <span class="math inline">\(\sim 8,5\%\)</span>. Se considerarmos que uma retenção de variância acima de <span class="math inline">\(90\%\)</span> é satisfatória, podemos concluir que a segunda componente é dispensável. Isso nos permite simplificar os dados, mantendo a essência das informações. Ao descartar a segunda componente, assumimos que ela representa principalmente ruído. Dessa forma, estamos utilizando a análise de forma mais eficiente, interpretando os dados com o emprego da análise de PCA servindo como um filtro, destacando as características mais relevantes e eliminando variações indesejadas.</p>
<p><strong>Mais alguns Detalhes</strong></p>
<p>A consequência da redução de dimensionalidade em dados altamente correlacionados, como os obtidos em várias técnicas espectroscópicas, resulta em que a variância acumulada nas duas primeiras componentes principais geralmente representa uma parte significativa da variância total. Isso permite a criação de gráficos bidimensionais, que facilitam a exploração visual das componentes e se há identificação de padrões e tendências no comportamento dos dados. Essa ferramenta se torna muito útil para análises e interpretações.</p>
<p>Outro aspecto está relacionado quanto a decisão que precisamos tomar de quantas componentes principais devemos manter. No entanto, não existe uma regra fixa ou um número mágico para isso. A razão é que a escolha do <span class="math inline">\(k\)</span> depende de vários fatores, como o objetivo da análise e a natureza dos dados. Por exemplo, se queremos uma representação menos rigorosa dos dados, podemos optar por menos componentes. Mas se buscamos capturar a maior parte da variação nos dados, talvez precisemos de mais componentes.</p>
<p>Além disso, cada conjunto de dados é único, então o que funciona bem para um conjunto pode não ser adequado para outro. É por isso que, em geral, utilizamos a variação explicada por cada componente, mostrada em gráfico, ou soma acumulada da variância para nos ajudar a decidir quantas componentes manter, mas no final, a escolha sempre envolve um certo grau de julgamento e análise do contexto.</p>
<p>Outra característica intrínseca ao uso do PCA é que conseguir uma interpretação fácil e direta das componentes principais em termos físicos pode ser desafiador. Especialmente quando os dados têm muitas variáveis, a interpretação física é praticamente impossível e é melhor não tentar fazê-la. Isso acontece porque as componentes principais são combinações lineares das variáveis originais, e essas combinações podem não ter um significado físico claro.</p>
<p>Em vez disso, é muito mais eficaz focar nas variáveis com maior influência nas componentes, e assim sendo, é possível obter alguma interpretação ao analisar os coeficientes (ou pesos) que cada variável tem nas componentes. Se uma variável tem um peso alto em uma componente principal, isso indica que ela é importante para essa nova dimensão. Assim, você pode olhar para essas variáveis mais influentes e tentar entender como elas se relacionam com a variação dos dados.</p>
<p>Por fim, ter entendimento sobre PCA é importante também pois ela serve de base e suporte para outros métodos, como PCR (Regressão em Componentes Principais) e o método de classificação SIMCA (<em>Soft Independent Modeling of Class Analogy</em>). Além disso, a Análise Discriminante e vários métodos de agrupamento (<em>clustering</em>) podem ter um desempenho melhor quando aplicam a PCA para eliminar ruídos nos dados. Para qualquer outro método que enfrente desafios relacionados à multicolinearidade nas variáveis, a aplicação da PCA como etapa prévia se torna conveniente, pois permite eliminar redundâncias. Assim, a PCA atua como uma técnica de pré-processamento que aprimora a eficácia e a precisão desses métodos.</p>
<p> </p>
<p><strong>Resumindo:</strong></p>
<p>A PCA serve a vários propósitos no contexto da quimiometria e da análise multivariada de dados:</p>
<ul>
<li><p><strong>Redução de Dimensionalidade</strong>: Ao simplificar conjuntos de dados de alta dimensão, a PCA permite que os analistas visualizem e interpretem os dados mais facilmente.</p></li>
<li><p><strong>Identificação de Padrões</strong>: A técnica ajuda a revelar padrões ocultos, tendências e estruturas nos dados, facilitando a identificação de grupos ou clusters de amostras.</p></li>
<li><p><strong>Eliminação de Redundância</strong>: A PCA pode ajudar a eliminar variáveis redundantes, concentrando-se nas variáveis que mais contribuem para a variabilidade dos dados.</p></li>
<li><p><strong>Pré-processamento para Modelagem</strong>: A PCA é frequentemente utilizada como um passo de pré-processamento antes de aplicar modelos de regressão, classificação ou outras análises multivariadas, melhorando a eficiência e a precisão dos resultados.</p></li>
</ul>
<p> </p>
<p><strong>Recomendação para quando usar</strong>:</p>
<p>A PCA é recomendada em várias situações dentro da quimiometria, especialmente quando:</p>
<ul>
<li><p><strong>Dados de Alta Dimensionalidade</strong>: Quando se trabalha com conjuntos de dados que contêm um grande número de variáveis em relação ao número de amostras, a PCA é útil para reduzir a complexidade.</p></li>
<li><p><strong>Correlações Entre Variáveis</strong>: A PCA é especialmente eficaz quando há alta correlação entre as variáveis, pois a técnica ajuda a identificar as direções principais da variação nos dados.</p></li>
<li><p><strong>Visualização de Dados</strong>: Quando é necessário visualizar a estrutura dos dados, a PCA pode ser utilizada para criar gráficos bidimensionais ou tridimensionais que facilitam a interpretação e a comunicação dos resultados.</p></li>
<li><p><strong>Detecção de Outliers</strong>: A PCA pode ajudar a identificar pontos de dados que se afastam do padrão esperado, facilitando a detecção de outliers.</p></li>
<li><p><strong>Modelos Previstos</strong>: A PCA é útil como etapa inicial na modelagem, onde as variáveis transformadas em componentes principais podem ser usadas como entradas para modelos de regressão ou classificação, garantindo que a variabilidade significativa dos dados seja capturada.</p></li>
</ul>
</div>
<div id="pca-como-decomposição-matricial" class="section level2 hasAnchor" number="6.5">
<h2><span class="header-section-number">6.5</span> PCA como Decomposição Matricial<a href="pca---análise-de-componentes-principais.html#pca-como-decomposição-matricial" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As combinações lineares das quais discutimos anteriormente podem ser reestruturadas em formato matricial, tornando a notação mais compacta.</p>
<p><span class="math display">\[
\mathbf{T} = \mathbf{X} \cdot \mathbf{P}
\]</span></p>
<p>       onde:</p>
<ul>
<ul>
<li>
<span class="math inline">\(\mathbf{T}\)</span> é a matriz de <em>scores</em>
</li>
<li>
<span class="math inline">\(\mathbf{X}\)</span> é a matriz de dados
</li>
<li>
<span class="math inline">\(\mathbf{P}\)</span> é a matriz de <em>loadings</em>
</li>
</ul>
</ul>
<p> </p>
<p>E decorre, após manipulações, que PCA pode igualmente ser entendido e abordado como uma decomposição da matriz <span class="math inline">\(\mathbf{X}\)</span> nessas outras duas novas matrizes:</p>
<p> </p>
<p><span class="math display">\[
\mathbf{X} = \mathbf{T} \cdot \mathbf{P^T}
\]</span></p>
<p> </p>
<ul>
<li><p>Os <em>scores</em> referem-se às coordenadas das observações nos novos eixos criados pelos componentes principais. Em outras palavras, eles representam a projeção das observações originais na nova base de componentes principais. Cada <em>score</em> indica como uma observação se posiciona em relação aos novos eixos.</p></li>
<li><p>Os <em>loadings</em>, por outro lado, representam a contribuição de cada variável original para os componentes principais. Eles indicam a correlação entre as variáveis originais e os componentes principais. Os <em>loadings</em> ajudam a entender como as variáveis influenciam a formação dos novos eixos e, consequentemente, os <em>scores</em>. Em geral, um <em>loading</em> alto (positivo ou negativo) para uma variável em um componente principal sugere que essa variável tem uma forte influência nesse componente.</p></li>
</ul>
<p> </p>
<p><strong>Equivalência de Notação</strong></p>
<p>As duas formas são equivalentes, pois podemos monstrar que <span class="math inline">\(\mathbf{T} = \mathbf{X} \cdot \mathbf{P}\)</span> pode ser expresso como <span class="math inline">\(\mathbf{X} = \mathbf{T} \cdot \mathbf{P^T}\)</span>. Só precisamos manipular a equação de forma a isolarmos <span class="math inline">\(\mathbf{X}\)</span>.</p>
<ol style="list-style-type: decimal">
<li>Começamos com a equação original:</li>
</ol>
<p><span class="math display">\[
\mathbf{T} = \mathbf{X} \cdot \mathbf{P}
\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Multiplicamos ambos os lados pela inversa de <span class="math inline">\(\mathbf{P}\)</span> à direita. Como <span class="math inline">\(\mathbf{P}\)</span> é ortogonal, usamos a relação <span class="math inline">\(\mathbf{P^{-1}} = \mathbf{P^T}\)</span>:</li>
</ol>
<p><span class="math display">\[
\mathbf{T} \cdot \mathbf{P^{-1}} = ( \mathbf{X} \cdot \mathbf{P} ) \cdot \mathbf{P^{-1}}
\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>O lado direito simplifica-se como:</li>
</ol>
<p><span class="math display">\[
\mathbf{T} \cdot \mathbf{P^T} = \mathbf{X} \cdot \mathbf{I} = \mathbf{X}
\]</span></p>
<ol start="4" style="list-style-type: decimal">
<li>Portanto, podemos reescrever a equação como:</li>
</ol>
<p><span class="math display">\[
\mathbf{X} = \mathbf{T} \cdot \mathbf{P^T}
\]</span></p>
<p>Assim, está demonstrado que, sob a condição de que <span class="math inline">\(\mathbf{P}\)</span> sendo uma matriz ortogonal, a relação <span class="math inline">\(\mathbf{T} = \mathbf{X} \cdot \mathbf{P}\)</span> implica em <span class="math inline">\(\mathbf{X} = \mathbf{T} \cdot \mathbf{P^T}\)</span> e que o ponto de vista sobre PCA ser uma decomposição matricial é factível.</p>
<div id="principais-algoritmos" class="section level3 hasAnchor" number="6.5.1">
<h3><span class="header-section-number">6.5.1</span> Principais Algoritmos<a href="pca---análise-de-componentes-principais.html#principais-algoritmos" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Adotando a perpectiva da decomposição matricial, conseguimos estabelecer uma conexão natural para explorar outras decomposições matriciais já existentes e suas interconexões metodológicas. Assim sendo, ao menos os métodos mais usuais utilizados para realizar a PCA aproveitam abordagens matemáticas distintas já consolidadas.</p>
<p>Cada uma dessas abordagens oferece suas particularidades e aplicações específicas, proporcionando vantagens em diferentes contextos e tamanhos de conjuntos de dados.</p>
<p>A seguir, apresentamos três formatos diferentes, cada um implementado por meio das seguintes abordagens:</p>
<div id="decomposição-espectral" class="section level4 hasAnchor" number="6.5.1.1">
<h4><span class="header-section-number">6.5.1.1</span> Decomposição Espectral<a href="pca---análise-de-componentes-principais.html#decomposição-espectral" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A Decomposição Espectral<a href="#fn51" class="footnote-ref" id="fnref51"><sup>51</sup></a><sup>,</sup><a href="#fn52" class="footnote-ref" id="fnref52"><sup>52</sup></a> também conhecida como Decomposição em Autovetores e Autovalores<a href="#fn53" class="footnote-ref" id="fnref53"><sup>53</sup></a><sup>,</sup><a href="#fn54" class="footnote-ref" id="fnref54"><sup>54</sup></a> decompõe uma matriz quadrada <span class="math inline">\(\mathbf{A}\)</span> de dimensão <span class="math inline">\(m \times m\)</span> na forma:</p>
<p><span class="math display">\[
\mathbf{A} = \mathbf{V} \cdot \mathbf{\Sigma} \cdot \mathbf{V^{-1}}
\]</span></p>
<p>       onde:</p>
<ul>
<ul>
<li>
<span class="math inline">\(\mathbf{A}\)</span> é a matriz de covariância<a href="#fn55" class="footnote-ref" id="fnref55"><sup>55</sup></a> de <span class="math inline">\(\mathbf{X}\)</span>.
</li>
<li>
<span class="math inline">\(\mathbf{V}\)</span> é uma matriz <span class="math inline">\(m \times m\)</span> cujas colunas são os autovetores. <span class="math inline">\(\mathbf{V^{-1}}\)</span> é a inversa de <span class="math inline">\(\mathbf{V}\)</span>.
</li>
<li>
<p><span class="math inline">\(\mathbf{\Sigma}\)</span> é uma matriz diagonal de dimensão <span class="math inline">\(m \times m\)</span> contendo os autovalores correspondentes.</p>
</ul>
</ul>
<p> </p>
<p>As matrizes <span class="math inline">\(\mathbf{P}\)</span> (<em>loadings</em>) e <span class="math inline">\(\mathbf{T}\)</span> (<em>scores</em>), da Análise de Componentes Principais (PCA), podem ser obtidas por:</p>
<p> </p>
<p><span class="math display">\[
\mathbf{P} = \mathbf{V}
\]</span></p>
<p> </p>
<p><span class="math display">\[
\mathbf{T} = \mathbf{X} \cdot \mathbf{V}
\]</span></p>
<p> </p>
<p>Uma das características intrísecas a esse método de implementação de PCA é que exige cálculo explícito tanto de <span class="math inline">\(\mathbf{A} = \mathbf{X^T} \cdot \mathbf{X}\)</span> quanto de <span class="math inline">\(\mathbf{T} = \mathbf{X} \cdot \mathbf{V}\)</span> , que são custosas computacionalmente em conjuntos de dados muito grandes (caso das conhecidas ciências “<em>ômicas</em>” e da área de <em>Big Data</em>) devido ao número de operação. O que por sua vez, acarreta em maior propagação dos erros inerentes à representação numérica na computação em ponto flutuante<a href="#fn56" class="footnote-ref" id="fnref56"><sup>56</sup></a> quanto comparável à implementação a ser mostrada em sequência por SVD. Algoritmos que implementam essa decomposição não permitem dados faltantes. Caso exista na matriz de dados, o usuário deve tratar os dados perdidos previamente.</p>
<p>No R, existe a função <code>princomp</code> que se utiliza justamente dessa forma de abordagem para PCA. Porém a leitura do manual da função declara que a mesma existe somente por compatibilidade com o <em>software</em> S-PLUS e que deve ser um método preterido. Para saber mais detalhes, digite : <code>?princomp</code> no console do R.</p>
<p>O leitor que se sentir satisfeito ao saber que, uma vez obtida a solução da Decomposição Espectral da matriz <span class="math inline">\(\mathbf{A}\)</span>, a solução de PCA será automaticamente alcançada, pode pular os detalhes e prosseguir para o próximo algoritmo.</p>
<p> </p>
<details>
<summary>
😈 + Detalhes
</summary>
<p><br><strong>Uma Solução para a Decomposição Espectral</strong><br><br></p>
<p>Para realizar a Decomposição Espectral podemos seguir os seguintes passos:</p>
<ol style="list-style-type: decimal">
<li><strong>Encontrar os autovalores</strong>:
<ul>
<li>Calcular o polinômio característico de <span class="math inline">\(\mathbf{A}\)</span>, que é dado por <span class="math inline">\(\det(\mathbf{A} - \lambda \mathbf{I}) = 0\)</span>, onde <span class="math inline">\(\lambda\)</span> são os autovalores, <span class="math inline">\(\mathbf{I}\)</span> é a matriz identidade e <span class="math inline">\(\det()\)</span> é a função matricial determinante<a href="#fn57" class="footnote-ref" id="fnref57"><sup>57</sup></a>.</li>
<li>Resolver a equação para encontrar os autovalores <span class="math inline">\(\lambda_1, \lambda_2, \ldots, \lambda_n\)</span>.</li>
</ul></li>
</ol>
<ol start="2" style="list-style-type: decimal">
<li><strong>Encontrar os autovetores</strong>:
<ul>
<li>Para cada autovalor <span class="math inline">\(\lambda_i\)</span>, resolva o sistema <span class="math inline">\((\mathbf{A} - \lambda_i \mathbf{I})\mathbf{v}_i = 0\)</span> para encontrar o autovetor correspondente <span class="math inline">\(\mathbf{v}_i\)</span>.</li>
<li>Repita isso para todos os autovalores.</li>
</ul></li>
<li><strong>Formar as matrizes <span class="math inline">\(\mathbf{V}\)</span> e <span class="math inline">\(\mathbf{\Sigma}\)</span></strong>:
<ul>
<li>A matriz <span class="math inline">\(\mathbf{V}\)</span> é formada pelos autovetores como colunas: <span class="math inline">\(\mathbf{V} = [\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_n]\)</span>.</li>
<li>A matriz <span class="math inline">\(\mathbf{\Sigma}\)</span> é uma matriz diagonal cujos elementos na diagonal são os autovalores correspondentes: <span class="math inline">\(\mathbf{\Sigma} = \text{diag}(\lambda_1, \lambda_2, \ldots, \lambda_n)\)</span>.</li>
</ul></li>
<li><strong>Verificar a decomposição</strong>:
<ul>
<li>Você pode verificar a decomposição calculando <span class="math inline">\(\mathbf{V} \cdot \mathbf{\Sigma} \cdot \mathbf{V^{-1}}\)</span> e confirmando que resulta na matriz original <span class="math inline">\(\mathbf{A}\)</span>.</li>
</ul></li>
</ol>
<p><strong>Observação</strong>: A decomposição espectral é aplicável a matrizes que são diagonalisáveis<a href="#fn58" class="footnote-ref" id="fnref58"><sup>58</sup></a></p>
<p> </p>
<p><strong>Resolução usando o Polinômio Característico</strong></p>
<p>Aqui, para o leitor que deseja acompanhar um exemplo de solução passo a passo, exploraremos com mais alguns detalhes o caso ilustrado na figura <a href="pca---análise-de-componentes-principais.html#fig:pca-rot">6.1</a>.</p>
<p>Aqui, mais uma vez, faremos uso explícito do ChatGPT para demonstrar como utilizá-lo como ferramenta de suporte.</p>
<p>Podemos iniciar com uma pergunta ao ChatGPT da seguinte forma:</p>
<p> </p>
<hr />
<p><em>Encontre os autovalores e autovetores para a matriz abaixo:</em></p>
<pre><code>&gt; cov(data)
          x1        x2
x1 1.0000000 0.8303746
x2 0.8303746 1.0000000
&gt;</code></pre>
<p><em>Use somente elementos de álgebra. Não use funções prontas do R.</em></p>
<hr />
<p> </p>
<p>A resposta fornecida pelo ChatGPT foi a seguinte:</p>
<p> </p>
<hr />
<p>Para encontrar os autovalores e autovetores da matriz de covariância</p>
<p><span class="math display">\[
A = \begin{bmatrix} 1.0000000 &amp; 0.8303746 \\ 0.8303746 &amp; 1.0000000 \end{bmatrix}
\]</span></p>
<p>podemos proceder usando métodos da álgebra linear. O primeiro passo é encontrar os autovalores <span class="math inline">\(\lambda\)</span> resolvendo o <strong>polinômio característico</strong><a href="#fn59" class="footnote-ref" id="fnref59"><sup>59</sup></a>.</p>
<p><strong>Passo 1: Determinar o polinômio característico</strong></p>
<p>O polinômio característico é dado por:</p>
<p><span class="math display">\[
\text{det}( \mathbf{A} - \mathbf{\lambda} \mathbf{I}) = 0
\]</span></p>
<p>onde <span class="math inline">\(\mathbf{I}\)</span> é a matriz identidade de mesma ordem que <span class="math inline">\(\mathbf{A}\)</span>, e <span class="math inline">\(\mathbf{\lambda}\)</span> são os autovalores. Assim, temos:</p>
<p><span class="math display">\[
\text{det} \left( \begin{bmatrix} 1.0000000 &amp; 0.8303746 \\ 0.8303746 &amp; 1.0000000 \end{bmatrix} - \lambda \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix} \right) = 0
\]</span></p>
<p>Isto se traduz na seguinte equação:</p>
<p><span class="math display">\[
\text{det} \left( \begin{bmatrix} 1.0000000 - \lambda &amp; 0.8303746 \\ 0.8303746 &amp; 1.0000000 - \lambda \end{bmatrix} \right) = 0
\]</span></p>
<p>O determinante de uma matriz <span class="math inline">\(2 \times 2\)</span> é dado por:</p>
<p><span class="math display">\[
\text{det}(\mathbf{A}) = (a_{11} \cdot a_{22}) - (a_{12} \cdot a_{21})
\]</span></p>
<p>Aplicando isso à matriz <span class="math inline">\(\mathbf{A} - \mathbf{\lambda} \mathbf{I}\)</span>:</p>
<p><span class="math display">\[
(1.0000000 - \lambda)(1.0000000 - \lambda) - (0.8303746 \cdot 0.8303746) = 0
\]</span></p>
<p>Simplificando:</p>
<p><span class="math display">\[
(1.0000000 - \lambda)^2 - 0.8303746^2 = 0
\]</span></p>
<p>Expanda os termos:</p>
<p><span class="math display">\[
(1.0000000 - \lambda)^2 = 1 - 2.0000000\lambda + \lambda^2
\]</span>
<span class="math display">\[
0.8303746^2 = 0.6895219
\]</span></p>
<p>Agora temos a equação:</p>
<p><span class="math display">\[
1 - 2.0000000\lambda + \lambda^2 - 0.6895219 = 0
\]</span></p>
<p>Simplifique para chegar na forma canônica do <strong>polinômio característico</strong>:</p>
<p><span class="math display">\[
\lambda^2 - 2.0000000\lambda + 0.3104781 = 0
\]</span></p>
<p><strong>Passo 2: Resolver a equação quadrática</strong></p>
<p>Agora, podemos resolver essa equação quadrática usando a fórmula de Bhaskara<a href="#fn60" class="footnote-ref" id="fnref60"><sup>60</sup></a>:</p>
<p><span class="math display">\[
\lambda = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}
\]</span></p>
<p>Onde <span class="math inline">\(a = 1\)</span>, <span class="math inline">\(b = -2.0000000\)</span> e <span class="math inline">\(c = 0.3104781\)</span>. Calculando o discriminante:</p>
<p><span class="math display">\[
\Delta = (-2.0000000)^2 - 4(1)(0.3104781)
\]</span>
<span class="math display">\[
\Delta = 4.0000000 - 1.2419124 = 2.7580876
\]</span></p>
<p>Agora, extraímos a raiz quadrada do discriminante:</p>
<p><span class="math display">\[
\sqrt{\Delta} = \sqrt{2.7580876} \approx 1.660211
\]</span></p>
<p>Agora, podemos calcular os autovalores:</p>
<p><span class="math display">\[
\lambda_1 = \frac{2.0000000 + 1.660211}{2} = \frac{3.660211}{2} \approx 1.830106
\]</span>
<span class="math display">\[
\lambda_2 = \frac{2.0000000 - 1.660211}{2} = \frac{0.339789}{2} \approx 0.169894
\]</span></p>
<p><strong>Passo 3: Encontrar os autovetores</strong></p>
<p>Agora que temos os autovalores <span class="math inline">\(\lambda_1 = 1.830106\)</span> e <span class="math inline">\(\lambda_2 = 0.169894\)</span>, podemos encontrar os autovetores resolvendo o sistema <span class="math inline">\((A - \lambda I)v = 0\)</span> para cada autovalor.</p>
<p><strong>Para <span class="math inline">\(\lambda_1 = 1.830106\)</span>:</strong></p>
<p>A equação <span class="math inline">\(( \mathbf{A} - \lambda_1 \mathbf{I})\mathbf{v} = 0\)</span> é:</p>
<p><span class="math display">\[
\begin{bmatrix} 1.0000000 - 1.830106 &amp; 0.8303746 \\ 0.8303746 &amp; 1.0000000 - 1.830106 \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \end{bmatrix} = 0
\]</span></p>
<p>Isso resulta no sistema:</p>
<p><span class="math display">\[
\begin{bmatrix} -0.830106 &amp; 0.8303746 \\ 0.8303746 &amp; -0.830106 \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \end{bmatrix} = 0
\]</span></p>
<p>Resolvendo, obtemos uma relação entre <span class="math inline">\(v_1\)</span> e <span class="math inline">\(v_2\)</span>. Para simplificação, o autovetor associado será proporcional a:</p>
<p><span class="math display">\[
v_1 \approx v_2
\]</span></p>
<p><strong>Para <span class="math inline">\(\lambda_2 = 0.169894\)</span>:</strong></p>
<p>Similarmente, para <span class="math inline">\(\lambda_2\)</span>:</p>
<p><span class="math display">\[
\begin{bmatrix} 1.0000000 - 0.169894 &amp; 0.8303746 \\ 0.8303746 &amp; 1.0000000 - 0.169894 \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \end{bmatrix} = 0
\]</span></p>
<p>Isso resulta no sistema:</p>
<p><span class="math display">\[
\begin{bmatrix} 0.830106 &amp; 0.8303746 \\ 0.8303746 &amp; 0.830106 \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \end{bmatrix} = 0
\]</span></p>
<p>Resolvendo, temos que:</p>
<p><span class="math display">\[
v_1 \approx -v_2
\]</span></p>
<p><strong>Resumo dos autovalores e autovetores (não normalizados):</strong></p>
<ul>
<li>Autovalores: <span class="math inline">\(\lambda_1 \approx 1.830106\)</span> e <span class="math inline">\(\lambda_2 \approx 0.169894\)</span></li>
<li>Autovetores (não normalizados):
<ul>
<li>Para <span class="math inline">\(\lambda_1\)</span>: <span class="math inline">\(v_1 = \begin{bmatrix} 1 \\ 1 \end{bmatrix}\)</span></li>
<li>Para <span class="math inline">\(\lambda_2\)</span>: <span class="math inline">\(v_2 = \begin{bmatrix} 1 \\ -1 \end{bmatrix}\)</span></li>
</ul></li>
</ul>
<p><strong>Passo 4: Normalizar os autovetores</strong></p>
<p>Para normalizar os autovetores, devemos dividir cada autovetor pela sua norma<a href="#fn61" class="footnote-ref" id="fnref61"><sup>61</sup></a> (ou módulo). A norma de um vetor <span class="math inline">\(\mathbf{v} = \begin{bmatrix} v_1 \\ v_2 \end{bmatrix}\)</span> é dada por:</p>
<p><span class="math display">\[
\|\mathbf{v}\| = \sqrt{v_1^2 + v_2^2}
\]</span></p>
<p>Agora, vamos calcular a norma de cada um deles e dividi-los por essa norma.</p>
<p><strong>Para o autovetor <span class="math inline">\(\mathbf{v_1} = \begin{bmatrix} 1 \\ 1 \end{bmatrix}\)</span>:</strong></p>
<p><span class="math display">\[
\|\mathbf{v_1}\| = \sqrt{1^2 + 1^2} = \sqrt{2}
\]</span></p>
<p>O autovetor normalizado é:</p>
<p><span class="math display">\[
\mathbf{\hat{v_1}} = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ 1 \end{bmatrix} = \begin{bmatrix} \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} \end{bmatrix}
\]</span></p>
<p>Ou, de forma aproximada:</p>
<p><span class="math display">\[
\mathbf{\hat{v_1}} = \begin{bmatrix} 0.707 \\ 0.707 \end{bmatrix}
\]</span></p>
<p><strong>Para o autovetor <span class="math inline">\(\mathbf{v_2} = \begin{bmatrix} 1 \\ -1 \end{bmatrix}\)</span>:</strong></p>
<p><span class="math display">\[
\|\mathbf{v_2}\| = \sqrt{1^2 + (-1)^2} = \sqrt{2}
\]</span></p>
<p>O autovetor normalizado é:</p>
<p><span class="math display">\[
\mathbf{\hat{v_2}} = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ -1 \end{bmatrix} = \begin{bmatrix} \frac{1}{\sqrt{2}} \\ \frac{-1}{\sqrt{2}} \end{bmatrix}
\]</span></p>
<p>Ou, de forma aproximada:</p>
<p><span class="math display">\[
\mathbf{\hat{v_2}} = \begin{bmatrix} 0.707 \\ -0.707 \end{bmatrix}
\]</span></p>
<p><strong>Resumo dos autovetores normalizados:</strong></p>
<ul>
<li>Autovetor normalizado para <span class="math inline">\(\lambda_1 = 1.830106\)</span>:</li>
</ul>
<p><span class="math display">\[
\mathbf{\hat{v_1}} = \begin{bmatrix} 0.707 \\ 0.707 \end{bmatrix}
\]</span></p>
<ul>
<li>Autovetor normalizado para <span class="math inline">\(\lambda_2 = 0.169894\)</span>:</li>
</ul>
<p><span class="math display">\[
\mathbf{\hat{v_2}} = \begin{bmatrix} 0.707 \\ -0.707 \end{bmatrix}
\]</span></p>
<hr />
<p><strong>Considerações</strong></p>
<p><strong>Atenção: Não assuma que o resultado fornecido pelo ChatGPT esteja 100% correto!</strong></p>
<p>As respostas devem ser vistas como um ponto de partida ou guia de estudo. Embora possam oferecer informações valiosas, é comum que contenham erros ou imprecisões.</p>
<p>Por isso, é essencial revisar e ajustar o conteúdo. Essa busca por refinamento se torna um ótimo exercício de aprendizado, permitindo que você desenvolva habilidades críticas e analíticas.</p>
<p>Se o leitor acredita que a saída fornecida pelo ChatGPT não foi alterada, engana-se.
Várias versões de perguntas e mudanças no texto foram feitas até esse resultado final.</p>
<p>Dedique um tempo para analisar as informações recebidas, identificando inconsistências ou áreas que podem ser aprimoradas. Essa prática não só fortalecerá seu entendimento, mas também o ajudará a se tornar um leitor mais crítico.</p>
<p>Dado o aconselhamento, realizaremos uma verificação dos resultados em comparação com os obtidos no R.</p>
<p>Utilizando a função nativa <code>eigen()</code>, podemos calcular os autovalores e autovetores:</p>
<pre><code>&gt; eigen( cov( X ) )
eigen() decomposition
$values
[1] 1.8303746 0.1696254

$vectors
          [,1]       [,2]
[1,] 0.7071068 -0.7071068
[2,] 0.7071068  0.7071068
&gt;</code></pre>
<p>Aqui, podemos observar que nossa solução utilizando o polinômio característico está alinhada com o <em>output</em> fornecido pelo console do R, com uma pequena exceção!</p>
<p>O segundo autovetor apresenta sinais invertidos, o que é uma característica inerente à Análise de Componentes Principais (PCA). Não é possível prever a direção exata dada pelos sinais gerados, mas, se necessário para um contexto específico, podemos multiplicar esse autovetor por -1 sem que isso altere a interpretação da análise. Quanto aos autovalores, os mesmos estão com plena concordânica a menos de pequeno erro de aproximações numéricas.</p>
<p>A inversão dos sinais é corroborada pelo uso de outra função do R, a <code>princomp</code>. Veja abaixo:</p>
<pre><code>&gt; princomp( X )$loadings

Loadings:
   Comp.1 Comp.2
x1  0.707  0.707
x2  0.707 -0.707
&gt;</code></pre>
<p>Por fim, devemos verificar se conseguimos restaurar a matriz de covariância original por meio da multiplicação das nossas matrizes de autovalores e autovetores. Mais uma vez, salvo por possíveis erros de aproximação numérica, alcançamos a conferência desejada.</p>
<pre><code>&gt; v1 &lt;- c( 0.707, 0.707 )
&gt; v2 &lt;- c( 0.707, -0.707 )
&gt; V &lt;- cbind( v1, v2 )
&gt; Sigma &lt;- diag( c( 1.8301, 0.169894 ) )
&gt; round( V %*% Sig %*% solve(V), 4 )
       [,1]   [,2]
[1,] 1.0000 0.8301
[2,] 0.8301 1.0000
&gt; </code></pre>
<p>Como detalhe adicional, é importante ressaltar para os iniciantes que a abordagem adotada nesta resolução é puramente didática. O desenvolvimento de algoritmos numéricos e eficientes envolve técnicas complexas e avançadas, que vão muito além do escopo deste texto introdutório. O objetivo aqui é proporcionar uma compreensão básica dos conceitos, preparando o terreno para estudos mais aprofundados no futuro.</p>
<p>Os algoritmos utilizados em <em>softwares</em> especializados são significativamente mais avançados e sofisticados. Eles são projetados para lidar com uma ampla variedade de situações, levando em consideração fatores como eficiência computacional e precisão dos resultados.</p>
<center>
△
</center>
</details>
<p> </p>
</div>
<div id="decomposição-em-valores-singulares-svd" class="section level4 hasAnchor" number="6.5.1.2">
<h4><span class="header-section-number">6.5.1.2</span> Decomposição em Valores Singulares (SVD)<a href="pca---análise-de-componentes-principais.html#decomposição-em-valores-singulares-svd" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A Decomposição em Valores Singulares (<em>Singular Value Decomposition - SVD</em>)<a href="#fn62" class="footnote-ref" id="fnref62"><sup>62</sup></a> é considerada uma generalização da Decomposição Espectral para casos gerais de matrizes retangulares. É uma técnica que fatoriza uma matriz <span class="math inline">\(\mathbf{X}\)</span> de dimensões <span class="math inline">\(n \times m\)</span> em três matrizes:</p>
<p> </p>
<p><span class="math display">\[ \mathbf{X} = \mathbf{U} \cdot \mathbf{\Sigma} \cdot \mathbf{V^T} \]</span></p>
<p>       onde:</p>
<ul>
<ul>
<li>
<span class="math inline">\(\mathbf{U}\)</span> é uma matriz ortonormal<span class="math inline">\(\dagger\)</span> de dimensão <span class="math inline">\(n \times n\)</span>. É a matriz de vetores singulares a esquerda.
</li>
<li>
<span class="math inline">\(\mathbf{V}\)</span> é uma matriz ortonormal<span class="math inline">\(\dagger\)</span> de dimensão <span class="math inline">\(m \times m\)</span>. É a matriz de vetores singulares a direita.
</li>
<li>
<p><span class="math inline">\(\mathbf{\Sigma}\)</span> é uma matriz de dimensão <span class="math inline">\(n \times m\)</span> onde os valores singulares estão efetivamente organizados em uma submatriz diagonal <span class="math inline">\(r \times r\)</span>, onde <span class="math inline">\(r\)</span> é o posto da matriz <span class="math inline">\(\mathbf{X}\)</span> (o número de valores singulares não nulos e não-negativos). O valor de <span class="math inline">\(r\)</span> pode ser no máximo igual a <span class="math inline">\(min(n,m)\)</span>. Os valores singulares em <span class="math inline">\(\mathbf{\Sigma}\)</span> indicam a importância das direções representadas pelas colunas de <span class="math inline">\(\mathbf{U}\)</span> e <span class="math inline">\(\mathbf{V}\)</span>.</p>
<p><strong><span class="math inline">\(\dagger\)</span>Nota</strong>: Uma matriz <span class="math inline">\(\mathbf{M}\)</span> ser ortonormal significa que suas colunas são vetores ortogonais entre si e têm norma unitária<a href="#fn63" class="footnote-ref" id="fnref63"><sup>63</sup></a>. Isso garante que <span class="math inline">\(\mathbf{M^T} \cdot \mathbf{M} = \mathbf{I}\)</span>, onde <span class="math inline">\(\mathbf{I}\)</span> é a matriz identidade.</p>
</ul>
</ul>
<p> </p>
<p>Na Análise de Componentes Principais (PCA), utiliza-se da decomposição da SVD pois as matrizes <span class="math inline">\(\mathbf{P}\)</span> (<em>loadings</em>) e <span class="math inline">\(\mathbf{T}\)</span> (<em>scores</em>) podem ser diretamente obtidas pelas relações:</p>
<p> </p>
<p><span class="math display">\[
\mathbf{P} = \mathbf{V}
\]</span></p>
<p> </p>
<p><span class="math display">\[
\mathbf{T} = \mathbf{U} \cdot \mathbf{\Sigma}
\]</span></p>
<p> </p>
<p>Nas implementações da decomposição SVD também não é possível ter dados faltantes. Caso exista na matriz de dados, o usuário deve tratar os dados perdidos previamente.
No R, existe a função <code>prcomp</code> que implementa PCA via SVD. Deve ser o método de preferência pela melhor precisão numérica conforme relatada no manual e pelo que evidencia as equações. Consulte o manual da função pelo comando: <nobr><code>?prcomp</code>.</nobr></p>
<p> </p>
<details>
<summary>
😈 + Detalhes
</summary>
<p> </p>
<p><strong>Relação entre as decomposições SVD e Espectral</strong></p>
<p>Conforme já mencionado, a decomposição SVD pode ser vista como uma generalização da Decomposição Espectral para matrizes retangulares. Para entender a SVD, podemos conceitualmente derivá-la a partir da Decomposição Espectral, aplicando-a de maneira adequada às matrizes que não são quadradas.</p>
<p> </p>
<ol style="list-style-type: decimal">
<li><strong>Defina a matriz de dados</strong>:
<ul>
<li>Considere uma matriz <span class="math inline">\(\mathbf{X}\)</span> de tamanho <span class="math inline">\(n \times m\)</span>.</li>
</ul></li>
<li><strong>Calcule <span class="math inline">\(\mathbf{X^TX}\)</span> e <span class="math inline">\(\mathbf{XX^T}\)</span></strong>:
<ul>
<li>Compute <span class="math inline">\(\mathbf{X^TX}\)</span> (matriz <span class="math inline">\(m \times m\)</span>) e <span class="math inline">\(\mathbf{XX^T}\)</span> (matriz <span class="math inline">\(n \times n\)</span>).</li>
</ul></li>
<li><strong>Encontre os autovalores</strong>:
<ul>
<li>Calcule os autovalores de <span class="math inline">\(\mathbf{X^TX}\)</span> e <span class="math inline">\(\mathbf{XX^T}\)</span>.</li>
<li>Os autovalores não negativos de <span class="math inline">\(\mathbf{X^TX}\)</span> são os quadrados dos valores singulares de <span class="math inline">\(\mathbf{X}\)</span>.</li>
</ul></li>
<li><strong>Calcule os autovetores</strong>:
<ul>
<li>Encontre os autovetores correspondentes aos autovalores de:
<ul>
<li><span class="math inline">\(\mathbf{X^TX}\)</span> (que formarão a matriz <span class="math inline">\(\mathbf{V}\)</span>) e de;</li>
<li><span class="math inline">\(\mathbf{XX^T}\)</span> (que formarão a matriz <span class="math inline">\(\mathbf{U}\)</span>).</li>
</ul></li>
</ul></li>
<li><strong>Organize os valores singulares</strong>:
<ul>
<li>Os valores singulares <span class="math inline">\(\sigma_i\)</span> são as raízes quadradas dos autovalores.</li>
<li>Organize-os em ordem decrescente.</li>
</ul></li>
<li><strong>Construa as matrizes <span class="math inline">\(\mathbf{U}\)</span>, <span class="math inline">\(\mathbf{\Sigma}\)</span> e <span class="math inline">\(\mathbf{V}\)</span></strong>:
<ul>
<li><span class="math inline">\(\mathbf{U}\)</span> é formado pelos autovetores de <span class="math inline">\(\mathbf{XX^T}\)</span>.</li>
<li><span class="math inline">\(\mathbf{V}\)</span> é formado pelos autovetores de <span class="math inline">\(\mathbf{X^TX}\)</span>.</li>
<li><span class="math inline">\(\mathbf{\Sigma}\)</span> é uma matriz diagonal onde os valores singulares <span class="math inline">\(\sigma_i\)</span> estão dispostos na diagonal.</li>
</ul></li>
<li><strong>Monte a decomposição</strong>:
<ul>
<li>A decomposição SVD é dada por <span class="math inline">\(\mathbf{X} = \mathbf{U \Sigma V^T}\)</span>.</li>
</ul></li>
<li><strong>Verifique a decomposição</strong>:
<ul>
<li>Multiplique as matrizes <span class="math inline">\(\mathbf{U}\)</span>, <span class="math inline">\(\mathbf{\Sigma}\)</span> e <span class="math inline">\(\mathbf{V^T}\)</span> para garantir que você retorna à matriz original <span class="math inline">\(\mathbf{X}\)</span>.</li>
</ul></li>
</ol>
<p> </p>
<p>Entretanto, essa relação deve ser vista apenas como uma ferramenta de compreensão, e não deve ser utilizada para a implementação numérica, especialmente em matrizes de grandes dimensões.</p>
<p> </p>
<p><strong>Verificação Numérica da Equivalência</strong></p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="pca---análise-de-componentes-principais.html#cb39-1" tabindex="-1"></a><span class="co"># Cria uma matriz de exemplo X</span></span>
<span id="cb39-2"><a href="pca---análise-de-componentes-principais.html#cb39-2" tabindex="-1"></a><span class="fu">set.seed</span>( <span class="dv">42</span> )</span>
<span id="cb39-3"><a href="pca---análise-de-componentes-principais.html#cb39-3" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb39-4"><a href="pca---análise-de-componentes-principais.html#cb39-4" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="dv">8</span></span>
<span id="cb39-5"><a href="pca---análise-de-componentes-principais.html#cb39-5" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">round</span>( <span class="fu">matrix</span>( <span class="fu">rnorm</span>( n<span class="sc">*</span>m ), n, m ), <span class="dv">2</span> )</span>
<span id="cb39-6"><a href="pca---análise-de-componentes-principais.html#cb39-6" tabindex="-1"></a></span>
<span id="cb39-7"><a href="pca---análise-de-componentes-principais.html#cb39-7" tabindex="-1"></a><span class="co"># Calcula a SVD de X</span></span>
<span id="cb39-8"><a href="pca---análise-de-componentes-principais.html#cb39-8" tabindex="-1"></a>svd_decomp <span class="ot">&lt;-</span> <span class="fu">La.svd</span>( X )</span>
<span id="cb39-9"><a href="pca---análise-de-componentes-principais.html#cb39-9" tabindex="-1"></a></span>
<span id="cb39-10"><a href="pca---análise-de-componentes-principais.html#cb39-10" tabindex="-1"></a><span class="co"># Calcula X^T X e XX^T</span></span>
<span id="cb39-11"><a href="pca---análise-de-componentes-principais.html#cb39-11" tabindex="-1"></a>XtX <span class="ot">&lt;-</span> <span class="fu">crossprod</span>( X )</span>
<span id="cb39-12"><a href="pca---análise-de-componentes-principais.html#cb39-12" tabindex="-1"></a>XXt <span class="ot">&lt;-</span> <span class="fu">tcrossprod</span>( X )</span>
<span id="cb39-13"><a href="pca---análise-de-componentes-principais.html#cb39-13" tabindex="-1"></a></span>
<span id="cb39-14"><a href="pca---análise-de-componentes-principais.html#cb39-14" tabindex="-1"></a><span class="co"># Matriz U via eigen()</span></span>
<span id="cb39-15"><a href="pca---análise-de-componentes-principais.html#cb39-15" tabindex="-1"></a>U <span class="ot">&lt;-</span> <span class="fu">eigen</span>(XXt)<span class="sc">$</span>vectors</span>
<span id="cb39-16"><a href="pca---análise-de-componentes-principais.html#cb39-16" tabindex="-1"></a>U <span class="ot">&lt;-</span> <span class="fu">abs</span>( U ) <span class="sc">*</span> <span class="fu">sign</span>( svd_decomp<span class="sc">$</span>u )</span>
<span id="cb39-17"><a href="pca---análise-de-componentes-principais.html#cb39-17" tabindex="-1"></a></span>
<span id="cb39-18"><a href="pca---análise-de-componentes-principais.html#cb39-18" tabindex="-1"></a><span class="co"># Matriz V via eigen()</span></span>
<span id="cb39-19"><a href="pca---análise-de-componentes-principais.html#cb39-19" tabindex="-1"></a>V <span class="ot">&lt;-</span> <span class="fu">eigen</span>(XtX)<span class="sc">$</span>vectors</span>
<span id="cb39-20"><a href="pca---análise-de-componentes-principais.html#cb39-20" tabindex="-1"></a>V[, <span class="dv">1</span><span class="sc">:</span>n ] <span class="ot">&lt;-</span> <span class="fu">abs</span>( V[, <span class="dv">1</span><span class="sc">:</span>n ] ) <span class="sc">*</span> <span class="fu">sign</span>( <span class="fu">t</span>( svd_decomp<span class="sc">$</span>vt ) )</span>
<span id="cb39-21"><a href="pca---análise-de-componentes-principais.html#cb39-21" tabindex="-1"></a></span>
<span id="cb39-22"><a href="pca---análise-de-componentes-principais.html#cb39-22" tabindex="-1"></a><span class="co"># Matriz Sigma via eigen()</span></span>
<span id="cb39-23"><a href="pca---análise-de-componentes-principais.html#cb39-23" tabindex="-1"></a>Sigma <span class="ot">&lt;-</span> <span class="fu">matrix</span>( <span class="dv">0</span>, n, m )</span>
<span id="cb39-24"><a href="pca---análise-de-componentes-principais.html#cb39-24" tabindex="-1"></a><span class="fu">diag</span>(Sigma) <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">round</span>(<span class="fu">eigen</span>(XtX)<span class="sc">$</span>values,<span class="dv">12</span>))[ <span class="dv">1</span><span class="sc">:</span>n ]</span>
<span id="cb39-25"><a href="pca---análise-de-componentes-principais.html#cb39-25" tabindex="-1"></a></span>
<span id="cb39-26"><a href="pca---análise-de-componentes-principais.html#cb39-26" tabindex="-1"></a><span class="co"># Compara ( U %*% Sigma %*% t(V) ) com X</span></span>
<span id="cb39-27"><a href="pca---análise-de-componentes-principais.html#cb39-27" tabindex="-1"></a><span class="fu">all.equal</span>( ( U <span class="sc">%*%</span> Sigma <span class="sc">%*%</span> <span class="fu">t</span>(V) ), X )</span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<p>Desde que o resultado final seja <strong>TRUE</strong>, fica também demonstrado numericamente a equivalência entre a Decomposição Espectral e a Decomposição SVD.</p>
<center>
△
</center>
</details>
<p> </p>
</div>
<div id="algoritmo-nipals" class="section level4 hasAnchor" number="6.5.1.3">
<h4><span class="header-section-number">6.5.1.3</span> Algoritmo NIPALS<a href="pca---análise-de-componentes-principais.html#algoritmo-nipals" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>O algoritmo NIPALS, um acrônimo de <em>Non-linear Iterative Partial Least Squares</em>, implementa a análise de componentes principais de forma direta.</p>
<p>O método foi desenvolvido como uma adaptação do Método das Potências<a href="#fn64" class="footnote-ref" id="fnref64"><sup>64</sup></a>. Para facilitar a compreensão, é importante entender sua estratégia, uma vez que o núcleo do NIPALS se baseia nesse método.</p>
<p><strong>Método das Potências</strong></p>
<p>O Método das Potências, conhecido em inglês como <em>Power Iteration</em>, <em>Power Method</em> e <em>Von Mises iteration</em><a href="#fn65" class="footnote-ref" id="fnref65"><sup>65</sup></a>, é um método numérico usado para calcular o autovetor correspondente ao maior autovalor da matriz. Sua publicação remonta a 1929.</p>
<p>O método funciona da seguinte forma:</p>
<ol style="list-style-type: decimal">
<li><strong>Inicialização</strong>: Começa com uma estimativa inicial do vetor <span class="math inline">\(\mathbf{ t_k }\)</span>.</li>
<li><strong>Iteração</strong>:</li>
</ol>
<ul>
<li>Em cada iteração, o vetor é multiplicado pela matriz <span class="math inline">\(\mathbf{X}\)</span> e, em seguida, normalizado produzindo <span class="math inline">\(\mathbf{p_k}\)</span> :</li>
</ul>
<p><span class="math display">\[
\mathbf{p_{k}} = \mathbf{ \frac{X \cdot t_k}{\|X \cdot t_k\|} }
\]</span></p>
<ul>
<li>Em seguida, produz-se uma nova estimativa de <span class="math inline">\(\mathbf{t_{k+1}}\)</span>:</li>
</ul>
<p><span class="math display">\[
\mathbf{t_{k+1}} = \mathbf{ X \cdot p_k }
\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li><strong>Convergência</strong>: O processo é repetido até que a diferença entre <span class="math inline">\(\mathbf{t_k}\)</span> e <span class="math inline">\(\mathbf{t_{k+1}}\)</span> seja insignificante, indicando que o vetor convergiu.</li>
</ol>
<p>Após convergência, <span class="math inline">\(\mathbf{t_k}\)</span> representa o autovetor correspondente ao maior autovalor da matriz <span class="math inline">\(\mathbf{X}\)</span>. O autovalor
pode ser facilmente determinado, pois corresponde à variância do autovetor.</p>
<p>Observe que, ao tornar a estrutura redutível ao máximo, fica mais fácil entender o porque o método pode ser interpretado como uma forma de regressão linear parcial (representada pela equação <span class="math inline">\(\mathbf{p} = \mathbf{X} \cdot  \mathbf{t}\)</span>).</p>
<p> </p>
<details>
<summary>
😈 + <strong>Método das Potências no R</strong>
</summary>
<p> </p>
<p>A seguir, é apresentada uma implementação didática do Método de Potências na linguagem R.</p>
<p>Os parâmetros são:</p>
<ol style="list-style-type: decimal">
<li><p><code>x</code>: matriz dos dados;</p></li>
<li><p><code>tol</code>: a tolerância para a convergência;</p></li>
</ol>
<p> </p>
<p><strong>Código</strong></p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="pca---análise-de-componentes-principais.html#cb41-1" tabindex="-1"></a>powIter <span class="ot">&lt;-</span> <span class="cf">function</span>( X, <span class="at">tol=</span><span class="fl">1e-10</span> ) {</span>
<span id="cb41-2"><a href="pca---análise-de-componentes-principais.html#cb41-2" tabindex="-1"></a>    <span class="co"># Inicializa tk com a primeira coluna de X</span></span>
<span id="cb41-3"><a href="pca---análise-de-componentes-principais.html#cb41-3" tabindex="-1"></a>    tk <span class="ot">&lt;-</span> X[, <span class="dv">1</span>, drop <span class="ot">=</span> <span class="cn">FALSE</span>]</span>
<span id="cb41-4"><a href="pca---análise-de-componentes-principais.html#cb41-4" tabindex="-1"></a>    <span class="co"># Inicializa tk1 diferente de tk para forçar a iteração</span></span>
<span id="cb41-5"><a href="pca---análise-de-componentes-principais.html#cb41-5" tabindex="-1"></a>    tk1 <span class="ot">&lt;-</span> tk <span class="sc">*</span> <span class="dv">10</span></span>
<span id="cb41-6"><a href="pca---análise-de-componentes-principais.html#cb41-6" tabindex="-1"></a></span>
<span id="cb41-7"><a href="pca---análise-de-componentes-principais.html#cb41-7" tabindex="-1"></a>    <span class="co"># Continua iterando até que a convergência seja alcançada</span></span>
<span id="cb41-8"><a href="pca---análise-de-componentes-principais.html#cb41-8" tabindex="-1"></a>    <span class="cf">while</span> ( <span class="fu">sum</span>( <span class="fu">abs</span>( tk1 <span class="sc">-</span> tk ) ) <span class="sc">&gt;</span> tol ) {</span>
<span id="cb41-9"><a href="pca---análise-de-componentes-principais.html#cb41-9" tabindex="-1"></a>        <span class="co"># Calcula o produto cruzado</span></span>
<span id="cb41-10"><a href="pca---análise-de-componentes-principais.html#cb41-10" tabindex="-1"></a>        p <span class="ot">&lt;-</span> <span class="fu">crossprod</span>( X, tk )</span>
<span id="cb41-11"><a href="pca---análise-de-componentes-principais.html#cb41-11" tabindex="-1"></a>    </span>
<span id="cb41-12"><a href="pca---análise-de-componentes-principais.html#cb41-12" tabindex="-1"></a>        <span class="co"># Normaliza o vetor p</span></span>
<span id="cb41-13"><a href="pca---análise-de-componentes-principais.html#cb41-13" tabindex="-1"></a>        p <span class="ot">&lt;-</span> p <span class="sc">/</span> <span class="fu">norm</span>( p, <span class="st">&#39;F&#39;</span> )</span>
<span id="cb41-14"><a href="pca---análise-de-componentes-principais.html#cb41-14" tabindex="-1"></a>        </span>
<span id="cb41-15"><a href="pca---análise-de-componentes-principais.html#cb41-15" tabindex="-1"></a>        <span class="co"># Atualiza tk1 e tk</span></span>
<span id="cb41-16"><a href="pca---análise-de-componentes-principais.html#cb41-16" tabindex="-1"></a>        tk1 <span class="ot">&lt;-</span> tk</span>
<span id="cb41-17"><a href="pca---análise-de-componentes-principais.html#cb41-17" tabindex="-1"></a>        tk <span class="ot">&lt;-</span> X <span class="sc">%*%</span> p</span>
<span id="cb41-18"><a href="pca---análise-de-componentes-principais.html#cb41-18" tabindex="-1"></a>    }</span>
<span id="cb41-19"><a href="pca---análise-de-componentes-principais.html#cb41-19" tabindex="-1"></a></span>
<span id="cb41-20"><a href="pca---análise-de-componentes-principais.html#cb41-20" tabindex="-1"></a>    <span class="fu">return</span>(tk)</span>
<span id="cb41-21"><a href="pca---análise-de-componentes-principais.html#cb41-21" tabindex="-1"></a>}</span></code></pre></div>
<p> </p>
<center>
△
</center>
</details>
<p> </p>
<p><strong>Entendimento do NIPALS</strong></p>
<p>Após compreender a abordagem do Método das Potências, o algoritmo NIPALS a adpta para extrair informações da primeira componente da matriz e atualizar os dados, permitindo o reinício do processo.</p>
<p>O método decompõe a matriz de dados <span class="math inline">\(\mathbf{X}\)</span> ao gerar os vetores <span class="math inline">\(\mathbf{t_i}\)</span> e <span class="math inline">\(\mathbf{p_i^T}\)</span>.</p>
<p>A partir disso, é possível obter uma matriz residual <span class="math inline">\(\mathbf{E}\)</span>, que é expressa como <span class="math inline">\(\mathbf{E} = \mathbf{X} - ( \mathbf{ t_i } \cdot \mathbf{p_i^T} )\)</span>.</p>
<p>Em seguida, essa matriz <span class="math inline">\(\mathbf{E}\)</span> é utilizada como a nova versão de <span class="math inline">\(\mathbf{X}\)</span>, permitindo que o processo seja repetido até que o número desejado de componentes seja alcançado.</p>
<p> </p>
<details>
<summary>
😈 + <strong>NIPALS no R</strong>
</summary>
<p> </p>
<p>Abaixo segue uma implementação tradicional do algoritmo NIPALS na linguagem R.
A implementação tem propósito meramente didático, podendo gerar imprecisões nos <em>loadings</em> e <em>scores</em> ao requisitar a determinação de mais que duas componentes principais em matrizes grandes.</p>
<p>A implementação tem como parâmetros:</p>
<ol style="list-style-type: decimal">
<li><p><code>x</code>: matriz dos dados;</p></li>
<li><p><code>npc</code>: o número de componentes a serem determinadas;</p></li>
<li><p><code>tol</code>: a tolerância para a convergência;</p></li>
<li><p><code>maxiter</code>: o número máximo de interações por componente;</p></li>
<li><p><code>verbose</code>: imprimir em tela o estágio da interação.</p></li>
</ol>
<p>A escolha dos parâmetros <code>tol</code> e <code>maxiter</code> tem grande influência nos resultados.</p>
<p> </p>
<p><strong>Código</strong></p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="pca---análise-de-componentes-principais.html#cb42-1" tabindex="-1"></a>nipals <span class="ot">&lt;-</span> <span class="cf">function</span>(x, <span class="at">npc =</span> <span class="dv">2</span>, <span class="at">tol =</span> <span class="fl">1e-10</span>, <span class="at">maxiter =</span> <span class="dv">500</span>, <span class="at">verbose=</span><span class="cn">TRUE</span>) {</span>
<span id="cb42-2"><a href="pca---análise-de-componentes-principais.html#cb42-2" tabindex="-1"></a>    <span class="co"># Inicializa as matrizes de loadings e scores</span></span>
<span id="cb42-3"><a href="pca---análise-de-componentes-principais.html#cb42-3" tabindex="-1"></a>    T <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> <span class="fu">nrow</span>(x), <span class="at">ncol =</span> npc)</span>
<span id="cb42-4"><a href="pca---análise-de-componentes-principais.html#cb42-4" tabindex="-1"></a>    P <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> <span class="fu">ncol</span>(x), <span class="at">ncol =</span> npc)</span>
<span id="cb42-5"><a href="pca---análise-de-componentes-principais.html#cb42-5" tabindex="-1"></a>    <span class="co"># Guarda n° de iterações</span></span>
<span id="cb42-6"><a href="pca---análise-de-componentes-principais.html#cb42-6" tabindex="-1"></a>    g <span class="ot">&lt;-</span> <span class="fu">numeric</span>(npc)</span>
<span id="cb42-7"><a href="pca---análise-de-componentes-principais.html#cb42-7" tabindex="-1"></a></span>
<span id="cb42-8"><a href="pca---análise-de-componentes-principais.html#cb42-8" tabindex="-1"></a>    <span class="cf">for</span> (iter <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>npc) {</span>
<span id="cb42-9"><a href="pca---análise-de-componentes-principais.html#cb42-9" tabindex="-1"></a>        <span class="co"># Inicializa variáveis para o loop interno</span></span>
<span id="cb42-10"><a href="pca---análise-de-componentes-principais.html#cb42-10" tabindex="-1"></a>        d <span class="ot">&lt;-</span> <span class="dv">10</span> <span class="sc">*</span> tol</span>
<span id="cb42-11"><a href="pca---análise-de-componentes-principais.html#cb42-11" tabindex="-1"></a>        niter <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb42-12"><a href="pca---análise-de-componentes-principais.html#cb42-12" tabindex="-1"></a></span>
<span id="cb42-13"><a href="pca---análise-de-componentes-principais.html#cb42-13" tabindex="-1"></a>        <span class="co"># Escolhe a coluna com maior soma de quadrados como o primeiro t</span></span>
<span id="cb42-14"><a href="pca---análise-de-componentes-principais.html#cb42-14" tabindex="-1"></a>        t <span class="ot">&lt;-</span> x[, <span class="fu">which.max</span>(<span class="fu">colSums</span>(x<span class="sc">^</span><span class="dv">2</span>)), drop <span class="ot">=</span> <span class="cn">FALSE</span>]</span>
<span id="cb42-15"><a href="pca---análise-de-componentes-principais.html#cb42-15" tabindex="-1"></a></span>
<span id="cb42-16"><a href="pca---análise-de-componentes-principais.html#cb42-16" tabindex="-1"></a>        <span class="cf">while</span> (d <span class="sc">&gt;</span> tol <span class="sc">&amp;&amp;</span> niter <span class="sc">&lt;</span> maxiter) {</span>
<span id="cb42-17"><a href="pca---análise-de-componentes-principais.html#cb42-17" tabindex="-1"></a>            <span class="co"># Calcula os loadings</span></span>
<span id="cb42-18"><a href="pca---análise-de-componentes-principais.html#cb42-18" tabindex="-1"></a>            p <span class="ot">&lt;-</span> <span class="fu">crossprod</span>(x, t)</span>
<span id="cb42-19"><a href="pca---análise-de-componentes-principais.html#cb42-19" tabindex="-1"></a>            p <span class="ot">&lt;-</span> p <span class="sc">/</span> <span class="fu">norm</span>(p, <span class="st">&#39;F&#39;</span>)  <span class="co"># Normaliza os loadings</span></span>
<span id="cb42-20"><a href="pca---análise-de-componentes-principais.html#cb42-20" tabindex="-1"></a>            </span>
<span id="cb42-21"><a href="pca---análise-de-componentes-principais.html#cb42-21" tabindex="-1"></a>            t_old <span class="ot">&lt;-</span> t</span>
<span id="cb42-22"><a href="pca---análise-de-componentes-principais.html#cb42-22" tabindex="-1"></a>            t <span class="ot">&lt;-</span> x <span class="sc">%*%</span> p <span class="co"># Atualiza os scores</span></span>
<span id="cb42-23"><a href="pca---análise-de-componentes-principais.html#cb42-23" tabindex="-1"></a>            </span>
<span id="cb42-24"><a href="pca---análise-de-componentes-principais.html#cb42-24" tabindex="-1"></a>            <span class="co"># Calcula a convergência</span></span>
<span id="cb42-25"><a href="pca---análise-de-componentes-principais.html#cb42-25" tabindex="-1"></a>            d <span class="ot">&lt;-</span> <span class="fu">sum</span>( <span class="fu">abs</span>( t_old <span class="sc">-</span> t ) )</span>
<span id="cb42-26"><a href="pca---análise-de-componentes-principais.html#cb42-26" tabindex="-1"></a>            </span>
<span id="cb42-27"><a href="pca---análise-de-componentes-principais.html#cb42-27" tabindex="-1"></a>            <span class="co"># Incrementa o contador de iterações</span></span>
<span id="cb42-28"><a href="pca---análise-de-componentes-principais.html#cb42-28" tabindex="-1"></a>            niter <span class="ot">&lt;-</span> niter <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb42-29"><a href="pca---análise-de-componentes-principais.html#cb42-29" tabindex="-1"></a>            </span>
<span id="cb42-30"><a href="pca---análise-de-componentes-principais.html#cb42-30" tabindex="-1"></a>            <span class="cf">if</span>( verbose ){ <span class="co"># Imprima info</span></span>
<span id="cb42-31"><a href="pca---análise-de-componentes-principais.html#cb42-31" tabindex="-1"></a>                <span class="fu">cat</span>(<span class="st">&quot;niter =&quot;</span>, niter, <span class="st">&quot;em pc:&quot;</span>, iter, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb42-32"><a href="pca---análise-de-componentes-principais.html#cb42-32" tabindex="-1"></a>                <span class="fu">flush.console</span>()</span>
<span id="cb42-33"><a href="pca---análise-de-componentes-principais.html#cb42-33" tabindex="-1"></a>            }</span>
<span id="cb42-34"><a href="pca---análise-de-componentes-principais.html#cb42-34" tabindex="-1"></a>        }</span>
<span id="cb42-35"><a href="pca---análise-de-componentes-principais.html#cb42-35" tabindex="-1"></a></span>
<span id="cb42-36"><a href="pca---análise-de-componentes-principais.html#cb42-36" tabindex="-1"></a>        <span class="co"># Atualiza X e armazena os resultados</span></span>
<span id="cb42-37"><a href="pca---análise-de-componentes-principais.html#cb42-37" tabindex="-1"></a>        x <span class="ot">&lt;-</span> x <span class="sc">-</span> <span class="fu">tcrossprod</span>(t, p)</span>
<span id="cb42-38"><a href="pca---análise-de-componentes-principais.html#cb42-38" tabindex="-1"></a>        T[, iter] <span class="ot">&lt;-</span> t</span>
<span id="cb42-39"><a href="pca---análise-de-componentes-principais.html#cb42-39" tabindex="-1"></a>        P[, iter] <span class="ot">&lt;-</span> p</span>
<span id="cb42-40"><a href="pca---análise-de-componentes-principais.html#cb42-40" tabindex="-1"></a>        g[iter] <span class="ot">&lt;-</span> niter</span>
<span id="cb42-41"><a href="pca---análise-de-componentes-principais.html#cb42-41" tabindex="-1"></a>    }</span>
<span id="cb42-42"><a href="pca---análise-de-componentes-principais.html#cb42-42" tabindex="-1"></a></span>
<span id="cb42-43"><a href="pca---análise-de-componentes-principais.html#cb42-43" tabindex="-1"></a>    <span class="co"># Retorna os resultados em uma lista</span></span>
<span id="cb42-44"><a href="pca---análise-de-componentes-principais.html#cb42-44" tabindex="-1"></a>    <span class="fu">return</span>(<span class="fu">list</span>(</span>
<span id="cb42-45"><a href="pca---análise-de-componentes-principais.html#cb42-45" tabindex="-1"></a>        <span class="at">T =</span> T,</span>
<span id="cb42-46"><a href="pca---análise-de-componentes-principais.html#cb42-46" tabindex="-1"></a>        <span class="at">P =</span> P,</span>
<span id="cb42-47"><a href="pca---análise-de-componentes-principais.html#cb42-47" tabindex="-1"></a>        <span class="at">VarExpl =</span> <span class="fu">colSums</span>(T<span class="sc">^</span><span class="dv">2</span>) <span class="sc">/</span> <span class="fu">sum</span>(x<span class="sc">^</span><span class="dv">2</span>),</span>
<span id="cb42-48"><a href="pca---análise-de-componentes-principais.html#cb42-48" tabindex="-1"></a>        <span class="at">g =</span> g,</span>
<span id="cb42-49"><a href="pca---análise-de-componentes-principais.html#cb42-49" tabindex="-1"></a>        <span class="at">tol =</span> tol,</span>
<span id="cb42-50"><a href="pca---análise-de-componentes-principais.html#cb42-50" tabindex="-1"></a>        <span class="at">maxiter =</span> maxiter</span>
<span id="cb42-51"><a href="pca---análise-de-componentes-principais.html#cb42-51" tabindex="-1"></a>    ))</span>
<span id="cb42-52"><a href="pca---análise-de-componentes-principais.html#cb42-52" tabindex="-1"></a>}</span></code></pre></div>
<p> </p>
<center>
△
</center>
</details>
<p> </p>
<p>Um ponto de destaque do algoritmo NIPALS é permitir a determinação das componentes principais na ordem decrescente como critério obrigatório. O mesmo não é possível nos dois primeiros métodos discutidos anteriormente. Neles, todas as possíveis componentes devem ser obtidas e somente após está obtenção é que se procede a reordenação das mesmas.</p>
<p>Dito isso, uma característica importante do NIPALS é poder, a priori, determinar o número de componentes a serem calculadas bem como determinar qual é a variância que se quer representar e assumida satisfatória. Desde modo, por fim, tempo de cálculo computacional pode ser poupado.</p>
<p>Outro ponto, é que o algoritmo NIPALS permite extrair componentes principais sem necessidade de imputar valores ausentes previamente (desde de que devidamente implementado) sendo uma vantagem significativa em relação a outros métodos que não lidam bem com dados faltantes. Como desvantagem, é conhecido que o algoritmo tem lenta convergência.</p>
<p>No R, não há função nativa mas há o método em alguns pacotes. Em especial, vale mencionar o pacote <code>nipals</code> que implementa o algoritmo com opção padrão da execução da ortogonalização de Gram-Schmidt em cada iteração. Isso é necessário pois, devido ao acúmulo de erros de ponto flutuante, a ortogonalidade dos componentes principais é rapidamente perdida à medida que o número de componentes aumenta. A versão modificada do NIPALS, NIPALS-GS<a href="#fn66" class="footnote-ref" id="fnref66"><sup>66</sup></a>, com o processo de Gram-Schmidt, re-ortogonaliza tanto os <em>scores</em> quanto os <em>loadings</em> de forma a estabilizá-los. Com a versão tradicional do NIPALS apenas as primeiras componentes devem ser consideradas e de matrizes não muito grandes.</p>
<p> </p>
<ol start="4" style="list-style-type: decimal">
<li><strong>PCoA - Principal Coordinates Analysis</strong></li>
</ol>
<p> </p>
<ul>
<li><p>PCA e PCoA se equivalem quando a distância a ser preservada é a euclidiana.</p></li>
<li><p>Atualmente, o PCoA é mais comumente conhecido como Escalonamento Multidimensional Clássico<a href="#fn67" class="footnote-ref" id="fnref67"><sup>67</sup></a> (<em>Classical Multidimensional Scaling</em>)</p></li>
</ul>
<p> </p>
<p><strong>TODO (texto a ser desenvolvido)</strong></p>
<p> </p>
</div>
</div>
</div>
<div id="variância-por-pc" class="section level2 hasAnchor" number="6.6">
<h2><span class="header-section-number">6.6</span> Variância por PC<a href="pca---análise-de-componentes-principais.html#variância-por-pc" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p> </p>
<p> </p>
<p> </p>
<p>Considerando o exemplo ilustrado na figura <a href="pca---análise-de-componentes-principais.html#fig:pca-rot">6.1</a></p>
<pre><code>&gt;
&gt; 1.830106 / ( 1.830106 + 0.169894 )
[1] 0.915053
&gt; 0.169894 / ( 1.830106 + 0.169894 )
[1] 0.084947
&gt;</code></pre>
<p> </p>
<p> </p>
<p> </p>
<p><a href="https://pt.wikipedia.org/wiki/Matrizes_semelhantes" class="uri">https://pt.wikipedia.org/wiki/Matrizes_semelhantes</a></p>
</div>
<div id="aplicação-típica" class="section level2 hasAnchor" number="6.7">
<h2><span class="header-section-number">6.7</span> Aplicação Típica<a href="pca---análise-de-componentes-principais.html#aplicação-típica" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<!--
1) O que é SVD?
2) Mostre o passo a passo para se fazer essa decomposição - SVD -  de uma matriz 2x2. Exemplifique e explique as etapas.
3) Resolva o exemplo anterior usando o método de Jacobi.
4) Do exemplo anterior, resolução de SVD pelo método de Jacobi, faça um função em R puro, para esse exemplo.
5) Forneça a matrix usada anterior para aplicar a função anterior em código R
-->
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<details>
<summary>
😈Algoritmo para SVD
</summary>
<p><br><strong>O algoritmo apresentado abaixo é apenas uma possibilidade dentre vários</strong><br><br></p>
<p>O método de Jacobi é uma abordagem iterativa que pode ser aplicada à SVD, embora não seja a mais eficiente para matrizes grandes. É apresentado para incentivar o conhecimento de se ter ao menos uma alternativa de resolução do problema com apenas conceitos básicos de álgebra. Ou seja, sem o uso de funções prontas e abordagem <strong>caixa-preta</strong>.</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="pca---análise-de-componentes-principais.html#cb44-1" tabindex="-1"></a><span class="co"># Função para calcular SVD usando o método de Jacobi</span></span>
<span id="cb44-2"><a href="pca---análise-de-componentes-principais.html#cb44-2" tabindex="-1"></a>jacobi_svd <span class="ot">&lt;-</span> <span class="cf">function</span>(A, <span class="at">tol =</span> <span class="fl">1e-10</span>, <span class="at">max_iter =</span> <span class="dv">100</span>) {</span>
<span id="cb44-3"><a href="pca---análise-de-componentes-principais.html#cb44-3" tabindex="-1"></a>  <span class="co"># Inicialização</span></span>
<span id="cb44-4"><a href="pca---análise-de-componentes-principais.html#cb44-4" tabindex="-1"></a>  n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(A)</span>
<span id="cb44-5"><a href="pca---análise-de-componentes-principais.html#cb44-5" tabindex="-1"></a>  m <span class="ot">&lt;-</span> <span class="fu">ncol</span>(A)</span>
<span id="cb44-6"><a href="pca---análise-de-componentes-principais.html#cb44-6" tabindex="-1"></a>  U <span class="ot">&lt;-</span> <span class="fu">diag</span>(n)</span>
<span id="cb44-7"><a href="pca---análise-de-componentes-principais.html#cb44-7" tabindex="-1"></a>  V <span class="ot">&lt;-</span> <span class="fu">diag</span>(m)</span>
<span id="cb44-8"><a href="pca---análise-de-componentes-principais.html#cb44-8" tabindex="-1"></a>  </span>
<span id="cb44-9"><a href="pca---análise-de-componentes-principais.html#cb44-9" tabindex="-1"></a>  <span class="co"># Calcular A^T A</span></span>
<span id="cb44-10"><a href="pca---análise-de-componentes-principais.html#cb44-10" tabindex="-1"></a>  ATA <span class="ot">&lt;-</span> <span class="fu">t</span>(A) <span class="sc">%*%</span> A</span>
<span id="cb44-11"><a href="pca---análise-de-componentes-principais.html#cb44-11" tabindex="-1"></a>  </span>
<span id="cb44-12"><a href="pca---análise-de-componentes-principais.html#cb44-12" tabindex="-1"></a>  <span class="co"># Iterações de Jacobi</span></span>
<span id="cb44-13"><a href="pca---análise-de-componentes-principais.html#cb44-13" tabindex="-1"></a>  <span class="cf">for</span> (iter <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>max_iter) {</span>
<span id="cb44-14"><a href="pca---análise-de-componentes-principais.html#cb44-14" tabindex="-1"></a>    <span class="co"># Encontrar o maior elemento fora da diagonal</span></span>
<span id="cb44-15"><a href="pca---análise-de-componentes-principais.html#cb44-15" tabindex="-1"></a>    off_diag <span class="ot">&lt;-</span> <span class="fu">which</span>(<span class="fu">abs</span>(ATA) <span class="sc">&gt;</span> tol, <span class="at">arr.ind =</span> <span class="cn">TRUE</span>)</span>
<span id="cb44-16"><a href="pca---análise-de-componentes-principais.html#cb44-16" tabindex="-1"></a>    off_diag <span class="ot">&lt;-</span> off_diag[off_diag[, <span class="dv">1</span>] <span class="sc">!=</span> off_diag[, <span class="dv">2</span>], ]</span>
<span id="cb44-17"><a href="pca---análise-de-componentes-principais.html#cb44-17" tabindex="-1"></a>    </span>
<span id="cb44-18"><a href="pca---análise-de-componentes-principais.html#cb44-18" tabindex="-1"></a>    <span class="cf">if</span> (<span class="fu">nrow</span>(off_diag) <span class="sc">==</span> <span class="dv">0</span>) {</span>
<span id="cb44-19"><a href="pca---análise-de-componentes-principais.html#cb44-19" tabindex="-1"></a>      <span class="cf">break</span>  <span class="co"># Convergido</span></span>
<span id="cb44-20"><a href="pca---análise-de-componentes-principais.html#cb44-20" tabindex="-1"></a>    }</span>
<span id="cb44-21"><a href="pca---análise-de-componentes-principais.html#cb44-21" tabindex="-1"></a>    </span>
<span id="cb44-22"><a href="pca---análise-de-componentes-principais.html#cb44-22" tabindex="-1"></a>    <span class="co"># Selecionar o primeiro elemento fora da diagonal</span></span>
<span id="cb44-23"><a href="pca---análise-de-componentes-principais.html#cb44-23" tabindex="-1"></a>    p <span class="ot">&lt;-</span> off_diag[<span class="dv">1</span>, <span class="dv">1</span>]</span>
<span id="cb44-24"><a href="pca---análise-de-componentes-principais.html#cb44-24" tabindex="-1"></a>    q <span class="ot">&lt;-</span> off_diag[<span class="dv">1</span>, <span class="dv">2</span>]</span>
<span id="cb44-25"><a href="pca---análise-de-componentes-principais.html#cb44-25" tabindex="-1"></a>    </span>
<span id="cb44-26"><a href="pca---análise-de-componentes-principais.html#cb44-26" tabindex="-1"></a>    <span class="co"># Cálculo dos ângulos de rotação</span></span>
<span id="cb44-27"><a href="pca---análise-de-componentes-principais.html#cb44-27" tabindex="-1"></a>    <span class="cf">if</span> (ATA[p, p] <span class="sc">==</span> ATA[q, q]) {</span>
<span id="cb44-28"><a href="pca---análise-de-componentes-principais.html#cb44-28" tabindex="-1"></a>      theta <span class="ot">&lt;-</span> pi <span class="sc">/</span> <span class="dv">4</span></span>
<span id="cb44-29"><a href="pca---análise-de-componentes-principais.html#cb44-29" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb44-30"><a href="pca---análise-de-componentes-principais.html#cb44-30" tabindex="-1"></a>      theta <span class="ot">&lt;-</span> <span class="fu">atan</span>(<span class="dv">2</span> <span class="sc">*</span> ATA[p, q] <span class="sc">/</span> (ATA[p, p] <span class="sc">-</span> ATA[q, q])) <span class="sc">/</span> <span class="dv">2</span></span>
<span id="cb44-31"><a href="pca---análise-de-componentes-principais.html#cb44-31" tabindex="-1"></a>    }</span>
<span id="cb44-32"><a href="pca---análise-de-componentes-principais.html#cb44-32" tabindex="-1"></a>    </span>
<span id="cb44-33"><a href="pca---análise-de-componentes-principais.html#cb44-33" tabindex="-1"></a>    <span class="co"># Matriz de rotação</span></span>
<span id="cb44-34"><a href="pca---análise-de-componentes-principais.html#cb44-34" tabindex="-1"></a>    c <span class="ot">&lt;-</span> <span class="fu">cos</span>(theta)</span>
<span id="cb44-35"><a href="pca---análise-de-componentes-principais.html#cb44-35" tabindex="-1"></a>    s <span class="ot">&lt;-</span> <span class="fu">sin</span>(theta)</span>
<span id="cb44-36"><a href="pca---análise-de-componentes-principais.html#cb44-36" tabindex="-1"></a>    </span>
<span id="cb44-37"><a href="pca---análise-de-componentes-principais.html#cb44-37" tabindex="-1"></a>    <span class="co"># Atualizar ATA</span></span>
<span id="cb44-38"><a href="pca---análise-de-componentes-principais.html#cb44-38" tabindex="-1"></a>    R <span class="ot">&lt;-</span> <span class="fu">diag</span>(n)</span>
<span id="cb44-39"><a href="pca---análise-de-componentes-principais.html#cb44-39" tabindex="-1"></a>    R[<span class="fu">c</span>(p, q), <span class="fu">c</span>(p, q)] <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(c, <span class="sc">-</span>s, s, c), <span class="at">nrow =</span> <span class="dv">2</span>)</span>
<span id="cb44-40"><a href="pca---análise-de-componentes-principais.html#cb44-40" tabindex="-1"></a>    </span>
<span id="cb44-41"><a href="pca---análise-de-componentes-principais.html#cb44-41" tabindex="-1"></a>    ATA <span class="ot">&lt;-</span> <span class="fu">t</span>(R) <span class="sc">%*%</span> ATA <span class="sc">%*%</span> R</span>
<span id="cb44-42"><a href="pca---análise-de-componentes-principais.html#cb44-42" tabindex="-1"></a>    V <span class="ot">&lt;-</span> V <span class="sc">%*%</span> R</span>
<span id="cb44-43"><a href="pca---análise-de-componentes-principais.html#cb44-43" tabindex="-1"></a>    </span>
<span id="cb44-44"><a href="pca---análise-de-componentes-principais.html#cb44-44" tabindex="-1"></a>    <span class="co"># Calcular os autovalores</span></span>
<span id="cb44-45"><a href="pca---análise-de-componentes-principais.html#cb44-45" tabindex="-1"></a>    <span class="cf">if</span> (<span class="fu">all</span>(<span class="fu">abs</span>(ATA[off_diag]) <span class="sc">&lt;</span> tol)) {</span>
<span id="cb44-46"><a href="pca---análise-de-componentes-principais.html#cb44-46" tabindex="-1"></a>      <span class="cf">break</span></span>
<span id="cb44-47"><a href="pca---análise-de-componentes-principais.html#cb44-47" tabindex="-1"></a>    }</span>
<span id="cb44-48"><a href="pca---análise-de-componentes-principais.html#cb44-48" tabindex="-1"></a>  }</span>
<span id="cb44-49"><a href="pca---análise-de-componentes-principais.html#cb44-49" tabindex="-1"></a>  </span>
<span id="cb44-50"><a href="pca---análise-de-componentes-principais.html#cb44-50" tabindex="-1"></a>  <span class="co"># Valores singulares</span></span>
<span id="cb44-51"><a href="pca---análise-de-componentes-principais.html#cb44-51" tabindex="-1"></a>  singular_values <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">diag</span>(ATA))</span>
<span id="cb44-52"><a href="pca---análise-de-componentes-principais.html#cb44-52" tabindex="-1"></a>  </span>
<span id="cb44-53"><a href="pca---análise-de-componentes-principais.html#cb44-53" tabindex="-1"></a>  <span class="co"># Calcular U</span></span>
<span id="cb44-54"><a href="pca---análise-de-componentes-principais.html#cb44-54" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n) {</span>
<span id="cb44-55"><a href="pca---análise-de-componentes-principais.html#cb44-55" tabindex="-1"></a>    <span class="cf">if</span> (singular_values[i] <span class="sc">&gt;</span> <span class="dv">0</span>) {</span>
<span id="cb44-56"><a href="pca---análise-de-componentes-principais.html#cb44-56" tabindex="-1"></a>      U[, i] <span class="ot">&lt;-</span> A <span class="sc">%*%</span> V[, i] <span class="sc">/</span> singular_values[i]</span>
<span id="cb44-57"><a href="pca---análise-de-componentes-principais.html#cb44-57" tabindex="-1"></a>    }</span>
<span id="cb44-58"><a href="pca---análise-de-componentes-principais.html#cb44-58" tabindex="-1"></a>  }</span>
<span id="cb44-59"><a href="pca---análise-de-componentes-principais.html#cb44-59" tabindex="-1"></a>  </span>
<span id="cb44-60"><a href="pca---análise-de-componentes-principais.html#cb44-60" tabindex="-1"></a>  <span class="co"># Retornar os resultados</span></span>
<span id="cb44-61"><a href="pca---análise-de-componentes-principais.html#cb44-61" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">U =</span> U, <span class="at">D =</span> <span class="fu">diag</span>(singular_values), <span class="at">V =</span> V))</span>
<span id="cb44-62"><a href="pca---análise-de-componentes-principais.html#cb44-62" tabindex="-1"></a>}</span>
<span id="cb44-63"><a href="pca---análise-de-componentes-principais.html#cb44-63" tabindex="-1"></a></span>
<span id="cb44-64"><a href="pca---análise-de-componentes-principais.html#cb44-64" tabindex="-1"></a><span class="co"># Matriz exemplo</span></span>
<span id="cb44-65"><a href="pca---análise-de-componentes-principais.html#cb44-65" tabindex="-1"></a>A <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">4</span>, <span class="dv">0</span>, <span class="dv">3</span>, <span class="sc">-</span><span class="dv">5</span>), <span class="at">nrow =</span> <span class="dv">2</span>, <span class="at">byrow =</span> <span class="cn">TRUE</span>)</span>
<span id="cb44-66"><a href="pca---análise-de-componentes-principais.html#cb44-66" tabindex="-1"></a></span>
<span id="cb44-67"><a href="pca---análise-de-componentes-principais.html#cb44-67" tabindex="-1"></a><span class="co"># Chamar a função</span></span>
<span id="cb44-68"><a href="pca---análise-de-componentes-principais.html#cb44-68" tabindex="-1"></a>result <span class="ot">&lt;-</span> <span class="fu">jacobi_svd</span>(A)</span>
<span id="cb44-69"><a href="pca---análise-de-componentes-principais.html#cb44-69" tabindex="-1"></a></span>
<span id="cb44-70"><a href="pca---análise-de-componentes-principais.html#cb44-70" tabindex="-1"></a><span class="co"># Resultados</span></span>
<span id="cb44-71"><a href="pca---análise-de-componentes-principais.html#cb44-71" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Matriz U:</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## Matriz U:</code></pre>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="pca---análise-de-componentes-principais.html#cb46-1" tabindex="-1"></a><span class="fu">print</span>(result<span class="sc">$</span>U)</span></code></pre></div>
<pre><code>##            [,1]       [,2]
## [1,]  0.8944272 -0.4472136
## [2,] -0.4472136 -0.8944272</code></pre>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="pca---análise-de-componentes-principais.html#cb48-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;</span><span class="sc">\n</span><span class="st">Matriz D (valores singulares):</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## 
## Matriz D (valores singulares):</code></pre>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="pca---análise-de-componentes-principais.html#cb50-1" tabindex="-1"></a><span class="fu">print</span>(result<span class="sc">$</span>D)</span></code></pre></div>
<pre><code>##          [,1]     [,2]
## [1,] 3.162278 0.000000
## [2,] 0.000000 6.324555</code></pre>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="pca---análise-de-componentes-principais.html#cb52-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;</span><span class="sc">\n</span><span class="st">Matriz V:</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## 
## Matriz V:</code></pre>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="pca---análise-de-componentes-principais.html#cb54-1" tabindex="-1"></a><span class="fu">print</span>(result<span class="sc">$</span>V)</span></code></pre></div>
<pre><code>##           [,1]       [,2]
## [1,] 0.7071068 -0.7071068
## [2,] 0.7071068  0.7071068</code></pre>
<p><strong>Algoritmos para SVD</strong></p>
<p>Existem vários algoritmos para calcular a SVD, incluindo:</p>
<ul>
<li>Método de Jacobi: Uma abordagem iterativa, mas geralmente mais lenta para matrizes grandes.</li>
<li>Algoritmo de Golub-Reinsch: Um método mais eficiente que combina a decomposição QR e o método de Jacobi.</li>
<li>Algoritmo de Lanczos: Usado para matrizes grandes e esparsas; baseia-se em métodos de subespaço.</li>
<li>SVD baseado em bidiagonalização: Usa decomposições bidiagonais, seguido por uma SVD da matriz bidiagonal.</li>
<li>Algoritmos de fatoração de matrizes de baixa-rank: Técnicas como o método de potência e métodos estocásticos.</li>
</ul>
<p><strong>Eficiência dos Algoritmos</strong></p>
<p>O algoritmo de Golub-Reinsch é um dos mais utilizados na prática e considerado eficiente para a maioria das aplicações. Para matrizes muito grandes ou esparsas, métodos baseados em Lanczos ou bidiagonalização tendem a ser mais eficientes.</p>
<p>A escolha do algoritmo SVD mais eficiente depende do tamanho e da estrutura da matriz que você está tratando, bem como das necessidades específicas da aplicação.</p>
<p>Em bibliotecas computacionais otimizadas, tais como <em>BLAS</em><a href="#fn68" class="footnote-ref" id="fnref68"><sup>68</sup></a>, <em>LAPACK</em><a href="#fn69" class="footnote-ref" id="fnref69"><sup>69</sup></a>, <em>MKL</em><a href="#fn70" class="footnote-ref" id="fnref70"><sup>70</sup></a> em geral as funções implementadas trazem esses diversos algoritmos e alguma análise inicial da matriz determina o algoritmo mais adequado para a situação específica.</p>
<center>
△
</center>
</details>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="46">
<li id="fn46"><p><a href="https://pt.wikipedia.org/wiki/An%C3%A1lise_de_componentes_principais" class="uri">https://pt.wikipedia.org/wiki/An%C3%A1lise_de_componentes_principais</a><a href="pca---análise-de-componentes-principais.html#fnref46" class="footnote-back">↩︎</a></p></li>
<li id="fn47"><p><a href="https://pt.wikipedia.org/wiki/Karl_Pearson" class="uri">https://pt.wikipedia.org/wiki/Karl_Pearson</a><a href="pca---análise-de-componentes-principais.html#fnref47" class="footnote-back">↩︎</a></p></li>
<li id="fn48"><p>Pearson, K. (1901). LIII. <em>On lines and planes of closest fit to systems of points in space</em>,
<strong>The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science</strong>,
2(11), 559–572. <a href="https://doi.org/10.1080/14786440109462720" class="uri">https://doi.org/10.1080/14786440109462720</a><a href="pca---análise-de-componentes-principais.html#fnref48" class="footnote-back">↩︎</a></p></li>
<li id="fn49"><p><a href="https://pt.wikipedia.org/wiki/Harold_Hotelling" class="uri">https://pt.wikipedia.org/wiki/Harold_Hotelling</a><a href="pca---análise-de-componentes-principais.html#fnref49" class="footnote-back">↩︎</a></p></li>
<li id="fn50"><p>Hotelling, H. (1933). <em>Analysis of a complex of statistical variables into principal components</em>, <strong>Journal of Educational Psychology</strong>, 24(6), 417–441. <a href="https://doi.org/10.1037/h0071325" class="uri">https://doi.org/10.1037/h0071325</a><a href="pca---análise-de-componentes-principais.html#fnref50" class="footnote-back">↩︎</a></p></li>
<li id="fn51"><p><a href="https://pt.wikipedia.org/wiki/Teorema_espectral" class="uri">https://pt.wikipedia.org/wiki/Teorema_espectral</a><a href="pca---análise-de-componentes-principais.html#fnref51" class="footnote-back">↩︎</a></p></li>
<li id="fn52"><p><a href="https://en.wikipedia.org/wiki/Spectral_theorem" class="uri">https://en.wikipedia.org/wiki/Spectral_theorem</a><a href="pca---análise-de-componentes-principais.html#fnref52" class="footnote-back">↩︎</a></p></li>
<li id="fn53"><p><a href="https://pt.wikipedia.org/wiki/Autovalores_e_autovetores" class="uri">https://pt.wikipedia.org/wiki/Autovalores_e_autovetores</a><a href="pca---análise-de-componentes-principais.html#fnref53" class="footnote-back">↩︎</a></p></li>
<li id="fn54"><p><a href="https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors" class="uri">https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors</a><a href="pca---análise-de-componentes-principais.html#fnref54" class="footnote-back">↩︎</a></p></li>
<li id="fn55"><p><a href="https://pt.wikipedia.org/wiki/Covari%C3%A2ncia" class="uri">https://pt.wikipedia.org/wiki/Covari%C3%A2ncia</a><a href="pca---análise-de-componentes-principais.html#fnref55" class="footnote-back">↩︎</a></p></li>
<li id="fn56"><p><a href="https://pt.wikipedia.org/wiki/V%C3%ADrgula_flutuante" class="uri">https://pt.wikipedia.org/wiki/V%C3%ADrgula_flutuante</a><a href="pca---análise-de-componentes-principais.html#fnref56" class="footnote-back">↩︎</a></p></li>
<li id="fn57"><p><a href="https://pt.wikipedia.org/wiki/Determinante" class="uri">https://pt.wikipedia.org/wiki/Determinante</a><a href="pca---análise-de-componentes-principais.html#fnref57" class="footnote-back">↩︎</a></p></li>
<li id="fn58"><p><a href="https://pt.wikipedia.org/wiki/Matriz_diagonaliz%C3%A1vel" class="uri">https://pt.wikipedia.org/wiki/Matriz_diagonaliz%C3%A1vel</a><a href="pca---análise-de-componentes-principais.html#fnref58" class="footnote-back">↩︎</a></p></li>
<li id="fn59"><p><a href="https://pt.wikipedia.org/wiki/Polin%C3%B4mio_caracter%C3%ADstico" class="uri">https://pt.wikipedia.org/wiki/Polin%C3%B4mio_caracter%C3%ADstico</a><a href="pca---análise-de-componentes-principais.html#fnref59" class="footnote-back">↩︎</a></p></li>
<li id="fn60"><p><a href="https://pt.wikipedia.org/wiki/F%C3%B3rmula_quadr%C3%A1tica" class="uri">https://pt.wikipedia.org/wiki/F%C3%B3rmula_quadr%C3%A1tica</a><a href="pca---análise-de-componentes-principais.html#fnref60" class="footnote-back">↩︎</a></p></li>
<li id="fn61"><p><a href="https://pt.wikipedia.org/wiki/Norma_(matem%C3%A1tica)" class="uri">https://pt.wikipedia.org/wiki/Norma_(matem%C3%A1tica)</a><a href="pca---análise-de-componentes-principais.html#fnref61" class="footnote-back">↩︎</a></p></li>
<li id="fn62"><p><a href="https://pt.wikipedia.org/wiki/Decomposi%C3%A7%C3%A3o_em_valores_singulares" class="uri">https://pt.wikipedia.org/wiki/Decomposi%C3%A7%C3%A3o_em_valores_singulares</a><a href="pca---análise-de-componentes-principais.html#fnref62" class="footnote-back">↩︎</a></p></li>
<li id="fn63"><p><a href="https://pt.wikipedia.org/wiki/Ortonormalidade" class="uri">https://pt.wikipedia.org/wiki/Ortonormalidade</a><a href="pca---análise-de-componentes-principais.html#fnref63" class="footnote-back">↩︎</a></p></li>
<li id="fn64"><p><a href="https://pt.wikipedia.org/wiki/M%C3%A9todo_das_pot%C3%AAncias" class="uri">https://pt.wikipedia.org/wiki/M%C3%A9todo_das_pot%C3%AAncias</a><a href="pca---análise-de-componentes-principais.html#fnref64" class="footnote-back">↩︎</a></p></li>
<li id="fn65"><p>Richard von Mises and H. Pollaczek-Geiringer, <em>Praktische Verfahren der Gleichungsauflösung</em>, <strong>ZAMM - Zeitschrift für Angewandte Mathematik und Mechanik</strong>, 9, 152-164 (1929).<a href="pca---análise-de-componentes-principais.html#fnref65" class="footnote-back">↩︎</a></p></li>
<li id="fn66"><p>M. Andrecut, <em>Parallel GPU Implementation of Iterative PCA Algorithms</em>,
<strong>Journal of Computational Biology</strong>, 2009 16:11, 1593-1599,
<a href="https://www.liebertpub.com/doi/10.1089/cmb.2008.0221" class="uri">https://www.liebertpub.com/doi/10.1089/cmb.2008.0221</a><a href="pca---análise-de-componentes-principais.html#fnref66" class="footnote-back">↩︎</a></p></li>
<li id="fn67"><p><a href="https://en.wikipedia.org/wiki/Multidimensional_scaling" class="uri">https://en.wikipedia.org/wiki/Multidimensional_scaling</a><a href="pca---análise-de-componentes-principais.html#fnref67" class="footnote-back">↩︎</a></p></li>
<li id="fn68"><p><a href="https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms" class="uri">https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms</a><a href="pca---análise-de-componentes-principais.html#fnref68" class="footnote-back">↩︎</a></p></li>
<li id="fn69"><p><a href="https://en.wikipedia.org/wiki/LAPACK" class="uri">https://en.wikipedia.org/wiki/LAPACK</a><a href="pca---análise-de-componentes-principais.html#fnref69" class="footnote-back">↩︎</a></p></li>
<li id="fn70"><p><a href="https://en.wikipedia.org/wiki/Math_Kernel_Library" class="uri">https://en.wikipedia.org/wiki/Math_Kernel_Library</a><a href="pca---análise-de-componentes-principais.html#fnref70" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="parte-iii---análise-exploratória-de-dados.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="hca---análise-de-agrupamento-hierárquico.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsubsection",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"info": true,
"post_processor": null
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
