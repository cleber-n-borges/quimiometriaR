<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>&#8474;&#8477; 6 PCA - An√°lise de Componentes Principais | Caderno de Quimiometria com R</title>
  <meta name="description" content="Este caderno foi criado para ser um guia acess√≠vel para todos que desejam explorar e aplicar conceitos de quimiometria utilizando a linguagem de programa√ß√£o R. Foi desenvolvido com suporte do ChatGPT." />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="&#8474;&#8477; 6 PCA - An√°lise de Componentes Principais | Caderno de Quimiometria com R" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://cleber-n-borges.github.io/quimiometriaR/img/cover.jpg" />
  <meta property="og:description" content="Este caderno foi criado para ser um guia acess√≠vel para todos que desejam explorar e aplicar conceitos de quimiometria utilizando a linguagem de programa√ß√£o R. Foi desenvolvido com suporte do ChatGPT." />
  <meta name="github-repo" content="cleber-n-borges/quimiometriaR" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="&#8474;&#8477; 6 PCA - An√°lise de Componentes Principais | Caderno de Quimiometria com R" />
  
  <meta name="twitter:description" content="Este caderno foi criado para ser um guia acess√≠vel para todos que desejam explorar e aplicar conceitos de quimiometria utilizando a linguagem de programa√ß√£o R. Foi desenvolvido com suporte do ChatGPT." />
  <meta name="twitter:image" content="https://cleber-n-borges.github.io/quimiometriaR/img/cover.jpg" />




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="favicon.svg" type="image/x-icon" />
<link rel="prev" href="parte-iii---an√°lise-explorat√≥ria-de-dados.html"/>
<link rel="next" href="hca---an√°lise-de-agrupamento-hier√°rquico.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.33/datatables.js"></script>
<link href="libs/dt-core-1.13.6/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.13.6/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.13.6/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.1/js/crosstalk.min.js"></script>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-YJVXYW1C58"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-YJVXYW1C58');
</script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">&#8474;&#8477; Quimiometria com R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Bem-vindos!</a></li>
<li class="chapter" data-level="" data-path="√∫til-para-voc√™.html"><a href="√∫til-para-voc√™.html"><i class="fa fa-check"></i>√ötil para Voc√™!</a></li>
<li class="chapter" data-level="" data-path="pergunte-ao-chatgpt.html"><a href="pergunte-ao-chatgpt.html"><i class="fa fa-check"></i>Pergunte ao ChatGPT</a></li>
<li class="chapter" data-level="" data-path="estrutura-do-caderno.html"><a href="estrutura-do-caderno.html"><i class="fa fa-check"></i>Estrutura do Caderno</a></li>
<li class="chapter" data-level="" data-path="parte-i---ponto-de-partida.html"><a href="parte-i---ponto-de-partida.html"><i class="fa fa-check"></i>PARTE I - Ponto de Partida</a>
<ul>
<li class="chapter" data-level="" data-path="parte-i---ponto-de-partida.html"><a href="parte-i---ponto-de-partida.html#primeiro-passo"><i class="fa fa-check"></i>Primeiro Passo</a></li>
<li class="chapter" data-level="" data-path="parte-i---ponto-de-partida.html"><a href="parte-i---ponto-de-partida.html#o-que-√©-quimiometria"><i class="fa fa-check"></i>O que √© Quimiometria?</a></li>
<li class="chapter" data-level="" data-path="parte-i---ponto-de-partida.html"><a href="parte-i---ponto-de-partida.html#aplica√ß√µes-do-r-na-quimiometria"><i class="fa fa-check"></i>Aplica√ß√µes do R na Quimiometria</a></li>
<li class="chapter" data-level="" data-path="parte-i---ponto-de-partida.html"><a href="parte-i---ponto-de-partida.html#contribui√ß√µes-do-chatgpt"><i class="fa fa-check"></i>Contribui√ß√µes do ChatGPT</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="comece-a-usar-o-r.html"><a href="comece-a-usar-o-r.html"><i class="fa fa-check"></i><b>1</b> Comece a Usar o R</a>
<ul>
<li class="chapter" data-level="1.1" data-path="comece-a-usar-o-r.html"><a href="comece-a-usar-o-r.html#instale-o-r-e-rstudio"><i class="fa fa-check"></i><b>1.1</b> Instale o R e RStudio</a></li>
<li class="chapter" data-level="1.2" data-path="comece-a-usar-o-r.html"><a href="comece-a-usar-o-r.html#conhe√ßa-o-b√°sico-do-r"><i class="fa fa-check"></i><b>1.2</b> Conhe√ßa o B√°sico do R</a></li>
<li class="chapter" data-level="1.3" data-path="comece-a-usar-o-r.html"><a href="comece-a-usar-o-r.html#realize-an√°lises-simples"><i class="fa fa-check"></i><b>1.3</b> Realize An√°lises Simples</a></li>
<li class="chapter" data-level="1.4" data-path="comece-a-usar-o-r.html"><a href="comece-a-usar-o-r.html#a-conveni√™ncia-do-help"><i class="fa fa-check"></i><b>1.4</b> A Conveni√™ncia do <code>help()</code></a>
<ul>
<li class="chapter" data-level="" data-path="comece-a-usar-o-r.html"><a href="comece-a-usar-o-r.html#como-usar-o-comando-help"><i class="fa fa-check"></i>Como Usar o Comando <code>help()</code>:</a></li>
<li class="chapter" data-level="" data-path="comece-a-usar-o-r.html"><a href="comece-a-usar-o-r.html#conveni√™ncia-da-pr√°tica"><i class="fa fa-check"></i>Conveni√™ncia da Pr√°tica</a></li>
<li class="chapter" data-level="" data-path="comece-a-usar-o-r.html"><a href="comece-a-usar-o-r.html#exemplo-de-uso"><i class="fa fa-check"></i>Exemplo de Uso</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="comece-a-usar-o-r.html"><a href="comece-a-usar-o-r.html#manuseie-arquivos"><i class="fa fa-check"></i><b>1.5</b> Manuseie Arquivos</a></li>
<li class="chapter" data-level="1.6" data-path="comece-a-usar-o-r.html"><a href="comece-a-usar-o-r.html#salve-e-restaure-seu-workspace"><i class="fa fa-check"></i><b>1.6</b> Salve e Restaure seu <i>Workspace</i></a></li>
<li class="chapter" data-level="1.7" data-path="comece-a-usar-o-r.html"><a href="comece-a-usar-o-r.html#instale-pacotes-r"><i class="fa fa-check"></i><b>1.7</b> Instale Pacotes R</a></li>
<li class="chapter" data-level="1.8" data-path="comece-a-usar-o-r.html"><a href="comece-a-usar-o-r.html#aprenda-mais-e-pratique"><i class="fa fa-check"></i><b>1.8</b> Aprenda Mais e Pratique</a></li>
<li class="chapter" data-level="1.9" data-path="comece-a-usar-o-r.html"><a href="comece-a-usar-o-r.html#participar-de-comunidades"><i class="fa fa-check"></i><b>1.9</b> Participar de Comunidades</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="conjunto-de-dados.html"><a href="conjunto-de-dados.html"><i class="fa fa-check"></i><b>2</b> Conjunto de Dados</a>
<ul>
<li class="chapter" data-level="2.1" data-path="conjunto-de-dados.html"><a href="conjunto-de-dados.html#organiza√ß√£o-e-nota√ß√£o"><i class="fa fa-check"></i><b>2.1</b> Organiza√ß√£o e Nota√ß√£o</a></li>
<li class="chapter" data-level="2.2" data-path="conjunto-de-dados.html"><a href="conjunto-de-dados.html#valores-faltantes-na"><i class="fa fa-check"></i><b>2.2</b> Valores faltantes: <code>NA</code></a></li>
<li class="chapter" data-level="2.3" data-path="conjunto-de-dados.html"><a href="conjunto-de-dados.html#dados-dispon√≠veis"><i class="fa fa-check"></i><b>2.3</b> Dados Dispon√≠veis</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="visualiza√ß√£o-de-dados.html"><a href="visualiza√ß√£o-de-dados.html"><i class="fa fa-check"></i><b>3</b> Visualiza√ß√£o de Dados</a>
<ul>
<li class="chapter" data-level="3.1" data-path="visualiza√ß√£o-de-dados.html"><a href="visualiza√ß√£o-de-dados.html#gr√°ficos-com-o-r"><i class="fa fa-check"></i><b>3.1</b> Gr√°ficos com o R</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="visualiza√ß√£o-de-dados.html"><a href="visualiza√ß√£o-de-dados.html#fun√ß√µes-nativas"><i class="fa fa-check"></i><b>3.1.1</b> Fun√ß√µes nativas</a></li>
<li class="chapter" data-level="3.1.2" data-path="visualiza√ß√£o-de-dados.html"><a href="visualiza√ß√£o-de-dados.html#pacote-animation"><i class="fa fa-check"></i><b>3.1.2</b> Pacote <code>animation</code></a></li>
<li class="chapter" data-level="3.1.3" data-path="visualiza√ß√£o-de-dados.html"><a href="visualiza√ß√£o-de-dados.html#pacote-ggplot2"><i class="fa fa-check"></i><b>3.1.3</b> Pacote <code>ggplot2</code></a></li>
<li class="chapter" data-level="3.1.4" data-path="visualiza√ß√£o-de-dados.html"><a href="visualiza√ß√£o-de-dados.html#pacote-plotly"><i class="fa fa-check"></i><b>3.1.4</b> Pacote <code>plotly</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="parte-ii---tratamento-dos-dados.html"><a href="parte-ii---tratamento-dos-dados.html"><i class="fa fa-check"></i>PARTE II - Tratamento dos Dados</a></li>
<li class="chapter" data-level="4" data-path="pr√©-processamento-de-vari√°veis.html"><a href="pr√©-processamento-de-vari√°veis.html"><i class="fa fa-check"></i><b>4</b> Pr√©-Processamento de Vari√°veis</a>
<ul>
<li class="chapter" data-level="4.1" data-path="pr√©-processamento-de-vari√°veis.html"><a href="pr√©-processamento-de-vari√°veis.html#autoescalamento"><i class="fa fa-check"></i><b>4.1</b> Autoescalamento</a></li>
<li class="chapter" data-level="4.2" data-path="pr√©-processamento-de-vari√°veis.html"><a href="pr√©-processamento-de-vari√°veis.html#normaliza√ß√£o-min-max"><i class="fa fa-check"></i><b>4.2</b> Normaliza√ß√£o Min-Max:</a></li>
<li class="chapter" data-level="4.3" data-path="pr√©-processamento-de-vari√°veis.html"><a href="pr√©-processamento-de-vari√°veis.html#transforma√ß√µes"><i class="fa fa-check"></i><b>4.3</b> Transforma√ß√µes</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="pr√©-processamento-de-vari√°veis.html"><a href="pr√©-processamento-de-vari√°veis.html#logar√≠tmica-e-raiz-quadrada"><i class="fa fa-check"></i><b>4.3.1</b> Logar√≠tmica e Raiz Quadrada</a></li>
<li class="chapter" data-level="4.3.2" data-path="pr√©-processamento-de-vari√°veis.html"><a href="pr√©-processamento-de-vari√°veis.html#transforma√ß√£o-box-cox"><i class="fa fa-check"></i><b>4.3.2</b> Transforma√ß√£o Box-Cox</a></li>
<li class="chapter" data-level="4.3.3" data-path="pr√©-processamento-de-vari√°veis.html"><a href="pr√©-processamento-de-vari√°veis.html#log√≠stica-e-logit"><i class="fa fa-check"></i><b>4.3.3</b> Log√≠stica e <em>Logit</em></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="pr√©-processamento-de-amostras.html"><a href="pr√©-processamento-de-amostras.html"><i class="fa fa-check"></i><b>5</b> Pr√©-Processamento de Amostras</a>
<ul>
<li class="chapter" data-level="5.1" data-path="pr√©-processamento-de-amostras.html"><a href="pr√©-processamento-de-amostras.html#todo"><i class="fa fa-check"></i><b>5.1</b> TODO</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="parte-iii---an√°lise-explorat√≥ria-de-dados.html"><a href="parte-iii---an√°lise-explorat√≥ria-de-dados.html"><i class="fa fa-check"></i>PARTE III - An√°lise Explorat√≥ria de Dados</a></li>
<li class="chapter" data-level="6" data-path="pca---an√°lise-de-componentes-principais.html"><a href="pca---an√°lise-de-componentes-principais.html"><i class="fa fa-check"></i><b>6</b> PCA - An√°lise de Componentes Principais</a>
<ul>
<li class="chapter" data-level="6.1" data-path="pca---an√°lise-de-componentes-principais.html"><a href="pca---an√°lise-de-componentes-principais.html#desenvolvimento-hist√≥rico"><i class="fa fa-check"></i><b>6.1</b> Desenvolvimento hist√≥rico</a></li>
<li class="chapter" data-level="6.2" data-path="pca---an√°lise-de-componentes-principais.html"><a href="pca---an√°lise-de-componentes-principais.html#defini√ß√£o"><i class="fa fa-check"></i><b>6.2</b> Defini√ß√£o</a></li>
<li class="chapter" data-level="6.3" data-path="pca---an√°lise-de-componentes-principais.html"><a href="pca---an√°lise-de-componentes-principais.html#interpreta√ß√£o-geom√©trica"><i class="fa fa-check"></i><b>6.3</b> Interpreta√ß√£o Geom√©trica</a></li>
<li class="chapter" data-level="6.4" data-path="pca---an√°lise-de-componentes-principais.html"><a href="pca---an√°lise-de-componentes-principais.html#objetivo"><i class="fa fa-check"></i><b>6.4</b> Objetivo</a></li>
<li class="chapter" data-level="6.5" data-path="pca---an√°lise-de-componentes-principais.html"><a href="pca---an√°lise-de-componentes-principais.html#pca-como-decomposi√ß√£o-matricial"><i class="fa fa-check"></i><b>6.5</b> PCA como Decomposi√ß√£o Matricial</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="pca---an√°lise-de-componentes-principais.html"><a href="pca---an√°lise-de-componentes-principais.html#principais-algoritmos"><i class="fa fa-check"></i><b>6.5.1</b> Principais Algoritmos</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="pca---an√°lise-de-componentes-principais.html"><a href="pca---an√°lise-de-componentes-principais.html#vari√¢ncia-por-pc"><i class="fa fa-check"></i><b>6.6</b> Vari√¢ncia por PC</a></li>
<li class="chapter" data-level="6.7" data-path="pca---an√°lise-de-componentes-principais.html"><a href="pca---an√°lise-de-componentes-principais.html#aplica√ß√£o-t√≠pica"><i class="fa fa-check"></i><b>6.7</b> Aplica√ß√£o T√≠pica</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="hca---an√°lise-de-agrupamento-hier√°rquico.html"><a href="hca---an√°lise-de-agrupamento-hier√°rquico.html"><i class="fa fa-check"></i><b>7</b> HCA - An√°lise de Agrupamento Hier√°rquico</a></li>
<li class="divider"></li>
<li><a href="https://github.com/cleber-n-borges/quimiometriaR" target="blank">&#8474;&#8477; Reposit√≥rio GitHub</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Caderno de Quimiometria com R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="pca---an√°lise-de-componentes-principais" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">&#8474;&#8477; 6</span> PCA - An√°lise de Componentes Principais<a href="pca---an√°lise-de-componentes-principais.html#pca---an√°lise-de-componentes-principais" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>A <strong>An√°lise de Componentes Principais (PCA)</strong><a href="#fn46" class="footnote-ref" id="fnref46"><sup>46</sup></a> √© uma t√©cnica estat√≠stica multivariada amplamente utilizada para reduzir a dimensionalidade de conjuntos de dados, preservando a maior quantidade poss√≠vel de variabilidade presente nos dados originais. Essa abordagem transforma um grande n√∫mero de vari√°veis correlacionadas em um conjunto menor de vari√°veis n√£o correlacionadas, chamadas de componentes principais. Os componentes principais s√£o combina√ß√µes lineares das vari√°veis originais e s√£o ordenados de forma que o primeiro componente captura a maior parte da variabilidade dos dados, seguido pelo segundo, e assim por diante.</p>
<div id="desenvolvimento-hist√≥rico" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Desenvolvimento hist√≥rico<a href="pca---an√°lise-de-componentes-principais.html#desenvolvimento-hist√≥rico" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A An√°lise de Componentes Principais tem seu hist√≥rico que remonta ao in√≠cio do s√©culo XX. Aqui est√£o os principais marcos na evolu√ß√£o dessa t√©cnica:</p>
<ol style="list-style-type: decimal">
<li><strong>Princ√≠pios Iniciais</strong>:
<ul>
<li>Os princ√≠pios da PCA podem ser tra√ßados at√© o trabalho de <strong>Karl Pearson</strong><a href="#fn47" class="footnote-ref" id="fnref47"><sup>47</sup></a>, um estat√≠stico brit√¢nico. Em 1901, ele publicou um artigo intitulado ‚ÄúOn Lines and Planes of Closest Fit to Systems of Points in Space‚Äù<a href="#fn48" class="footnote-ref" id="fnref48"><sup>48</sup></a>, onde apresentou o conceito de ‚Äúan√°lise de fatores‚Äù e a ideia de encontrar uma linha de melhor ajuste para um conjunto de pontos em um espa√ßo multidimensional. Este trabalho √© considerado uma das primeiras refer√™ncias √† ideia subjacente da PCA.</li>
</ul></li>
</ol>
<ol start="2" style="list-style-type: decimal">
<li><strong>Desenvolvimentos Posteriores</strong>:
<ul>
<li>Em 1933, <strong>Harold Hotelling</strong><a href="#fn49" class="footnote-ref" id="fnref49"><sup>49</sup></a>, um estat√≠stico americano, expandiu as ideias de Pearson e formalizou a t√©cnica da PCA em seu trabalho <em>‚ÄúAnalysis of a Complex of Statistical Variables into Principal Components‚Äù</em><a href="#fn50" class="footnote-ref" id="fnref50"><sup>50</sup></a>. Neste artigo, Hotelling descreveu um m√©todo para reduzir a dimensionalidade de dados multivariados, mantendo a maior parte da variabilidade nos dados originais. Ele introduziu a terminologia e as f√≥rmulas que s√£o fundamentais para a PCA moderna.</li>
</ul></li>
</ol>
<ol start="3" style="list-style-type: decimal">
<li><strong>Ado√ß√£o e Populariza√ß√£o</strong>:
<ul>
<li>Ao longo do s√©culo XX, a PCA come√ßou a ser adotada em v√°rias disciplinas, incluindo psicologia, biologia, e, mais tarde, em √°reas como ci√™ncias sociais e, mais recentemente, em ci√™ncias de dados e quimiometria. O conceito foi utilizado para an√°lises de dados complexos, onde a visualiza√ß√£o e a interpreta√ß√£o de m√∫ltiplas vari√°veis se tornaram essenciais.</li>
</ul></li>
<li><strong>Avan√ßos Computacionais</strong>:
<ul>
<li>Com o advento dos computadores e o aumento do poder computacional nas d√©cadas de 1970 e 1980, a PCA tornou-se ainda mais acess√≠vel e pr√°tica. Softwares estat√≠sticos come√ßaram a incorporar a PCA como uma ferramenta padr√£o para an√°lise de dados, permitindo que pesquisadores e profissionais realizassem an√°lises multivariadas com maior facilidade.</li>
</ul></li>
<li><strong>Aplica√ß√µes Modernas</strong>:
<ul>
<li>Nos anos 2000 e al√©m, a PCA passou a ser uma t√©cnica fundamental na an√°lise de grandes volumes de dados, especialmente na era do <em>Big Data</em>. Sua aplica√ß√£o se estendeu para √°reas como aprendizado de m√°quina <em>etc</em>, onde a redu√ß√£o de dimensionalidade e a identifica√ß√£o de padr√µes s√£o cruciais.</li>
</ul></li>
</ol>
<p>Em s√≠ntese, o m√©todo PCA evoluiu de um conceito inicial apresentado por Pearson para uma t√©cnica robusta e amplamente utilizada, gra√ßas ao trabalho de Hotelling e ao avan√ßo da computac√£o. Sua relev√¢ncia na an√°lise de dados contempor√¢neos atesta sua efic√°cia na extra√ß√£o de informa√ß√µes significativas, contexto do qual a quimiometria est√° inserida.</p>
</div>
<div id="defini√ß√£o" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Defini√ß√£o<a href="pca---an√°lise-de-componentes-principais.html#defini√ß√£o" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A explica√ß√£o apresentada anteriormente pode ainda n√£o ser satisfat√≥ria e suficiente para um entendimento completo do PCA, especialmente para aqueles que est√£o iniciando seus estudos. Assim, vamos retomar e destacar alguns pontos-chave de forma mais direta para esclarecer o conceito de maneira mais eficaz. Vamos analisar cada ponto com aten√ß√£o e com melhores defini√ß√µes:</p>
<p>¬†</p>
<ol style="list-style-type: decimal">
<li>O que s√£o essas <strong>Componentes Principais</strong>?</li>
</ol>
<ul>
<li>S√£o <strong>novas vari√°veis</strong> produzidas a partir das vari√°veis originais</li>
</ul>
<p>¬†</p>
<ol start="2" style="list-style-type: decimal">
<li>Como s√£o criadas essas <strong>novas vari√°veis</strong> chamadas de Componentes Principais?</li>
</ol>
<ul>
<li>S√£o <strong>combina√ß√µes lineares</strong> das vari√°veis originais.</li>
</ul>
<p>¬†</p>
<p>Considerando que as combina√ß√µes lineares desempenham um papel crucial para a defini√ß√£o do PCA, vamos explorar esse aspecto em mais detalhes e compreender como elas geram as componentes principais.</p>
<p><strong>Combina√ß√µes Lineares</strong></p>
<p>Considere que temos nossos dados numa matrix <span class="math inline">\(\mathbf{X}\)</span> com <span class="math inline">\(n\)</span> amostras (linhas) e <span class="math inline">\(m\)</span> vari√°veis (colunas). Podemos combinar linearmente as <span class="math inline">\(m\)</span> vari√°veis para formar <span class="math inline">\(k\)</span> componentes principais da seguinte forma:</p>
<p><span class="math display">\[
\mathbf{t}_1 = p_{1,1} \cdot \mathbf{x}_{\cdot,1} + p_{1,2} \cdot \mathbf{x}_{\cdot,2} + p_{1,3} \cdot \mathbf{x}_{\cdot,3} + \ldots + p_{1,m} \cdot \mathbf{x}_{\cdot, m} \\
\\
\mathbf{t}_2 = p_{2,1} \cdot \mathbf{x}_{\cdot,1} + p_{2,2} \cdot \mathbf{x}_{\cdot,2} + p_{2,3} \cdot \mathbf{x}_{\cdot,3} + \ldots + p_{2,m} \cdot \mathbf{x}_{\cdot, m} \\
\\
\mathbf{t}_3 = p_{3,1} \cdot \mathbf{x}_{\cdot,1} + p_{3,2} \cdot \mathbf{x}_{\cdot,2} + p_{3,3} \cdot \mathbf{x}_{\cdot,3} + \ldots + p_{3,m} \cdot \mathbf{x}_{\cdot, m} \\
\\
\vdots \\
\\
\mathbf{t}_k = p_{k,1} \cdot \mathbf{x}_{\cdot,1} + p_{k,2} \cdot \mathbf{x}_{\cdot,2} + p_{k,3} \cdot \mathbf{x}_{\cdot,3} + \ldots + p_{k,m} \cdot \mathbf{x}_{\cdot, m}
\]</span></p>
<p>¬†¬†¬†¬†¬†¬† onde:</p>
<ul>
<ul>
<li>
<p><span class="math inline">\(\mathbf{t}_1\)</span>, <span class="math inline">\(\mathbf{t}_2\)</span>, <span class="math inline">\(\mathbf{t}_3\)</span>, ‚Ä¶, <span class="math inline">\(\mathbf{t}_k\)</span> s√£o vetores. S√£o chamados de <em>scores</em>.</p>
<li>
<p><span class="math inline">\(p_{1,1}\)</span>, <span class="math inline">\(p_{1,2}\)</span>, <span class="math inline">\(p_{1,3}\)</span>, ‚Ä¶, <span class="math inline">\(p_{k,m}\)</span> s√£o valores escalares e s√£o chamados de pesos para cada vari√°vel. S√£o chamados de <em>loadings</em>.</p>
<li>
<p><span class="math inline">\(\mathbf{x}_{\cdot,1}\)</span>, <span class="math inline">\(\mathbf{x}_{\cdot,2}\)</span>, <span class="math inline">\(\mathbf{x}_{\cdot,3}\)</span>, ‚Ä¶, <span class="math inline">\(\mathbf{x}_{\cdot,m}\)</span> s√£o as colunas da matriz <span class="math inline">\(\mathbf{X}\)</span>.</p>
<li>
<p><span class="math inline">\(k\)</span> pode variar de 1 at√© <span class="math inline">\(w=min(n,m)\)</span> (o menor valor entre <span class="math inline">\(n\)</span> e <span class="math inline">\(m\)</span>).</p>
</ul>
</ul>
<p>Uma nota√ß√£o mais compacta para as equa√ß√µes pode ser expressa como:</p>
<p><span class="math display">\[
\mathbf{t}_k = \sum_{k=1}^{w} \sum_{j=1}^{m} p_{k,j} \cdot \mathbf{x}_{\cdot,j}
\]</span></p>
<p>Essa nota√ß√£o resume e deixa explic√≠to as combina√ß√µes lineares que resultam em cada vetor <span class="math inline">\(\mathbf{t}_k\)</span> em termos dos coeficientes <span class="math inline">\(p_{k,j}\)</span> e dos vetores <span class="math inline">\(\mathbf{x}_{\cdot,j}\)</span>.</p>
<p>¬†</p>
<p>√â fundamental destacar que as combina√ß√µes lineares apresentadas at√© agora permitem infinitas possibilidades, pois ainda n√£o impusemos restri√ß√µes quanto √† variabilidade dos dados que elas devem descrever. Para que as combina√ß√µes lineares se tornem as componentes principais, como mencionamos no in√≠cio, √© essencial e necess√°rio definir um crit√©rio que as caracterize. Esse crit√©rio se refere √† forma como ser√° a representa√ß√£o da variabilidade dos dados. O pr√≥ximo ponto abordar√° essa condi√ß√£o espec√≠fica.</p>
<p>¬†</p>
<ol start="3" style="list-style-type: decimal">
<li>Ent√£o como s√£o feitas essas <strong>combina√ß√µes lineares</strong> considerando a descri√ß√£o da variabilidade dos dados?</li>
</ol>
<ul>
<li><p>A primeira componente principal captura, obrigatoriamente, a maior parte de informa√ß√£o poss√≠vel contida dos dados.</p></li>
<li><p>Cada outra nova componente principal, ao ser definida, deve ser obrigatoriamente ortogonal √†s anteriores e capturar a maior varia√ß√£o poss√≠vel dos dados que ainda n√£o foi explicada.</p></li>
</ul>
<p>¬†</p>
<p>Por ortogonalidade entenda-se que n√£o h√° correla√ß√£o entre eles, ou seja, cada componente principal representa uma dire√ß√£o distinta na variabilidade dos dados. Em termos estritamente matem√°ticos, duas componentes principais (vetores, dire√ß√µes <em>etc</em>) s√£o ortogonais se o produto interno entre elas √© zero.</p>
<p>No sentido sobre capturar informa√ß√µes dos dados, diz-se que as componentes s√£o independentes entre si. Essa independ√™ncia √© crucial no PCA, pois permite que cada componente capture informa√ß√µes √∫nicas sobre a varia√ß√£o nos dados, sem redund√¢ncia. Em outras palavras, a primeira componente principal explica a maior parte da vari√¢ncia dos dados, a segunda componente, que √© ortogonal √† primeira, explica a maior parte da vari√¢ncia restante (que necessariamente n√£o foi capturada na primeira), e assim por diante.</p>
<p>A inten√ß√£o dos tr√™s pontos abordados √© esclarecer as condi√ß√µes necess√°rias para a defini√ß√£o completa de PCA. Compreendendo esses aspectos, teremos uma base s√≥lida sobre o que √© PCA, em termos formais. No entanto, √© igualmente importante explorar mais dois outros pontos, que s√£o, a interpreta√ß√£o geom√©trica do PCA e os objetivos para os quais o PCA √© utilizado.</p>
</div>
<div id="interpreta√ß√£o-geom√©trica" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> Interpreta√ß√£o Geom√©trica<a href="pca---an√°lise-de-componentes-principais.html#interpreta√ß√£o-geom√©trica" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A explica√ß√£o anterior tratou do PCA sob uma perspectiva alg√©brica. No entanto, tamb√©m podemos compreend√™-lo geometricamente, destacando que a transforma√ß√£o resultante da combina√ß√£o linear pode ser vista como uma rota√ß√£o dos eixos originais. Essa rota√ß√£o √© realizada de forma a maximizar a captura da variabilidade dos dados, mantendo a ortogonalidade das novas dire√ß√µes, conforme j√° definido. No caso de duas vari√°veis, essa rota√ß√£o se torna facilmente visualiz√°vel e imediatamente reconhec√≠vel, permitindo uma compreens√£o intuitiva do processo.</p>
<p>Para facilitar a ilustra√ß√£o e a visualiza√ß√£o, consideraremos uma matriz <span class="math inline">\(\mathbf{X}_{100,2}\)</span>‚Äã composta por duas vari√°veis, <span class="math inline">\(\mathbf{x}_1\)</span>‚Äã e <span class="math inline">\(\mathbf{x}_{2}\)</span>‚Äã, contendo 100 linhas de valores aleat√≥rios. O processo de transformar as vari√°veis originais em duas novas componentes principais pode ser ilustrado no gr√°fico a seguir:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pca-rot"></span>
<img src="img/parte03/pca_interpreta_Geo_Rot.png" alt="PCA como rota√ß√£o dos dados" width="95%" />
<p class="caption">
Figure 6.1: PCA como rota√ß√£o dos dados
</p>
</div>
<details>
<summary>
üòà
</summary>
<p><br><strong>Suplemento: Mais detalhes sobre a matriz X[100,2]</strong><br><br></p>
<pre><code># correla√ß√£o
&gt; cor( X )
          x1        x2
x1 1.0000000 0.8303746
x2 0.8303746 1.0000000
&gt;
# Produto Cruzado de X
&gt; round( crossprod( as.matrix( X ) ), 3 )
       x1     x2
x1 99.000 82.207
x2 82.207 99.000
&gt; 
# PCA
&gt; summary( prcomp( X ) )
Importance of components:
                          PC1     PC2
Standard deviation     1.3529 0.41186
Proportion of Variance 0.9152 0.08481
Cumulative Proportion  0.9152 1.00000
&gt;
# Dados Originais
&gt; round(as.numeric( X[,1]),3)
  [1]  0.305  0.416 -0.318 -0.627  0.324 -1.851  0.345 -0.468 -0.475 -1.045
 [11] -0.324  1.410  0.113  0.247 -0.893  0.513 -1.015 -0.517  0.785  0.048
 [21]  0.479  1.086 -0.798 -1.613 -1.653  1.399 -0.652  0.337  0.329 -0.366
 [31]  0.508  1.751  1.618  1.244  0.008  0.221 -0.511 -1.711  1.366 -0.197
 [41]  0.792 -2.355 -1.171  0.621  0.547  1.090 -1.488  0.289  0.303 -1.392
 [51] -0.705  1.527 -0.172  0.096 -0.822  0.029  0.400  0.519  1.704 -2.325
 [61] -0.086 -1.424  0.276  1.206 -0.746 -1.864  0.577  1.210  0.244 -1.382
 [71] -0.171 -0.924 -1.161  1.871  1.038  0.626  0.521 -0.948  0.207  0.696
 [81]  0.359  0.716 -0.493  2.002  0.651  1.455  0.383 -0.496  0.261  0.520
 [91] -1.085 -0.987  1.473 -0.571 -1.100  0.397 -0.673  1.716 -0.758 -0.843
&gt;
&gt; round(as.numeric( X[,2]),3)
  [1]  0.335 -0.344 -0.037 -1.243  0.303 -1.751  0.069  0.476 -0.640 -0.654
 [11] -0.957  0.330  0.761  0.896 -0.759  0.091 -0.838 -0.076  1.789 -0.026
 [21]  1.201  1.545 -0.463 -0.422 -1.535  0.413 -1.551  0.876 -0.947 -0.970
 [31]  1.277  1.816  1.515  0.279  1.453 -0.435 -0.980 -0.975  1.101 -0.411
 [41] -0.045 -2.560 -1.259 -0.377  0.406  1.116 -2.031  0.669  0.705 -1.023
 [51] -1.466  1.458 -0.050 -0.398 -0.720 -0.141  1.225  0.185  1.328 -2.181
 [61] -0.361 -0.957  0.054  0.639 -1.151 -1.619  0.771  0.222  1.141 -1.105
 [71] -0.046  0.008 -0.902  1.876 -0.167  0.724  1.753 -0.535  0.254 -0.494
 [81] -0.253  0.500 -0.630  2.147  0.930  1.141  1.076 -0.201  0.082  0.098
 [91] -0.561 -1.067  1.548 -0.299 -0.148  0.723 -0.073  1.054 -0.858 -0.668
&gt; </code></pre>
<p>¬†</p>
<p><strong>Gr√°fico interativo</strong></p>
<center>
<iframe src="img/parte03/plotly/pca_rot.html" width="80%" height="400px" data-external="1">
</iframe>
</center>
</details>
<p>¬†</p>
<p>A figura <a href="pca---an√°lise-de-componentes-principais.html#fig:pca-rot">6.1</a> mostra 4 gr√°ficos, com o primeiro mostrando apenas a dispers√£o dos pontos nas vari√°veis originais. No segundo, a reta em azul representa a dire√ß√£o da primeira componente principal capturando a maior variabilidade dos dados. Ou seja, est√° na dire√ß√£o onde h√° a maior dispers√£o da informa√ß√£o contida nos dados. A reta vermelha no terceiro gr√°fico indica a segunda componente principal, que √© perpendicular √† primeira e por sua vez √© a dire√ß√£o que captura a maior parte da dispers√£o que a primeira componente n√£o consegue representar. O quarto gr√°fico usa as duas primeiras componentes principais como sistema de coordenadas e assim √© poss√≠vel visualizar e entender que essa transforma√ß√£o consistiu na rota√ß√£o do sistema dos eixos originais.</p>
<p>√â importante destacar que a rela√ß√£o entre os pontos permanece inalterada, independentemente do sistema de eixos utilizado como mostrado pela figura. Em outras palavras, a dist√¢ncia relativa entre os pontos n√£o muda com a rota√ß√£o dos eixos. Essa propriedade √© fundamental no contexto do PCA, pois significa que, ao aplicar a transforma√ß√£o para encontrar as componentes principais, a estrutura dos dados em termos de proximidade e dispers√£o √© preservada. Assim, mesmo ap√≥s a rota√ß√£o, as rela√ß√µes espaciais entre os pontos continuam a refletir a mesma informa√ß√£o sobre a variabilidade dos dados, permitindo que a an√°lise se concentre nas dire√ß√µes de maior vari√¢ncia sem distorcer a interpreta√ß√£o dos dados originais.</p>
</div>
<div id="objetivo" class="section level2 hasAnchor" number="6.4">
<h2><span class="header-section-number">6.4</span> Objetivo<a href="pca---an√°lise-de-componentes-principais.html#objetivo" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Ap√≥s uma apresenta√ß√£o formal da defini√ß√£o de PCA, a pergunta que se levanta √©: qual √© a motiva√ß√£o para aplicar essa t√©cnica aos dados? O ponto crucial para compreender suas aplica√ß√µes reside intrinsecamente ligada na interpreta√ß√£o dos dados nas novas vari√°veis geradas. O objetivo central do PCA √© permitir a redu√ß√£o a dimensionalidade de um conjunto de dados, preservando ao m√°ximo a variabilidade e a informa√ß√£o contida neles.</p>
<p>E como se d√° essa redu√ß√£o? A chave est√° na interpreta√ß√£o: o pesquisador assume que, a partir de certo ponto de ac√∫mulo de informa√ß√£o, o que excede pode ser considerado apenas res√≠duo ou erro estat√≠stico inerente aos dados. Ou seja, parte da informa√ß√£o pode ser descartada sem comprometer a descri√ß√£o que os dados prop√µem.</p>
<p>Para ser mais claro, a t√©cnica de PCA n√£o reduz a dimensionalidade por si s√≥ necessariamente; √© o pesquisador que, na maior parte das vezes, por meio de uma interpreta√ß√£o cuidadosa das componentes principais, que decide descartar as de ordem mais alta, levando √† redu√ß√£o efetiva da dimensionalidade dos dados.</p>
<p>A t√©cnica de PCA reduz a dimensionalidade somente em situa√ß√µes de depend√™ncia linear entre vari√°veis. Quando uma vari√°vel pode ser expressa como uma combina√ß√£o linear exata de outras, isso resulta em uma correla√ß√£o perfeita, com coeficiente igual a 1. Nesses casos, o n√∫mero de componentes principais gerados ser√° menor que o menor valor entre o n√∫mero de vari√°veis (<span class="math inline">\(n\)</span>) e o n√∫mero de observa√ß√µes (<span class="math inline">\(m\)</span>), pois a redund√¢ncia das informa√ß√µes √© representada em uma √∫nica componente. Essa caracter√≠stica permite que o PCA retenha apenas as combina√ß√µes de vari√°veis totalmente correlacionadas, simplificando a representa√ß√£o dos dados em termos de n√∫mero de vari√°veis. Por exemplo did√°tico, crie uma imagem mental da figura <a href="pca---an√°lise-de-componentes-principais.html#fig:pca-rot">6.1</a> e reflita sobre a aplica√ß√£o da PCA nos dados <span class="math inline">\(\mathbf{x}_1 = \{1;2;3\}\)</span> e <span class="math inline">\(\mathbf{x}_2 = \{1;2;3\}\)</span>.</p>
<p>Nos casos em que a correla√ß√£o entre as vari√°veis √© inferior a 1, mas ainda assim altamente significativa, a t√©cnica de PCA resulta em <span class="math inline">\(k\)</span> componentes principais, sendo <span class="math inline">\(k\)</span> igual ao menor valor entre o n√∫mero de vari√°veis (<span class="math inline">\(n\)</span>) e o n√∫mero de observa√ß√µes (<span class="math inline">\(m\)</span>). Nesses cen√°rios, algumas das componentes principais podem explicar muito pouca variabilidade, tornando-se candidatas a serem descartadas. Isso acontece porque as informa√ß√µes redundantes s√£o capturadas em uma componente e apenas a informa√ß√£o restante que √© independente, √© representada em outra componentes mas com baixa descri√ß√£o de variabilidade. Dessa forma, essas componentes n√£o ter√° peso substancial para a descri√ß√£o da variabilidade dos dados. Por exemplo, construa uma imagem mental da figura <a href="pca---an√°lise-de-componentes-principais.html#fig:pca-rot">6.1</a> e pense na PCA dos dados <span class="math inline">\(\mathbf{x}_1 = \{1;2;3\}\)</span> e <span class="math inline">\(\mathbf{x}_2 = \{1;2;3,3\}\)</span>.</p>
<p>Para o caso em que as vari√°veis s√£o inerentemente n√£o correlacionadas, todas as componentes principais ter√£o magnitudes semelhantes, tornando a aplica√ß√£o do PCA desnecess√°ria. Por exemplo, visualize a figura <a href="pca---an√°lise-de-componentes-principais.html#fig:pca-rot">6.1</a> e considere a PCA aplicada aos dados <span class="math inline">\(\mathbf{x}_1 = \{1, 0\}\)</span> e <span class="math inline">\(\mathbf{x}_2 = \{0, 1\}\)</span>. Nessa situa√ß√£o, a falta de correla√ß√£o entre as vari√°veis implica que a t√©cnica n√£o trar√° benef√≠cios, j√° que as informa√ß√µes j√° est√£o distribu√≠das de forma independente.</p>
<p>√â nesses termos que se afirma sobre a possibilidade de <strong>redu√ß√£o de dimensionalidade</strong>: A PCA permite simplificar conjuntos de dados de alta dimens√£o, ou seja, eliminar colinearidades quando estas estiverem presentes. Assim, permite que analistas visualizem e interpretem as informa√ß√µes de forma mais clara. Ao reduzir a dimensionalidade dos dados, a t√©cnica preserva a maior parte da variabilidade, facilitando a identifica√ß√£o de padr√µes (caso exista) e a tomada de decis√µes. Essa representa√ß√£o mais compacta dos dados torna a an√°lise mais acess√≠vel e eficiente, podendo revelar tend√™ncias que potencialmente passariam despercebidos em um espa√ßo dimensional maior.</p>
<p>Aproveitando o exemplo j√° mostrado pela figura <a href="pca---an√°lise-de-componentes-principais.html#fig:pca-rot">6.1</a> com as vari√°veis <span class="math inline">\(x_1\)</span> e <span class="math inline">\(x_2\)</span>, observamos que a correla√ß√£o √© <span class="math inline">\(\sim 83\%\)</span>. A primeira componente principal ret√©m <span class="math inline">\(\sim 91,5\%\)</span>, da vari√¢ncia dos dados, enquanto a segunda componente ret√©m apenas <span class="math inline">\(\sim 8,5\%\)</span>. Se considerarmos que uma reten√ß√£o de vari√¢ncia acima de <span class="math inline">\(90\%\)</span> √© satisfat√≥ria, podemos concluir que a segunda componente √© dispens√°vel. Isso nos permite simplificar os dados, mantendo a ess√™ncia das informa√ß√µes. Ao descartar a segunda componente, assumimos que ela representa principalmente ru√≠do. Dessa forma, estamos utilizando a an√°lise de forma mais eficiente, interpretando os dados com o emprego da an√°lise de PCA servindo como um filtro, destacando as caracter√≠sticas mais relevantes e eliminando varia√ß√µes indesejadas.</p>
<p><strong>Mais alguns Detalhes</strong></p>
<p>A consequ√™ncia da redu√ß√£o de dimensionalidade em dados altamente correlacionados, como os obtidos em v√°rias t√©cnicas espectrosc√≥picas, resulta em que a vari√¢ncia acumulada nas duas primeiras componentes principais geralmente representa uma parte significativa da vari√¢ncia total. Isso permite a cria√ß√£o de gr√°ficos bidimensionais, que facilitam a explora√ß√£o visual das componentes e se h√° identifica√ß√£o de padr√µes e tend√™ncias no comportamento dos dados. Essa ferramenta se torna muito √∫til para an√°lises e interpreta√ß√µes.</p>
<p>Outro aspecto est√° relacionado quanto a decis√£o que precisamos tomar de quantas componentes principais devemos manter. No entanto, n√£o existe uma regra fixa ou um n√∫mero m√°gico para isso. A raz√£o √© que a escolha do <span class="math inline">\(k\)</span> depende de v√°rios fatores, como o objetivo da an√°lise e a natureza dos dados. Por exemplo, se queremos uma representa√ß√£o menos rigorosa dos dados, podemos optar por menos componentes. Mas se buscamos capturar a maior parte da varia√ß√£o nos dados, talvez precisemos de mais componentes.</p>
<p>Al√©m disso, cada conjunto de dados √© √∫nico, ent√£o o que funciona bem para um conjunto pode n√£o ser adequado para outro. √â por isso que, em geral, utilizamos a varia√ß√£o explicada por cada componente, mostrada em gr√°fico, ou soma acumulada da vari√¢ncia para nos ajudar a decidir quantas componentes manter, mas no final, a escolha sempre envolve um certo grau de julgamento e an√°lise do contexto.</p>
<p>Outra caracter√≠stica intr√≠nseca ao uso do PCA √© que conseguir uma interpreta√ß√£o f√°cil e direta das componentes principais em termos f√≠sicos pode ser desafiador. Especialmente quando os dados t√™m muitas vari√°veis, a interpreta√ß√£o f√≠sica √© praticamente imposs√≠vel e √© melhor n√£o tentar faz√™-la. Isso acontece porque as componentes principais s√£o combina√ß√µes lineares das vari√°veis originais, e essas combina√ß√µes podem n√£o ter um significado f√≠sico claro.</p>
<p>Em vez disso, √© muito mais eficaz focar nas vari√°veis com maior influ√™ncia nas componentes, e assim sendo, √© poss√≠vel obter alguma interpreta√ß√£o ao analisar os coeficientes (ou pesos) que cada vari√°vel tem nas componentes. Se uma vari√°vel tem um peso alto em uma componente principal, isso indica que ela √© importante para essa nova dimens√£o. Assim, voc√™ pode olhar para essas vari√°veis mais influentes e tentar entender como elas se relacionam com a varia√ß√£o dos dados.</p>
<p>Por fim, ter entendimento sobre PCA √© importante tamb√©m pois ela serve de base e suporte para outros m√©todos, como PCR (Regress√£o em Componentes Principais) e o m√©todo de classifica√ß√£o SIMCA (<em>Soft Independent Modeling of Class Analogy</em>). Al√©m disso, a An√°lise Discriminante e v√°rios m√©todos de agrupamento (<em>clustering</em>) podem ter um desempenho melhor quando aplicam a PCA para eliminar ru√≠dos nos dados. Para qualquer outro m√©todo que enfrente desafios relacionados √† multicolinearidade nas vari√°veis, a aplica√ß√£o da PCA como etapa pr√©via se torna conveniente, pois permite eliminar redund√¢ncias. Assim, a PCA atua como uma t√©cnica de pr√©-processamento que aprimora a efic√°cia e a precis√£o desses m√©todos.</p>
<p>¬†</p>
<p><strong>Resumindo:</strong></p>
<p>A PCA serve a v√°rios prop√≥sitos no contexto da quimiometria e da an√°lise multivariada de dados:</p>
<ul>
<li><p><strong>Redu√ß√£o de Dimensionalidade</strong>: Ao simplificar conjuntos de dados de alta dimens√£o, a PCA permite que os analistas visualizem e interpretem os dados mais facilmente.</p></li>
<li><p><strong>Identifica√ß√£o de Padr√µes</strong>: A t√©cnica ajuda a revelar padr√µes ocultos, tend√™ncias e estruturas nos dados, facilitando a identifica√ß√£o de grupos ou clusters de amostras.</p></li>
<li><p><strong>Elimina√ß√£o de Redund√¢ncia</strong>: A PCA pode ajudar a eliminar vari√°veis redundantes, concentrando-se nas vari√°veis que mais contribuem para a variabilidade dos dados.</p></li>
<li><p><strong>Pr√©-processamento para Modelagem</strong>: A PCA √© frequentemente utilizada como um passo de pr√©-processamento antes de aplicar modelos de regress√£o, classifica√ß√£o ou outras an√°lises multivariadas, melhorando a efici√™ncia e a precis√£o dos resultados.</p></li>
</ul>
<p>¬†</p>
<p><strong>Recomenda√ß√£o para quando usar</strong>:</p>
<p>A PCA √© recomendada em v√°rias situa√ß√µes dentro da quimiometria, especialmente quando:</p>
<ul>
<li><p><strong>Dados de Alta Dimensionalidade</strong>: Quando se trabalha com conjuntos de dados que cont√™m um grande n√∫mero de vari√°veis em rela√ß√£o ao n√∫mero de amostras, a PCA √© √∫til para reduzir a complexidade.</p></li>
<li><p><strong>Correla√ß√µes Entre Vari√°veis</strong>: A PCA √© especialmente eficaz quando h√° alta correla√ß√£o entre as vari√°veis, pois a t√©cnica ajuda a identificar as dire√ß√µes principais da varia√ß√£o nos dados.</p></li>
<li><p><strong>Visualiza√ß√£o de Dados</strong>: Quando √© necess√°rio visualizar a estrutura dos dados, a PCA pode ser utilizada para criar gr√°ficos bidimensionais ou tridimensionais que facilitam a interpreta√ß√£o e a comunica√ß√£o dos resultados.</p></li>
<li><p><strong>Detec√ß√£o de Outliers</strong>: A PCA pode ajudar a identificar pontos de dados que se afastam do padr√£o esperado, facilitando a detec√ß√£o de outliers.</p></li>
<li><p><strong>Modelos Previstos</strong>: A PCA √© √∫til como etapa inicial na modelagem, onde as vari√°veis transformadas em componentes principais podem ser usadas como entradas para modelos de regress√£o ou classifica√ß√£o, garantindo que a variabilidade significativa dos dados seja capturada.</p></li>
</ul>
</div>
<div id="pca-como-decomposi√ß√£o-matricial" class="section level2 hasAnchor" number="6.5">
<h2><span class="header-section-number">6.5</span> PCA como Decomposi√ß√£o Matricial<a href="pca---an√°lise-de-componentes-principais.html#pca-como-decomposi√ß√£o-matricial" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As combina√ß√µes lineares das quais discutimos anteriormente podem ser reestruturadas em formato matricial, tornando a nota√ß√£o mais compacta.</p>
<p><span class="math display">\[
\mathbf{T} = \mathbf{X} \cdot \mathbf{P}
\]</span></p>
<p>¬†¬†¬†¬†¬†¬† onde:</p>
<ul>
<ul>
<li>
<span class="math inline">\(\mathbf{T}\)</span> √© a matriz de <em>scores</em>
</li>
<li>
<span class="math inline">\(\mathbf{X}\)</span> √© a matriz de dados
</li>
<li>
<span class="math inline">\(\mathbf{P}\)</span> √© a matriz de <em>loadings</em>
</li>
</ul>
</ul>
<p>¬†</p>
<p>E decorre, ap√≥s manipula√ß√µes, que PCA pode igualmente ser entendido e abordado como uma decomposi√ß√£o da matriz <span class="math inline">\(\mathbf{X}\)</span> nessas outras duas novas matrizes:</p>
<p>¬†</p>
<p><span class="math display">\[
\mathbf{X} = \mathbf{T} \cdot \mathbf{P^T}
\]</span></p>
<p>¬†</p>
<ul>
<li><p>Os <em>scores</em> referem-se √†s coordenadas das observa√ß√µes nos novos eixos criados pelos componentes principais. Em outras palavras, eles representam a proje√ß√£o das observa√ß√µes originais na nova base de componentes principais. Cada <em>score</em> indica como uma observa√ß√£o se posiciona em rela√ß√£o aos novos eixos.</p></li>
<li><p>Os <em>loadings</em>, por outro lado, representam a contribui√ß√£o de cada vari√°vel original para os componentes principais. Eles indicam a correla√ß√£o entre as vari√°veis originais e os componentes principais. Os <em>loadings</em> ajudam a entender como as vari√°veis influenciam a forma√ß√£o dos novos eixos e, consequentemente, os <em>scores</em>. Em geral, um <em>loading</em> alto (positivo ou negativo) para uma vari√°vel em um componente principal sugere que essa vari√°vel tem uma forte influ√™ncia nesse componente.</p></li>
</ul>
<p>¬†</p>
<p><strong>Equival√™ncia de Nota√ß√£o</strong></p>
<p>As duas formas s√£o equivalentes, pois podemos monstrar que <span class="math inline">\(\mathbf{T} = \mathbf{X} \cdot \mathbf{P}\)</span> pode ser expresso como <span class="math inline">\(\mathbf{X} = \mathbf{T} \cdot \mathbf{P^T}\)</span>. S√≥ precisamos manipular a equa√ß√£o de forma a isolarmos <span class="math inline">\(\mathbf{X}\)</span>.</p>
<ol style="list-style-type: decimal">
<li>Come√ßamos com a equa√ß√£o original:</li>
</ol>
<p><span class="math display">\[
\mathbf{T} = \mathbf{X} \cdot \mathbf{P}
\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Multiplicamos ambos os lados pela inversa de <span class="math inline">\(\mathbf{P}\)</span> √† direita. Como <span class="math inline">\(\mathbf{P}\)</span> √© ortogonal, usamos a rela√ß√£o <span class="math inline">\(\mathbf{P^{-1}} = \mathbf{P^T}\)</span>:</li>
</ol>
<p><span class="math display">\[
\mathbf{T} \cdot \mathbf{P^{-1}} = ( \mathbf{X} \cdot \mathbf{P} ) \cdot \mathbf{P^{-1}}
\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>O lado direito simplifica-se como:</li>
</ol>
<p><span class="math display">\[
\mathbf{T} \cdot \mathbf{P^T} = \mathbf{X} \cdot \mathbf{I} = \mathbf{X}
\]</span></p>
<ol start="4" style="list-style-type: decimal">
<li>Portanto, podemos reescrever a equa√ß√£o como:</li>
</ol>
<p><span class="math display">\[
\mathbf{X} = \mathbf{T} \cdot \mathbf{P^T}
\]</span></p>
<p>Assim, est√° demonstrado que, sob a condi√ß√£o de que <span class="math inline">\(\mathbf{P}\)</span> sendo uma matriz ortogonal, a rela√ß√£o <span class="math inline">\(\mathbf{T} = \mathbf{X} \cdot \mathbf{P}\)</span> implica em <span class="math inline">\(\mathbf{X} = \mathbf{T} \cdot \mathbf{P^T}\)</span> e que o ponto de vista sobre PCA ser uma decomposi√ß√£o matricial √© fact√≠vel.</p>
<div id="principais-algoritmos" class="section level3 hasAnchor" number="6.5.1">
<h3><span class="header-section-number">6.5.1</span> Principais Algoritmos<a href="pca---an√°lise-de-componentes-principais.html#principais-algoritmos" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Adotando a perpectiva da decomposi√ß√£o matricial, conseguimos estabelecer uma conex√£o natural para explorar outras decomposi√ß√µes matriciais j√° existentes e suas interconex√µes metodol√≥gicas. Assim sendo, ao menos os m√©todos mais usuais utilizados para realizar a PCA aproveitam abordagens matem√°ticas distintas j√° consolidadas.</p>
<p>Cada uma dessas abordagens oferece suas particularidades e aplica√ß√µes espec√≠ficas, proporcionando vantagens em diferentes contextos e tamanhos de conjuntos de dados.</p>
<p>A seguir, apresentamos tr√™s formatos diferentes, cada um implementado por meio das seguintes abordagens:</p>
<ol style="list-style-type: decimal">
<li><strong>Decomposi√ß√£o Espectral</strong></li>
</ol>
<p>A Decomposi√ß√£o Espectral<a href="#fn51" class="footnote-ref" id="fnref51"><sup>51</sup></a><sup>,</sup><a href="#fn52" class="footnote-ref" id="fnref52"><sup>52</sup></a> tamb√©m conhecida como Decomposi√ß√£o em Autovetores e Autovalores<a href="#fn53" class="footnote-ref" id="fnref53"><sup>53</sup></a><sup>,</sup><a href="#fn54" class="footnote-ref" id="fnref54"><sup>54</sup></a> decomp√µe uma matriz quadrada <span class="math inline">\(\mathbf{A}\)</span> de dimens√£o <span class="math inline">\(m \times m\)</span> na forma:</p>
<p><span class="math display">\[
\mathbf{A} = \mathbf{V} \cdot \mathbf{\Sigma} \cdot \mathbf{V^{-1}}
\]</span></p>
<p>¬†¬†¬†¬†¬†¬† onde:</p>
<ul>
<ul>
<li>
<span class="math inline">\(\mathbf{A}\)</span> √© a matriz de covari√¢ncia<a href="#fn55" class="footnote-ref" id="fnref55"><sup>55</sup></a> de <span class="math inline">\(\mathbf{X}\)</span>.
</li>
<li>
<span class="math inline">\(\mathbf{V}\)</span> √© uma matriz <span class="math inline">\(m \times m\)</span> cujas colunas s√£o os autovetores. <span class="math inline">\(\mathbf{V^{-1}}\)</span> √© a inversa de <span class="math inline">\(\mathbf{V}\)</span>.
</li>
<li>
<p><span class="math inline">\(\mathbf{\Sigma}\)</span> √© uma matriz diagonal de dimens√£o <span class="math inline">\(m \times m\)</span> contendo os autovalores correspondentes.</p>
</ul>
</ul>
<p>¬†</p>
<p>As matrizes <span class="math inline">\(\mathbf{P}\)</span> (<em>loadings</em>) e <span class="math inline">\(\mathbf{T}\)</span> (<em>scores</em>), da An√°lise de Componentes Principais (PCA), podem ser obtidas por:</p>
<p>¬†</p>
<p><span class="math display">\[
\mathbf{P} = \mathbf{V}
\]</span></p>
<p>¬†</p>
<p><span class="math display">\[
\mathbf{T} = \mathbf{X} \cdot \mathbf{V}
\]</span></p>
<p>¬†</p>
<p>Uma das caracter√≠sticas intr√≠secas a esse m√©todo de implementa√ß√£o de PCA √© que exige c√°lculo expl√≠cito tanto de <span class="math inline">\(\mathbf{A} = \mathbf{X^T} \cdot \mathbf{X}\)</span> quanto de <span class="math inline">\(\mathbf{T} = \mathbf{X} \cdot \mathbf{V}\)</span> , que s√£o custosas computacionalmente em conjuntos de dados muito grandes (caso das conhecidas ci√™ncias ‚Äú<em>√¥micas</em>‚Äù e da √°rea de <em>Big Data</em>) devido ao n√∫mero de opera√ß√£o. O que por sua vez, acarreta em maior propaga√ß√£o dos erros inerentes √† representa√ß√£o num√©rica na computa√ß√£o em ponto flutuante<a href="#fn56" class="footnote-ref" id="fnref56"><sup>56</sup></a> quanto compar√°vel √† implementa√ß√£o a ser mostrada em sequ√™ncia por SVD. Algoritmos que implementam essa decomposi√ß√£o n√£o permitem dados faltantes. Caso exista na matriz de dados, o usu√°rio deve tratar os dados perdidos previamente.</p>
<p>No R, existe a fun√ß√£o <code>princomp</code> que se utiliza justamente dessa forma de abordagem para PCA. Por√©m a leitura do manual da fun√ß√£o declara que a mesma existe somente por compatibilidade com o <em>software</em> S-PLUS e que deve ser um m√©todo preterido. Para saber mais detalhes, digite : <code>?princomp</code> no console do R.</p>
<p>O leitor que se sinta satisafeito em saber que, uma vez tendo a solu√ß√£o da Decomposi√ß√£o Espectral da matriz A, de alguma forma, ent√£o automaticamente j√° teremos ent√£o a solu√ß√£o de PCA, pode seguir para o pr√≥ximo algoritmo.</p>
<p>¬†</p>
<details>
<summary>
üòà
</summary>
<p><br><strong>Solu√ß√£o para a Decomposi√ß√£o Espectral</strong><br><br></p>
<p>Aqui, para o leitor curioso por ao menos ver um caso de solu√ß√£o, trataremos o exemplo da figura <a href="pca---an√°lise-de-componentes-principais.html#fig:pca-rot">6.1</a> em mais alguns detalhes.</p>
<p>Para realizar a Decomposi√ß√£o Espectral podemos seguir os seguintes passos:</p>
<ol style="list-style-type: decimal">
<li><strong>Encontrar os autovalores</strong>:
<ul>
<li>Calcular o polin√¥mio caracter√≠stico de <span class="math inline">\(\mathbf{A}\)</span>, que √© dado por <span class="math inline">\(\det(\mathbf{A} - \lambda \mathbf{I}) = 0\)</span>, onde <span class="math inline">\(\lambda\)</span> s√£o os autovalores, <span class="math inline">\(\mathbf{I}\)</span> √© a matriz identidade e <span class="math inline">\(\det()\)</span> √© a fun√ß√£o matricial determinate<a href="#fn57" class="footnote-ref" id="fnref57"><sup>57</sup></a>.</li>
<li>Resolver a equa√ß√£o para encontrar os autovalores <span class="math inline">\(\lambda_1, \lambda_2, \ldots, \lambda_n\)</span>.</li>
</ul></li>
</ol>
<ol start="2" style="list-style-type: decimal">
<li><strong>Encontrar os autovetores</strong>:
<ul>
<li>Para cada autovalor <span class="math inline">\(\lambda_i\)</span>, resolva o sistema <span class="math inline">\((\mathbf{A} - \lambda_i \mathbf{I})\mathbf{v}_i = 0\)</span> para encontrar o autovetor correspondente <span class="math inline">\(\mathbf{v}_i\)</span>.</li>
<li>Repita isso para todos os autovalores.</li>
</ul></li>
<li><strong>Formar as matrizes <span class="math inline">\(\mathbf{V}\)</span> e <span class="math inline">\(\mathbf{\Sigma}\)</span></strong>:
<ul>
<li>A matriz <span class="math inline">\(\mathbf{V}\)</span> √© formada pelos autovetores como colunas: <span class="math inline">\(\mathbf{V} = [\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_n]\)</span>.</li>
<li>A matriz <span class="math inline">\(\mathbf{\Sigma}\)</span> √© uma matriz diagonal cujos elementos na diagonal s√£o os autovalores correspondentes: <span class="math inline">\(\mathbf{\Sigma} = \text{diag}(\lambda_1, \lambda_2, \ldots, \lambda_n)\)</span>.</li>
</ul></li>
<li><strong>Verificar a decomposi√ß√£o</strong>:
<ul>
<li>Voc√™ pode verificar a decomposi√ß√£o calculando <span class="math inline">\(\mathbf{V} \cdot \mathbf{\Sigma} \cdot \mathbf{V^{-1}}\)</span> e confirmando que resulta na matriz original <span class="math inline">\(\mathbf{A}\)</span>.</li>
</ul></li>
</ol>
<p><strong>Observa√ß√£o</strong>: A decomposi√ß√£o espectral √© aplic√°vel a matrizes que s√£o diagonalis√°veis<a href="#fn58" class="footnote-ref" id="fnref58"><sup>58</sup></a></p>
<p><strong>Exemplo de Resolu√ß√£o Num√©rica</strong></p>
<p>Aqui, novamente, vamos fazer uso explic√≠to do ChatGPT para mostrarmos de como utiliz√°-lo como ajuda.</p>
<p>Perguntando ao ChatGPT desde modo:</p>
<p>¬†</p>
<hr />
<p><em>Encontre os autovalores e autovetores para a matriz abaixo:</em></p>
<pre><code>&gt; cov(data)
          x1        x2
x1 1.0000000 0.8303746
x2 0.8303746 1.0000000
&gt;</code></pre>
<p><em>Use somente elementos de √°lgebra. N√£o use fun√ß√µes prontas do R.</em></p>
<hr />
<p>¬†</p>
<p>Tivemos como resposta:</p>
<p>¬†</p>
<hr />
<p>Para encontrar os autovalores e autovetores da matriz de covari√¢ncia</p>
<p><span class="math display">\[
A = \begin{bmatrix} 1.0000000 &amp; 0.8303746 \\ 0.8303746 &amp; 1.0000000 \end{bmatrix}
\]</span></p>
<p>podemos proceder usando m√©todos da √°lgebra linear. O primeiro passo √© encontrar os autovalores <span class="math inline">\(\lambda\)</span> resolvendo o <strong>polin√¥mio caracter√≠stico</strong>.</p>
<p><strong>Passo 1: Determinar o polin√¥mio caracter√≠stico</strong></p>
<p>O polin√¥mio caracter√≠stico √© dado por:</p>
<p><span class="math display">\[
\text{det}(A - \lambda I) = 0
\]</span></p>
<p>onde <span class="math inline">\(I\)</span> √© a matriz identidade de mesma ordem que <span class="math inline">\(A\)</span>, e <span class="math inline">\(\lambda\)</span> s√£o os autovalores. Assim, temos:</p>
<p><span class="math display">\[
\text{det} \left( \begin{bmatrix} 1.0000000 &amp; 0.8303746 \\ 0.8303746 &amp; 1.0000000 \end{bmatrix} - \lambda \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix} \right) = 0
\]</span></p>
<p>Isto se traduz na seguinte equa√ß√£o:</p>
<p><span class="math display">\[
\text{det} \left( \begin{bmatrix} 1.0000000 - \lambda &amp; 0.8303746 \\ 0.8303746 &amp; 1.0000000 - \lambda \end{bmatrix} \right) = 0
\]</span></p>
<p>O determinante de uma matriz <span class="math inline">\(2 \times 2\)</span> √© dado por:</p>
<p><span class="math display">\[
\text{det}(A) = (a_{11} \cdot a_{22}) - (a_{12} \cdot a_{21})
\]</span></p>
<p>Aplicando isso √† matriz <span class="math inline">\(A - \lambda I\)</span>:</p>
<p><span class="math display">\[
(1.0000000 - \lambda)(1.0000000 - \lambda) - (0.8303746 \cdot 0.8303746) = 0
\]</span></p>
<p>Simplificando:</p>
<p><span class="math display">\[
(1.0000000 - \lambda)^2 - 0.8303746^2 = 0
\]</span></p>
<p>Expanda os termos:</p>
<p><span class="math display">\[
(1.0000000 - \lambda)^2 = 1 - 2.0000000\lambda + \lambda^2
\]</span>
<span class="math display">\[
0.8303746^2 = 0.6895219
\]</span></p>
<p>Agora temos a equa√ß√£o:</p>
<p><span class="math display">\[
1 - 2.0000000\lambda + \lambda^2 - 0.6895219 = 0
\]</span></p>
<p>Simplifique:</p>
<p><span class="math display">\[
\lambda^2 - 2.0000000\lambda + 0.3104781 = 0
\]</span></p>
<p><strong>Passo 2: Resolver a equa√ß√£o quadr√°tica</strong></p>
<p>Agora, podemos resolver essa equa√ß√£o quadr√°tica usando a f√≥rmula de Bhaskara:</p>
<p><span class="math display">\[
\lambda = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}
\]</span></p>
<p>Onde <span class="math inline">\(a = 1\)</span>, <span class="math inline">\(b = -2.0000000\)</span> e <span class="math inline">\(c = 0.3104781\)</span>. Calculando o discriminante:</p>
<p><span class="math display">\[
\Delta = (-2.0000000)^2 - 4(1)(0.3104781)
\]</span>
<span class="math display">\[
\Delta = 4.0000000 - 1.2419124 = 2.7580876
\]</span></p>
<p>Agora, extra√≠mos a raiz quadrada do discriminante:</p>
<p><span class="math display">\[
\sqrt{\Delta} = \sqrt{2.7580876} \approx 1.660211
\]</span></p>
<p>Agora, podemos calcular os autovalores:</p>
<p><span class="math display">\[
\lambda_1 = \frac{2.0000000 + 1.660211}{2} = \frac{3.660211}{2} \approx 1.830106
\]</span>
<span class="math display">\[
\lambda_2 = \frac{2.0000000 - 1.660211}{2} = \frac{0.339789}{2} \approx 0.169894
\]</span></p>
<p><strong>Passo 3: Encontrar os autovetores</strong></p>
<p>Agora que temos os autovalores <span class="math inline">\(\lambda_1 = 1.830106\)</span> e <span class="math inline">\(\lambda_2 = 0.169894\)</span>, podemos encontrar os autovetores resolvendo o sistema <span class="math inline">\((A - \lambda I)v = 0\)</span> para cada autovalor.</p>
<p><strong>Para <span class="math inline">\(\lambda_1 = 1.830106\)</span>:</strong></p>
<p>A equa√ß√£o <span class="math inline">\((A - \lambda_1 I)v = 0\)</span> √©:</p>
<p><span class="math display">\[
\begin{bmatrix} 1.0000000 - 1.830106 &amp; 0.8303746 \\ 0.8303746 &amp; 1.0000000 - 1.830106 \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \end{bmatrix} = 0
\]</span></p>
<p>Isso resulta no sistema:</p>
<p><span class="math display">\[
\begin{bmatrix} -0.830106 &amp; 0.8303746 \\ 0.8303746 &amp; -0.830106 \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \end{bmatrix} = 0
\]</span></p>
<p>Resolvendo, obtemos uma rela√ß√£o entre <span class="math inline">\(v_1\)</span> e <span class="math inline">\(v_2\)</span>. Para simplifica√ß√£o, o autovetor associado ser√° proporcional a:</p>
<p><span class="math display">\[
v_1 \approx v_2
\]</span></p>
<p><strong>Para <span class="math inline">\(\lambda_2 = 0.169894\)</span>:</strong></p>
<p>Similarmente, para <span class="math inline">\(\lambda_2\)</span>:</p>
<p><span class="math display">\[
\begin{bmatrix} 1.0000000 - 0.169894 &amp; 0.8303746 \\ 0.8303746 &amp; 1.0000000 - 0.169894 \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \end{bmatrix} = 0
\]</span></p>
<p>Isso resulta no sistema:</p>
<p><span class="math display">\[
\begin{bmatrix} 0.830106 &amp; 0.8303746 \\ 0.8303746 &amp; 0.830106 \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \end{bmatrix} = 0
\]</span></p>
<p>Resolvendo, temos que:</p>
<p><span class="math display">\[
v_1 \approx -v_2
\]</span></p>
<p><strong>Resumo dos autovalores e autovetores (n√£o normalizados):</strong></p>
<ul>
<li>Autovalores: <span class="math inline">\(\lambda_1 \approx 1.830106\)</span> e <span class="math inline">\(\lambda_2 \approx 0.169894\)</span></li>
<li>Autovetores (n√£o normalizados):
<ul>
<li>Para <span class="math inline">\(\lambda_1\)</span>: <span class="math inline">\(v_1 = \begin{bmatrix} 1 \\ 1 \end{bmatrix}\)</span></li>
<li>Para <span class="math inline">\(\lambda_2\)</span>: <span class="math inline">\(v_2 = \begin{bmatrix} 1 \\ -1 \end{bmatrix}\)</span></li>
</ul></li>
</ul>
<p><strong>Passo 4: Normalizar os autovetores</strong></p>
<p>Para normalizar os autovetores, devemos dividir cada autovetor pela sua norma (ou m√≥dulo). A norma de um vetor <span class="math inline">\(\mathbf{v} = \begin{bmatrix} v_1 \\ v_2 \end{bmatrix}\)</span> √© dada por:</p>
<p><span class="math display">\[
\|\mathbf{v}\| = \sqrt{v_1^2 + v_2^2}
\]</span></p>
<p>Agora, vamos calcular a norma de cada um deles e dividi-los por essa norma.</p>
<p><strong>Para o autovetor <span class="math inline">\(\mathbf{v_1} = \begin{bmatrix} 1 \\ 1 \end{bmatrix}\)</span>:</strong></p>
<p><span class="math display">\[
\|\mathbf{v_1}\| = \sqrt{1^2 + 1^2} = \sqrt{2}
\]</span></p>
<p>O autovetor normalizado √©:</p>
<p><span class="math display">\[
\mathbf{\hat{v_1}} = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ 1 \end{bmatrix} = \begin{bmatrix} \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} \end{bmatrix}
\]</span></p>
<p>Ou, de forma aproximada:</p>
<p><span class="math display">\[
\mathbf{\hat{v_1}} = \begin{bmatrix} 0.707 \\ 0.707 \end{bmatrix}
\]</span></p>
<p><strong>Para o autovetor <span class="math inline">\(\mathbf{v_2} = \begin{bmatrix} 1 \\ -1 \end{bmatrix}\)</span>:</strong></p>
<p><span class="math display">\[
\|\mathbf{v_2}\| = \sqrt{1^2 + (-1)^2} = \sqrt{2}
\]</span></p>
<p>O autovetor normalizado √©:</p>
<p><span class="math display">\[
\mathbf{\hat{v_2}} = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ -1 \end{bmatrix} = \begin{bmatrix} \frac{1}{\sqrt{2}} \\ \frac{-1}{\sqrt{2}} \end{bmatrix}
\]</span></p>
<p>Ou, de forma aproximada:</p>
<p><span class="math display">\[
\mathbf{\hat{v_2}} = \begin{bmatrix} 0.707 \\ -0.707 \end{bmatrix}
\]</span></p>
<p><strong>Resumo dos autovetores normalizados:</strong></p>
<ul>
<li>Autovetor normalizado para <span class="math inline">\(\lambda_1 = 1.830106\)</span>:</li>
</ul>
<p><span class="math display">\[
\mathbf{\hat{v_1}} = \begin{bmatrix} 0.707 \\ 0.707 \end{bmatrix}
\]</span></p>
<ul>
<li>Autovetor normalizado para <span class="math inline">\(\lambda_2 = 0.169894\)</span>:</li>
</ul>
<p><span class="math display">\[
\mathbf{\hat{v_2}} = \begin{bmatrix} 0.707 \\ -0.707 \end{bmatrix}
\]</span></p>
<hr />
<p>Finalizar com alguns coment√°rios:</p>
<pre><code>&gt; sqrt( 1.830106 )
[1] 1.352814
&gt; sqrt( 0.169894 )
[1] 0.412182
&gt;
&gt;
&gt; 1.830106 / ( 1.830106 + 0.169894 )
[1] 0.915053
&gt; 0.169894 / ( 1.830106 + 0.169894 )
[1] 0.084947
&gt; </code></pre>
</details>
<p>¬†</p>
<ol start="2" style="list-style-type: decimal">
<li><strong>Decomposi√ß√£o em Valores Singulares (SVD)</strong></li>
</ol>
<p>A Decomposi√ß√£o em Valores Singulares (<em>Singular Value Decomposition - SVD</em>)<a href="#fn59" class="footnote-ref" id="fnref59"><sup>59</sup></a> √© considerada uma generaliza√ß√£o da Decomposi√ß√£o Espectral para casos gerais de matrizes retangulares. √â uma t√©cnica que fatoriza uma matriz <span class="math inline">\(\mathbf{X}\)</span> de dimens√µes <span class="math inline">\(n \times m\)</span> em tr√™s matrizes:</p>
<p>¬†</p>
<p><span class="math display">\[ \mathbf{X} = \mathbf{U} \cdot \mathbf{\Sigma} \cdot \mathbf{V^T} \]</span></p>
<p>¬†¬†¬†¬†¬†¬† onde:</p>
<ul>
<ul>
<li>
<span class="math inline">\(\mathbf{U}\)</span> √© uma matriz ortonormal<span class="math inline">\(\dagger\)</span> de dimens√£o <span class="math inline">\(n \times n\)</span>. √â a matriz de vetores singulares a esquerda.
</li>
<li>
<span class="math inline">\(\mathbf{V}\)</span> √© uma matriz ortonormal<span class="math inline">\(\dagger\)</span> de dimens√£o <span class="math inline">\(m \times m\)</span>. √â a matriz de vetores singulares a direita.
</li>
<li>
<p><span class="math inline">\(\mathbf{\Sigma}\)</span> √© uma matriz de dimens√£o <span class="math inline">\(n \times m\)</span> onde os valores singulares est√£o efetivamente organizados em uma submatriz diagonal <span class="math inline">\(r \times r\)</span>, onde <span class="math inline">\(r\)</span> √© o posto da matriz <span class="math inline">\(\mathbf{X}\)</span> (o n√∫mero de valores singulares n√£o nulos e n√£o-negativos). O valor de <span class="math inline">\(r\)</span> pode ser no m√°ximo igual a <span class="math inline">\(min(n,m)\)</span>. Os valores singulares em <span class="math inline">\(\mathbf{\Sigma}\)</span> indicam a import√¢ncia das dire√ß√µes representadas pelas colunas de <span class="math inline">\(\mathbf{U}\)</span> e <span class="math inline">\(\mathbf{V}\)</span>.</p>
<p><strong><span class="math inline">\(\dagger\)</span>Nota</strong>: Uma matriz <span class="math inline">\(\mathbf{M}\)</span> ser ortonormal significa que suas colunas s√£o vetores ortogonais entre si e t√™m norma unit√°ria<a href="#fn60" class="footnote-ref" id="fnref60"><sup>60</sup></a>. Isso garante que <span class="math inline">\(\mathbf{M^T} \cdot \mathbf{M} = \mathbf{I}\)</span>, onde <span class="math inline">\(\mathbf{I}\)</span> √© a matriz identidade.</p>
</ul>
</ul>
<p>¬†</p>
<p>Na An√°lise de Componentes Principais (PCA), utiliza-se da decomposi√ß√£o da SVD pois as matrizes <span class="math inline">\(\mathbf{P}\)</span> (<em>loadings</em>) e <span class="math inline">\(\mathbf{T}\)</span> (<em>scores</em>) podem ser diretamente obtidas pelas rela√ß√µes:</p>
<p>¬†</p>
<p><span class="math display">\[
\mathbf{P} = \mathbf{V}
\]</span></p>
<p>¬†</p>
<p><span class="math display">\[
\mathbf{T} = \mathbf{U} \cdot \mathbf{\Sigma}
\]</span></p>
<p>¬†</p>
<p>Nas implementa√ß√µes da decomposi√ß√£o SVD tamb√©m n√£o √© poss√≠vel ter dados faltantes. Caso exista na matriz de dados, o usu√°rio deve tratar os dados perdidos previamente.
No R, existe a fun√ß√£o <code>prcomp</code> que implementa PCA via SVD. Deve ser o m√©todo de prefer√™ncia pela melhor precis√£o num√©rica conforme relatada no manual e pelo que evidencia as equa√ß√µes. Consulte o manual da fun√ß√£o pelo comando: <code>?prcomp</code>.</p>
<ol start="3" style="list-style-type: decimal">
<li>o m√©todo <strong>NIPALS - Non-linear Iterative Partial Least Squares</strong></li>
</ol>
<p><strong>Propriedade</strong></p>
<p>o algoritmo NIPALS permite extrair componentes principais sem necessidade de imputar valores ausentes previamente (desde de que devidamente implementado) sendo uma vantagem significativa em rela√ß√£o a outros m√©todos que n√£o lidam bem com dados faltantes.</p>
<p>No R, n√£o h√° fun√ß√£o nativa mas h√° o m√©todo em alguns pacotes. Em especial, vale mencionar o pacote <code>nipals</code> que implementa o algoritmo com op√ß√£o padr√£o da execu√ß√£o da ortogonaliza√ß√£o de Gram-Schmidt em cada itera√ß√£o. Isso √© necess√°rio pois ‚Ä¶</p>
<p>¬†</p>
<ol start="4" style="list-style-type: decimal">
<li><strong>PCoA - Principal Coordinates Analysis</strong></li>
</ol>
<p>PCA √© PCoA se equivalem quando a dist√¢ncia a ser preservada √© a euclidiana.</p>
<p>XX¬†</p>
<p>XX¬†</p>
<p>XX¬†</p>
<p>¬†</p>
<p>¬†</p>
<p>¬†</p>
<p><strong>EXERC√çCIO para entender resolu√ß√£o n√∫merica</strong></p>
<p>Desenvolvimento das equa√ß√µes de PCA, incluindo como obter <span class="math inline">\(W\)</span> de forma expl√≠cita:</p>
<p>1a. <strong>Centraliza√ß√£o dos dados:</strong>
<span class="math display">\[
   X&#39; = X - \mu
   \]</span>
onde <span class="math inline">\(\mu\)</span> √© o vetor das m√©dias de cada coluna de <span class="math inline">\(X\)</span>.</p>
<p>Passo a passo para extrair os autovetores correspondentes aos autovalores para formar a matriz <span class="math inline">\(W\)</span>:</p>
<p>1b. <strong>C√°lculo da matriz de covari√¢ncia:</strong>
<span class="math display">\[
   C = \frac{1}{n-1} X&#39;^{T} X&#39;
   \]</span></p>
<ol start="2" style="list-style-type: decimal">
<li><strong>Resolu√ß√£o da equa√ß√£o caracter√≠stica:</strong>
<ul>
<li>Calcule os autovalores <span class="math inline">\(\lambda\)</span> resolvendo:
<span class="math display">\[
\text{det}(C - \lambda I) = 0
\]</span>
Isso lhe dar√° uma lista de autovalores.</li>
</ul></li>
<li><strong>C√°lculo dos autovetores:</strong>
<ul>
<li>Para cada autovalor <span class="math inline">\(\lambda_i\)</span>, resolva o sistema:
<span class="math display">\[
(C - \lambda_i I) \mathbf{v}_i = 0
\]</span>
onde <span class="math inline">\(\mathbf{v}_i\)</span> √© o autovetor correspondente a <span class="math inline">\(\lambda_i\)</span>.</li>
<li>Este sistema pode ser resolvido usando m√©todos como elimina√ß√£o gaussiana ou decomposi√ß√£o de valores singulares (SVD).</li>
</ul></li>
<li><strong>Forma√ß√£o da matriz <span class="math inline">\(W\)</span>:</strong>
<ul>
<li>Organize os autovalores <span class="math inline">\(\lambda_i\)</span> em ordem decrescente.</li>
<li>Extraia os autovetores correspondentes a esses autovalores e os coloque em colunas para formar a matriz <span class="math inline">\(W\)</span>:
<span class="math display">\[
W = [\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_k]
\]</span>
onde <span class="math inline">\(k\)</span> √© o n√∫mero de componentes principais que voc√™ deseja manter.</li>
</ul></li>
<li><strong>Proje√ß√£o dos dados:</strong>
<span class="math display">\[
Z = X&#39;W
\]</span></li>
</ol>
<p>Esse processo permite que voc√™ extraia os autovetores e construa a matriz <span class="math inline">\(W\)</span> para a An√°lise de Componentes Principais (PCA).</p>
</div>
</div>
<div id="vari√¢ncia-por-pc" class="section level2 hasAnchor" number="6.6">
<h2><span class="header-section-number">6.6</span> Vari√¢ncia por PC<a href="pca---an√°lise-de-componentes-principais.html#vari√¢ncia-por-pc" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><a href="https://pt.wikipedia.org/wiki/Matrizes_semelhantes" class="uri">https://pt.wikipedia.org/wiki/Matrizes_semelhantes</a></p>
</div>
<div id="aplica√ß√£o-t√≠pica" class="section level2 hasAnchor" number="6.7">
<h2><span class="header-section-number">6.7</span> Aplica√ß√£o T√≠pica<a href="pca---an√°lise-de-componentes-principais.html#aplica√ß√£o-t√≠pica" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<!--
1) O que √© SVD?
2) Mostre o passo a passo para se fazer essa decomposi√ß√£o - SVD -  de uma matriz 2x2. Exemplifique e explique as etapas.
3) Resolva o exemplo anterior usando o m√©todo de Jacobi.
4) Do exemplo anterior, resolu√ß√£o de SVD pelo m√©todo de Jacobi, fa√ßa um fun√ß√£o em R puro, para esse exemplo.
5) Forne√ßa a matrix usada anterior para aplicar a fun√ß√£o anterior em c√≥digo R
-->
<p>¬†</p>
<p>¬†</p>
<p>¬†</p>
<p>¬†</p>
<p>¬†</p>
<details>
<summary>
üòàAlgoritmo para SVD
</summary>
<p><br><strong>O algoritmo apresentado abaixo √© apenas uma possibilidade dentre v√°rios</strong><br><br></p>
<p>O m√©todo de Jacobi √© uma abordagem iterativa que pode ser aplicada √† SVD, embora n√£o seja a mais eficiente para matrizes grandes. √â apresentado para incentivar o conhecimento de se ter ao menos uma alternativa de resolu√ß√£o do problema com apenas conceitos b√°sicos de √°lgebra. Ou seja, sem o uso de fun√ß√µes prontas e abordagem <strong>caixa-preta</strong>.</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="pca---an√°lise-de-componentes-principais.html#cb37-1" tabindex="-1"></a><span class="co"># Fun√ß√£o para calcular SVD usando o m√©todo de Jacobi</span></span>
<span id="cb37-2"><a href="pca---an√°lise-de-componentes-principais.html#cb37-2" tabindex="-1"></a>jacobi_svd <span class="ot">&lt;-</span> <span class="cf">function</span>(A, <span class="at">tol =</span> <span class="fl">1e-10</span>, <span class="at">max_iter =</span> <span class="dv">100</span>) {</span>
<span id="cb37-3"><a href="pca---an√°lise-de-componentes-principais.html#cb37-3" tabindex="-1"></a>  <span class="co"># Inicializa√ß√£o</span></span>
<span id="cb37-4"><a href="pca---an√°lise-de-componentes-principais.html#cb37-4" tabindex="-1"></a>  n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(A)</span>
<span id="cb37-5"><a href="pca---an√°lise-de-componentes-principais.html#cb37-5" tabindex="-1"></a>  m <span class="ot">&lt;-</span> <span class="fu">ncol</span>(A)</span>
<span id="cb37-6"><a href="pca---an√°lise-de-componentes-principais.html#cb37-6" tabindex="-1"></a>  U <span class="ot">&lt;-</span> <span class="fu">diag</span>(n)</span>
<span id="cb37-7"><a href="pca---an√°lise-de-componentes-principais.html#cb37-7" tabindex="-1"></a>  V <span class="ot">&lt;-</span> <span class="fu">diag</span>(m)</span>
<span id="cb37-8"><a href="pca---an√°lise-de-componentes-principais.html#cb37-8" tabindex="-1"></a>  </span>
<span id="cb37-9"><a href="pca---an√°lise-de-componentes-principais.html#cb37-9" tabindex="-1"></a>  <span class="co"># Calcular A^T A</span></span>
<span id="cb37-10"><a href="pca---an√°lise-de-componentes-principais.html#cb37-10" tabindex="-1"></a>  ATA <span class="ot">&lt;-</span> <span class="fu">t</span>(A) <span class="sc">%*%</span> A</span>
<span id="cb37-11"><a href="pca---an√°lise-de-componentes-principais.html#cb37-11" tabindex="-1"></a>  </span>
<span id="cb37-12"><a href="pca---an√°lise-de-componentes-principais.html#cb37-12" tabindex="-1"></a>  <span class="co"># Itera√ß√µes de Jacobi</span></span>
<span id="cb37-13"><a href="pca---an√°lise-de-componentes-principais.html#cb37-13" tabindex="-1"></a>  <span class="cf">for</span> (iter <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>max_iter) {</span>
<span id="cb37-14"><a href="pca---an√°lise-de-componentes-principais.html#cb37-14" tabindex="-1"></a>    <span class="co"># Encontrar o maior elemento fora da diagonal</span></span>
<span id="cb37-15"><a href="pca---an√°lise-de-componentes-principais.html#cb37-15" tabindex="-1"></a>    off_diag <span class="ot">&lt;-</span> <span class="fu">which</span>(<span class="fu">abs</span>(ATA) <span class="sc">&gt;</span> tol, <span class="at">arr.ind =</span> <span class="cn">TRUE</span>)</span>
<span id="cb37-16"><a href="pca---an√°lise-de-componentes-principais.html#cb37-16" tabindex="-1"></a>    off_diag <span class="ot">&lt;-</span> off_diag[off_diag[, <span class="dv">1</span>] <span class="sc">!=</span> off_diag[, <span class="dv">2</span>], ]</span>
<span id="cb37-17"><a href="pca---an√°lise-de-componentes-principais.html#cb37-17" tabindex="-1"></a>    </span>
<span id="cb37-18"><a href="pca---an√°lise-de-componentes-principais.html#cb37-18" tabindex="-1"></a>    <span class="cf">if</span> (<span class="fu">nrow</span>(off_diag) <span class="sc">==</span> <span class="dv">0</span>) {</span>
<span id="cb37-19"><a href="pca---an√°lise-de-componentes-principais.html#cb37-19" tabindex="-1"></a>      <span class="cf">break</span>  <span class="co"># Convergido</span></span>
<span id="cb37-20"><a href="pca---an√°lise-de-componentes-principais.html#cb37-20" tabindex="-1"></a>    }</span>
<span id="cb37-21"><a href="pca---an√°lise-de-componentes-principais.html#cb37-21" tabindex="-1"></a>    </span>
<span id="cb37-22"><a href="pca---an√°lise-de-componentes-principais.html#cb37-22" tabindex="-1"></a>    <span class="co"># Selecionar o primeiro elemento fora da diagonal</span></span>
<span id="cb37-23"><a href="pca---an√°lise-de-componentes-principais.html#cb37-23" tabindex="-1"></a>    p <span class="ot">&lt;-</span> off_diag[<span class="dv">1</span>, <span class="dv">1</span>]</span>
<span id="cb37-24"><a href="pca---an√°lise-de-componentes-principais.html#cb37-24" tabindex="-1"></a>    q <span class="ot">&lt;-</span> off_diag[<span class="dv">1</span>, <span class="dv">2</span>]</span>
<span id="cb37-25"><a href="pca---an√°lise-de-componentes-principais.html#cb37-25" tabindex="-1"></a>    </span>
<span id="cb37-26"><a href="pca---an√°lise-de-componentes-principais.html#cb37-26" tabindex="-1"></a>    <span class="co"># C√°lculo dos √¢ngulos de rota√ß√£o</span></span>
<span id="cb37-27"><a href="pca---an√°lise-de-componentes-principais.html#cb37-27" tabindex="-1"></a>    <span class="cf">if</span> (ATA[p, p] <span class="sc">==</span> ATA[q, q]) {</span>
<span id="cb37-28"><a href="pca---an√°lise-de-componentes-principais.html#cb37-28" tabindex="-1"></a>      theta <span class="ot">&lt;-</span> pi <span class="sc">/</span> <span class="dv">4</span></span>
<span id="cb37-29"><a href="pca---an√°lise-de-componentes-principais.html#cb37-29" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb37-30"><a href="pca---an√°lise-de-componentes-principais.html#cb37-30" tabindex="-1"></a>      theta <span class="ot">&lt;-</span> <span class="fu">atan</span>(<span class="dv">2</span> <span class="sc">*</span> ATA[p, q] <span class="sc">/</span> (ATA[p, p] <span class="sc">-</span> ATA[q, q])) <span class="sc">/</span> <span class="dv">2</span></span>
<span id="cb37-31"><a href="pca---an√°lise-de-componentes-principais.html#cb37-31" tabindex="-1"></a>    }</span>
<span id="cb37-32"><a href="pca---an√°lise-de-componentes-principais.html#cb37-32" tabindex="-1"></a>    </span>
<span id="cb37-33"><a href="pca---an√°lise-de-componentes-principais.html#cb37-33" tabindex="-1"></a>    <span class="co"># Matriz de rota√ß√£o</span></span>
<span id="cb37-34"><a href="pca---an√°lise-de-componentes-principais.html#cb37-34" tabindex="-1"></a>    c <span class="ot">&lt;-</span> <span class="fu">cos</span>(theta)</span>
<span id="cb37-35"><a href="pca---an√°lise-de-componentes-principais.html#cb37-35" tabindex="-1"></a>    s <span class="ot">&lt;-</span> <span class="fu">sin</span>(theta)</span>
<span id="cb37-36"><a href="pca---an√°lise-de-componentes-principais.html#cb37-36" tabindex="-1"></a>    </span>
<span id="cb37-37"><a href="pca---an√°lise-de-componentes-principais.html#cb37-37" tabindex="-1"></a>    <span class="co"># Atualizar ATA</span></span>
<span id="cb37-38"><a href="pca---an√°lise-de-componentes-principais.html#cb37-38" tabindex="-1"></a>    R <span class="ot">&lt;-</span> <span class="fu">diag</span>(n)</span>
<span id="cb37-39"><a href="pca---an√°lise-de-componentes-principais.html#cb37-39" tabindex="-1"></a>    R[<span class="fu">c</span>(p, q), <span class="fu">c</span>(p, q)] <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(c, <span class="sc">-</span>s, s, c), <span class="at">nrow =</span> <span class="dv">2</span>)</span>
<span id="cb37-40"><a href="pca---an√°lise-de-componentes-principais.html#cb37-40" tabindex="-1"></a>    </span>
<span id="cb37-41"><a href="pca---an√°lise-de-componentes-principais.html#cb37-41" tabindex="-1"></a>    ATA <span class="ot">&lt;-</span> <span class="fu">t</span>(R) <span class="sc">%*%</span> ATA <span class="sc">%*%</span> R</span>
<span id="cb37-42"><a href="pca---an√°lise-de-componentes-principais.html#cb37-42" tabindex="-1"></a>    V <span class="ot">&lt;-</span> V <span class="sc">%*%</span> R</span>
<span id="cb37-43"><a href="pca---an√°lise-de-componentes-principais.html#cb37-43" tabindex="-1"></a>    </span>
<span id="cb37-44"><a href="pca---an√°lise-de-componentes-principais.html#cb37-44" tabindex="-1"></a>    <span class="co"># Calcular os autovalores</span></span>
<span id="cb37-45"><a href="pca---an√°lise-de-componentes-principais.html#cb37-45" tabindex="-1"></a>    <span class="cf">if</span> (<span class="fu">all</span>(<span class="fu">abs</span>(ATA[off_diag]) <span class="sc">&lt;</span> tol)) {</span>
<span id="cb37-46"><a href="pca---an√°lise-de-componentes-principais.html#cb37-46" tabindex="-1"></a>      <span class="cf">break</span></span>
<span id="cb37-47"><a href="pca---an√°lise-de-componentes-principais.html#cb37-47" tabindex="-1"></a>    }</span>
<span id="cb37-48"><a href="pca---an√°lise-de-componentes-principais.html#cb37-48" tabindex="-1"></a>  }</span>
<span id="cb37-49"><a href="pca---an√°lise-de-componentes-principais.html#cb37-49" tabindex="-1"></a>  </span>
<span id="cb37-50"><a href="pca---an√°lise-de-componentes-principais.html#cb37-50" tabindex="-1"></a>  <span class="co"># Valores singulares</span></span>
<span id="cb37-51"><a href="pca---an√°lise-de-componentes-principais.html#cb37-51" tabindex="-1"></a>  singular_values <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">diag</span>(ATA))</span>
<span id="cb37-52"><a href="pca---an√°lise-de-componentes-principais.html#cb37-52" tabindex="-1"></a>  </span>
<span id="cb37-53"><a href="pca---an√°lise-de-componentes-principais.html#cb37-53" tabindex="-1"></a>  <span class="co"># Calcular U</span></span>
<span id="cb37-54"><a href="pca---an√°lise-de-componentes-principais.html#cb37-54" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n) {</span>
<span id="cb37-55"><a href="pca---an√°lise-de-componentes-principais.html#cb37-55" tabindex="-1"></a>    <span class="cf">if</span> (singular_values[i] <span class="sc">&gt;</span> <span class="dv">0</span>) {</span>
<span id="cb37-56"><a href="pca---an√°lise-de-componentes-principais.html#cb37-56" tabindex="-1"></a>      U[, i] <span class="ot">&lt;-</span> A <span class="sc">%*%</span> V[, i] <span class="sc">/</span> singular_values[i]</span>
<span id="cb37-57"><a href="pca---an√°lise-de-componentes-principais.html#cb37-57" tabindex="-1"></a>    }</span>
<span id="cb37-58"><a href="pca---an√°lise-de-componentes-principais.html#cb37-58" tabindex="-1"></a>  }</span>
<span id="cb37-59"><a href="pca---an√°lise-de-componentes-principais.html#cb37-59" tabindex="-1"></a>  </span>
<span id="cb37-60"><a href="pca---an√°lise-de-componentes-principais.html#cb37-60" tabindex="-1"></a>  <span class="co"># Retornar os resultados</span></span>
<span id="cb37-61"><a href="pca---an√°lise-de-componentes-principais.html#cb37-61" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">U =</span> U, <span class="at">D =</span> <span class="fu">diag</span>(singular_values), <span class="at">V =</span> V))</span>
<span id="cb37-62"><a href="pca---an√°lise-de-componentes-principais.html#cb37-62" tabindex="-1"></a>}</span>
<span id="cb37-63"><a href="pca---an√°lise-de-componentes-principais.html#cb37-63" tabindex="-1"></a></span>
<span id="cb37-64"><a href="pca---an√°lise-de-componentes-principais.html#cb37-64" tabindex="-1"></a><span class="co"># Matriz exemplo</span></span>
<span id="cb37-65"><a href="pca---an√°lise-de-componentes-principais.html#cb37-65" tabindex="-1"></a>A <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">4</span>, <span class="dv">0</span>, <span class="dv">3</span>, <span class="sc">-</span><span class="dv">5</span>), <span class="at">nrow =</span> <span class="dv">2</span>, <span class="at">byrow =</span> <span class="cn">TRUE</span>)</span>
<span id="cb37-66"><a href="pca---an√°lise-de-componentes-principais.html#cb37-66" tabindex="-1"></a></span>
<span id="cb37-67"><a href="pca---an√°lise-de-componentes-principais.html#cb37-67" tabindex="-1"></a><span class="co"># Chamar a fun√ß√£o</span></span>
<span id="cb37-68"><a href="pca---an√°lise-de-componentes-principais.html#cb37-68" tabindex="-1"></a>result <span class="ot">&lt;-</span> <span class="fu">jacobi_svd</span>(A)</span>
<span id="cb37-69"><a href="pca---an√°lise-de-componentes-principais.html#cb37-69" tabindex="-1"></a></span>
<span id="cb37-70"><a href="pca---an√°lise-de-componentes-principais.html#cb37-70" tabindex="-1"></a><span class="co"># Resultados</span></span>
<span id="cb37-71"><a href="pca---an√°lise-de-componentes-principais.html#cb37-71" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Matriz U:</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## Matriz U:</code></pre>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="pca---an√°lise-de-componentes-principais.html#cb39-1" tabindex="-1"></a><span class="fu">print</span>(result<span class="sc">$</span>U)</span></code></pre></div>
<pre><code>##            [,1]       [,2]
## [1,]  0.8944272 -0.4472136
## [2,] -0.4472136 -0.8944272</code></pre>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="pca---an√°lise-de-componentes-principais.html#cb41-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;</span><span class="sc">\n</span><span class="st">Matriz D (valores singulares):</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## 
## Matriz D (valores singulares):</code></pre>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="pca---an√°lise-de-componentes-principais.html#cb43-1" tabindex="-1"></a><span class="fu">print</span>(result<span class="sc">$</span>D)</span></code></pre></div>
<pre><code>##          [,1]     [,2]
## [1,] 3.162278 0.000000
## [2,] 0.000000 6.324555</code></pre>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="pca---an√°lise-de-componentes-principais.html#cb45-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;</span><span class="sc">\n</span><span class="st">Matriz V:</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## 
## Matriz V:</code></pre>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="pca---an√°lise-de-componentes-principais.html#cb47-1" tabindex="-1"></a><span class="fu">print</span>(result<span class="sc">$</span>V)</span></code></pre></div>
<pre><code>##           [,1]       [,2]
## [1,] 0.7071068 -0.7071068
## [2,] 0.7071068  0.7071068</code></pre>
<p><strong>Algoritmos para SVD</strong></p>
<p>Existem v√°rios algoritmos para calcular a SVD, incluindo:</p>
<ul>
<li>M√©todo de Jacobi: Uma abordagem iterativa, mas geralmente mais lenta para matrizes grandes.</li>
<li>Algoritmo de Golub-Reinsch: Um m√©todo mais eficiente que combina a decomposi√ß√£o QR e o m√©todo de Jacobi.</li>
<li>Algoritmo de Lanczos: Usado para matrizes grandes e esparsas; baseia-se em m√©todos de subespa√ßo.</li>
<li>SVD baseado em bidiagonaliza√ß√£o: Usa decomposi√ß√µes bidiagonais, seguido por uma SVD da matriz bidiagonal.</li>
<li>Algoritmos de fatora√ß√£o de matrizes de baixa-rank: T√©cnicas como o m√©todo de pot√™ncia e m√©todos estoc√°sticos.</li>
</ul>
<p><strong>Efici√™ncia dos Algoritmos</strong></p>
<p>O algoritmo de Golub-Reinsch √© um dos mais utilizados na pr√°tica e considerado eficiente para a maioria das aplica√ß√µes. Para matrizes muito grandes ou esparsas, m√©todos baseados em Lanczos ou bidiagonaliza√ß√£o tendem a ser mais eficientes.</p>
<p>A escolha do algoritmo SVD mais eficiente depende do tamanho e da estrutura da matriz que voc√™ est√° tratando, bem como das necessidades espec√≠ficas da aplica√ß√£o.</p>
<p>Em bibliotecas computacionais otimizadas, tais como <em>BLAS</em><a href="#fn61" class="footnote-ref" id="fnref61"><sup>61</sup></a>, <em>LAPACK</em><a href="#fn62" class="footnote-ref" id="fnref62"><sup>62</sup></a>, <em>MKL</em><a href="#fn63" class="footnote-ref" id="fnref63"><sup>63</sup></a> em geral as fun√ß√µes implementadas trazem esses diversos algoritmos e alguma an√°lise inicial da matriz determina o algoritmo mais adequado para a situa√ß√£o espec√≠fica.</p>
</details>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="46">
<li id="fn46"><p><a href="https://pt.wikipedia.org/wiki/An%C3%A1lise_de_componentes_principais" class="uri">https://pt.wikipedia.org/wiki/An%C3%A1lise_de_componentes_principais</a><a href="pca---an√°lise-de-componentes-principais.html#fnref46" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn47"><p><a href="https://pt.wikipedia.org/wiki/Karl_Pearson" class="uri">https://pt.wikipedia.org/wiki/Karl_Pearson</a><a href="pca---an√°lise-de-componentes-principais.html#fnref47" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn48"><p>Pearson, K. (1901). LIII. <em>On lines and planes of closest fit to systems of points in space</em>,
<strong>The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science</strong>,
2(11), 559‚Äì572. <a href="https://doi.org/10.1080/14786440109462720" class="uri">https://doi.org/10.1080/14786440109462720</a><a href="pca---an√°lise-de-componentes-principais.html#fnref48" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn49"><p><a href="https://pt.wikipedia.org/wiki/Harold_Hotelling" class="uri">https://pt.wikipedia.org/wiki/Harold_Hotelling</a><a href="pca---an√°lise-de-componentes-principais.html#fnref49" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn50"><p>Hotelling, H. (1933). <em>Analysis of a complex of statistical variables into principal components</em>, <strong>Journal of Educational Psychology</strong>, 24(6), 417‚Äì441. <a href="https://doi.org/10.1037/h0071325" class="uri">https://doi.org/10.1037/h0071325</a><a href="pca---an√°lise-de-componentes-principais.html#fnref50" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn51"><p><a href="https://pt.wikipedia.org/wiki/Teorema_espectral" class="uri">https://pt.wikipedia.org/wiki/Teorema_espectral</a><a href="pca---an√°lise-de-componentes-principais.html#fnref51" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn52"><p><a href="https://en.wikipedia.org/wiki/Spectral_theorem" class="uri">https://en.wikipedia.org/wiki/Spectral_theorem</a><a href="pca---an√°lise-de-componentes-principais.html#fnref52" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn53"><p><a href="https://pt.wikipedia.org/wiki/Autovalores_e_autovetores" class="uri">https://pt.wikipedia.org/wiki/Autovalores_e_autovetores</a><a href="pca---an√°lise-de-componentes-principais.html#fnref53" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn54"><p><a href="https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors" class="uri">https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors</a><a href="pca---an√°lise-de-componentes-principais.html#fnref54" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn55"><p><a href="https://pt.wikipedia.org/wiki/Covari%C3%A2ncia" class="uri">https://pt.wikipedia.org/wiki/Covari%C3%A2ncia</a><a href="pca---an√°lise-de-componentes-principais.html#fnref55" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn56"><p><a href="https://pt.wikipedia.org/wiki/V%C3%ADrgula_flutuante" class="uri">https://pt.wikipedia.org/wiki/V%C3%ADrgula_flutuante</a><a href="pca---an√°lise-de-componentes-principais.html#fnref56" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn57"><p><a href="https://pt.wikipedia.org/wiki/Determinante" class="uri">https://pt.wikipedia.org/wiki/Determinante</a><a href="pca---an√°lise-de-componentes-principais.html#fnref57" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn58"><p><a href="https://pt.wikipedia.org/wiki/Matriz_diagonaliz%C3%A1vel" class="uri">https://pt.wikipedia.org/wiki/Matriz_diagonaliz%C3%A1vel</a><a href="pca---an√°lise-de-componentes-principais.html#fnref58" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn59"><p><a href="https://pt.wikipedia.org/wiki/Decomposi%C3%A7%C3%A3o_em_valores_singulares" class="uri">https://pt.wikipedia.org/wiki/Decomposi%C3%A7%C3%A3o_em_valores_singulares</a><a href="pca---an√°lise-de-componentes-principais.html#fnref59" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn60"><p><a href="https://pt.wikipedia.org/wiki/Ortonormalidade" class="uri">https://pt.wikipedia.org/wiki/Ortonormalidade</a><a href="pca---an√°lise-de-componentes-principais.html#fnref60" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn61"><p><a href="https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms" class="uri">https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms</a><a href="pca---an√°lise-de-componentes-principais.html#fnref61" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn62"><p><a href="https://en.wikipedia.org/wiki/LAPACK" class="uri">https://en.wikipedia.org/wiki/LAPACK</a><a href="pca---an√°lise-de-componentes-principais.html#fnref62" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn63"><p><a href="https://en.wikipedia.org/wiki/Math_Kernel_Library" class="uri">https://en.wikipedia.org/wiki/Math_Kernel_Library</a><a href="pca---an√°lise-de-componentes-principais.html#fnref63" class="footnote-back">‚Ü©Ô∏é</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="parte-iii---an√°lise-explorat√≥ria-de-dados.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="hca---an√°lise-de-agrupamento-hier√°rquico.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsubsection",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"info": true,
"post_processor": null
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
