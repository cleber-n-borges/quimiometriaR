[["index.html", "Caderno de Quimiometria com R Bem-vindos!", " Caderno de Quimiometria com R Bem-vindos! ♦♦♦ Este caderno foi criado para ser um guia acessível para todos que desejam explorar e aplicar conceitos de quimiometria utilizando a linguagem de programação R. Este material foi cuidadosamente elaborado para atender às necessidades dos estudantes de graduação que já cursaram uma disciplina de Álgebra Linear. Foi desenvolvido com suporte do ChatGPT, uma ferramenta avançada de inteligência artificial projetada para auxiliar no aprendizado e na prática de diversas disciplinas. Recomendamos que você também utilize o ChatGPT como um recurso adicional no seu processo de aprendizado. Ele pode ser um aliado valioso para esclarecer dúvidas, explorar novos tópicos e aprofundar seus conhecimentos. Estamos confiantes de que este caderno será uma ferramenta importante e inspiradora em sua jornada pelo mundo da quimiometria com R. Aproveite cada seção, explore os exemplos e desafios, e não hesite em usar o ChatGPT para apoiar seu aprendizado. Boa leitura e boas análises! Atenciosamente, Equipe de Desenvolvimento do Caderno de Quimiometria com R ♦♦♦ Cleber Nogueira Borges Atualizado em: 17 de outubro de 2024 Nota: Em fase inicial do desenvolvimento "],["útil-para-você.html", "Útil para Você!", " Útil para Você! Veja por que aprender Quimiometria com R pode ser uma escolha incrível para você! Listamos várias razões que tornam esse aprendizado interessante e valioso: Aplicabilidade em Diversas Áreas: Quimiometria pode ser utilizada em setores como alimentos, farmacêutico, ambiental, e muitos outros, facilitando a análise de dados e tomada de decisões. Conexão entre Teoria e Prática: Conhecimento analítico, tal como Análise de dados, é uma ponte vital entre a teoria química e a prática real. Com este caderno, você terá a oportunidade de aplicar conceitos teóricos em análises práticas usando a linguagem R. Isso não apenas solidifica seu entendimento, mas também proporciona habilidades práticas em em estatística e programação que são altamente valorizadas no mercado de trabalho. Aprendizado Ativo e Interativo: Ao explorar exemplos práticos e realizar exercícios desafiadores, você se envolverá ativamente com o conteúdo. Esse tipo de aprendizado é muito mais eficaz do que a simples leitura passiva. O caderno foi projetado para ajudá-lo a explorar, praticar e dominar os métodos quimiométricos de forma interativa. Ferramenta Moderna de Aprendizado e Acesso a Recursos Educacionais: O ChatGPT, que auxiliou na criação deste material, é uma tecnologia de ponta que pode se tornar um recurso valioso no seu processo de aprendizagem. Usá-lo para esclarecer dúvidas, explorar novos tópicos e aprofundar seus conhecimentos pode acelerar seu desenvolvimento e enriquecer sua experiência educacional. Custo Benefício, Personalização e Flexibilidade: R é uma ferramenta open-source e gratuita, proporcionando acesso a recursos de quimiometria de alta qualidade sem a necessidade de investir em softwares caros. Com R, você pode personalizar seus scripts e criar soluções sob medida para suas necessidades específicas de análise. Poderosa Ferramenta Estatística: O conhecimento que você adquirirá através deste caderno não só ampliará suas habilidades técnicas. Você será capaz de enfrentar problemas complexos com um conjunto robusto de ferramentas analíticas e estatísticas. O R oferece uma ampla gama de pacotes especializados em análise quimiométrica, permitindo realizar desde as análises mais simples até as mais complexas com precisão e sendo reprodutíveis. Preparação para Desafios Reais: O domínio da quimiometria e do R é uma vantagem significativa no mundo científico e industrial. Este caderno foi projetado para prepará-lo para desafios reais, fornecendo uma base sólida e as habilidades necessárias para se destacar em projetos de pesquisa e desenvolvimento. A responsabilidade pela sua aprendizagem e progresso é sua. O futuro de sua carreira e suas oportunidades de crescimento dependem de suas escolhas e do esforço que você dedica aos estudos. Comece agora a explorar e dominar a quimiometria com R para garantir um desenvolvimento sólido e promissor na sua área. Aproveite ao Máximo… Possibilidades são muitas… Se envolva! Mergulhe Profundamente: Não se limite a ler o material; explore-o ativamente. Teste os códigos, faça os exercícios e reflita sobre os conceitos discutidos. Utilize o ChatGPT: Não hesite em usar o ChatGPT para obter ajuda adicional, esclarecer dúvidas ou buscar explicações mais detalhadas sobre tópicos que despertam seu interesse. Pratique Regularmente: A prática constante é essencial para o domínio de qualquer habilidade. Dedique tempo para praticar o que aprendeu e aplique os métodos quimiométricos a novos problemas e conjuntos de dados. Colabore e Compartilhe: Envolva-se em discussões com colegas sobre quimiometria e R. Compartilhar suas descobertas e colaborar com outros pode proporcionar novas perspectivas e enriquecer seu aprendizado. Desafie-se Continuamente: Procure por questões difíceis e buscar soluções inovadoras ajudará a consolidar suas habilidades e ampliar seu conhecimento na prática. Estamos aqui para apoiar sua jornada e acreditamos que este caderno será uma ferramenta poderosa para seu sucesso. Abra a mente, mergulhe no conteúdo e deixe sua curiosidade e paixão pela ciência guiá-lo. A aventura na quimiometria com R começa agora! "],["pergunte-ao-chatgpt.html", "Pergunte ao ChatGPT", " Pergunte ao ChatGPT Para aprender Quimiometria com R, é uma boa ideia fazer perguntas específicas sobre tópicos ou técnicas que você está estudando. Aqui estão algumas dicas sobre como formular suas perguntas: Identifique o Conceito ou Técnica: Se você está começando, pode querer entender o conceito básico da quimiometria ou como aplicar uma técnica específica usando R. Por exemplo, “O que é análise de componentes principais (PCA) na quimiometria?” Peça Exemplos Práticos: Se você quer ver como um conceito é aplicado em R, pergunte por exemplos práticos. Por exemplo, “Como faço uma análise de componentes principais (PCA) em R usando um conjunto de dados de espectros de absorção?” Solicite Código de Exemplo: Se precisar de ajuda com código específico, você pode pedir exemplos ou explicações de trechos de código. Exemplo, “Pode me mostrar como carregar um conjunto de dados de quimiometria em R e aplicar uma análise de regressão parcial dos mínimos quadrados (PLS)?” Peça Ajuda com Pacotes R: Pergunte sobre pacotes específicos do R que são úteis para quimiometria. Demonstração: “Quais pacotes R são recomendados para realizar análise de regressão em quimiometria?” Explique Dúvidas sobre Resultados ou Erros: Se encontrar problemas com o código ou resultados, explique o que está acontecendo. Uma possível, “Estou recebendo um erro ao tentar executar uma análise PLS no R. O que pode estar causando isso e como resolver?” Solicite Recomendações de Recursos: Se precisar de mais material para estudo, pergunte sobre livros, artigos ou tutoriais. Veja, “Quais são alguns bons recursos ou livros para aprender quimiometria com R?” Aqui estão mais ilustrações de casos de pedidos ao ChatGPT que você poderia fazer: “Como posso importar e preparar meus dados de espectrometria em R para análise de quimiometria?” “Pode me mostrar um exemplo de como realizar uma análise de cluster em um conjunto de dados de química usando R?” “Qual é a diferença entre a análise de componentes principais (PCA) e a análise de discriminante linear (LDA) na quimiometria, e como implemento cada uma em R?” “Estou tendo dificuldades em interpretar os resultados da análise PLS em R. Pode me ajudar a entender como ler os scores e loadings?” “Pode explicar o conceito de análise de componentes principais (PCA) e como ele é utilizado na quimiometria?” “Como posso instalar e carregar pacotes específicos no R que são úteis para quimiometria?” “Como posso preparar e limpar meus dados antes de aplicar um modelo de quimiometria em R?” “Estou recebendo um erro ao tentar rodar um código de regressão no R. Pode me ajudar a entender o que está acontecendo e como corrigir?” Ao fazer perguntas específicas e detalhadas, você pode obter respostas mais úteis e direcionadas ao que precisa aprender. Essas perguntas podem ajudá-lo a esclarecer dúvidas, aprofundar seu conhecimento e resolver problemas específicos enquanto explora o caderno e aplica as técnicas de quimiometria com R. Não hesite em fazer perguntas específicas sobre o conteúdo que você está estudando ou sobre qualquer desafio que encontrar. Comece a explorar o ChatGPT, em: https://chatgpt.com, e inicie seus testes de imediato!… "],["estrutura-do-caderno.html", "Estrutura do Caderno", " Estrutura do Caderno A estrutura do Caderno de Quimiometria com R foi projetada para fornecer uma abordagem clara e prática para o aprendizado de quimiometria usando a linguagem de programação R. Aqui está uma visão geral da estrutura do caderno:   Parte I - Ponto de Partida Primeiro Passo: Apresentação dos objetivos e benefícios do material. Sobre a Quimiometria: Definição e importância da quimiometria na ciência e na indústria. Instalação do R e RStudio: Instruções para instalar e configurar o R e o RStudio. Comandos Básicos: Conhecimento de comandos usuais em análises fundamentais. Uso de Pacotes Essenciais: Como instalar e carregar pacotes R Conjuntos de Dados: Como utilizar os dados no R. Visualização de Dados: Como criar gráficos e visualizações eficazes para análise quimiométrica.   Parte II - Tratamento dos Dados Pré-Processamentos: Procedimentos prévios à análise a realizar em amostras e variáveis.   Parte III - Análise Explorátoria de Dados Análise de Componentes Principais (PCA): Conceitos, aplicação e exemplos de código em R. Análise de Cluster: Métodos para segmentar e agrupar dados, incluindo exemplos de clustering hierárquico e K-means.   Parte IV - Classificação TODO:   Parte V - Regressão Análise de Regressão: Introdução à regressão linear e não linear, com exemplos práticos.   "],["parte-i---ponto-de-partida.html", "PARTE I - Ponto de Partida Primeiro Passo O que é Quimiometria? Aplicações do R na Quimiometria Contribuições do ChatGPT", " PARTE I - Ponto de Partida Primeiro Passo Iniciar a leitura de um novo material acadêmico pode ser tanto uma aventura empolgante quanto um desafio intimidador. Quando se trata de um campo específico como a quimiometria, que combina química e estatística para interpretar dados complexos, o começo é um momento crucial. O caderno de Quimiometria com R é mais do que um simples guia; ele representa o ponto de partida para uma jornada de descobertas e aprendizado profundo. Imagine-se no início de uma trilha em uma floresta desconhecida. O caderno de Quimiometria com R é esse mapa que nos orienta, apontando os caminhos a seguir e preparando-nos para a caminhada pela vasta paisagem da análise de dados químicos. O R, uma linguagem e ambiente de programação, é a ferramenta que nos permitirá explorar e entender os dados com uma precisão que seria difícil de alcançar de outra forma. Ao abrir o caderno pela primeira vez, é natural sentir uma mistura de ansiedade e curiosidade. O conteúdo pode parecer denso e técnico, mas cada seção está estruturada para construir uma base sólida, começando com os conceitos fundamentais de quimiometria e avançando gradualmente para aplicações mais complexas. O texto é escrito para ser acessível, começando com uma introdução clara ao que é a quimiometria e a importância do R nesse contexto. O primeiro contato com a programação em R pode ser desafiador, mas é essencial para o sucesso na quimiometria. O caderno não apenas apresenta o código, mas também explica a lógica por trás dele, desmistificando o processo e transformando o aprendizado em algo palpável. A prática é a chave, e o caderno está repleto de exemplos e exercícios que permitem aplicar o que foi aprendido de forma prática e imediata. Cada capítulo é uma oportunidade para mergulhar mais fundo nas técnicas de analíse, como análise de componentes principais (PCA), modelos de regressão e validação cruzada. Essas ferramentas são poderosas e permitem extrair informações valiosas dos dados químicos, ajudando na tomada de decisões e na solução de problemas complexos. A leitura do caderno de Quimiometria com R é, portanto, um processo de construção gradual de conhecimento. É o momento de estabelecer uma base sólida, de entender o que é quimiometria e como o R pode ser utilizado para explorar dados químicos de maneira eficiente. À medida que avançamos, cada conceito aprendido serve como uma pedra fundamental para a compreensão de tópicos mais avançados. Ao final dessa jornada, o que parecia um ponto de partida desafiador se transforma em uma sólida base de conhecimento e habilidades. O caderno de Quimiometria com R é o primeiro passo em uma trilha que pode levar a grandes realizações na análise de dados químicos. Com dedicação e prática, o que começou como uma leitura inicial se tornará uma ferramenta indispensável para enfrentar novos desafios e explorar novas fronteiras na ciência dos dados. Em última análise, o ponto de partida é mais do que o início de um estudo; é o começo de uma jornada intelectual que se desdobra à medida que mergulhamos nos tópicos e nos tornamos proficientes na aplicação das técnicas de quimiometria. Com o caderno como guia e o R como ferramenta, estamos prontos para explorar e descobrir as nuances dos dados químicos, um passo de cada vez. O que é Quimiometria? A quimiometria1, 2 é uma subárea da química que se dedica à aplicação de métodos estatísticos e matemáticos para analisar e interpretar dados químicos. Ela envolve métodos como planejamento e otimização de experimentos, análise multivariada, modelagem preditiva e validação de resultados. Seu objetivo principal é extrair o máximo de informações significativas a partir de dados de experimentos químicos, frequentemente gerados por técnicas analíticas avançadas como espectroscopia, cromatografia e eletroforese. Essencialmente, a quimiometria busca melhorar a compreensão dos processos químicos e analíticos, otimizar métodos experimentais, e interpretar dados de sistemas químicos de forma mais eficaz. Para aprender A melhor maneira de começar a entender, estudar e domiar Quimiometria é seguir estas etapas: primeiro, reforce conhecimentos básicos de química analítica e estatística, pois são essenciais para entender os métodos quimiométricos. Em seguida, aprenda a trabalhar com matrizes de dados, que são comuns em Quimiometria, especialmente no contexto de análises multivariadas. Para avançar, considere livros sobre o assunto que expliquem os conceitos formais de cada tema. Estude exemplos práticos e aplicações reais de Quimiometria para compreender como os métodos são aplicados em problemas concretos. Por fim, considere participar de minicursos, workshops ou seminários. Esses passos proporcionarão uma base sólida para compreender e aplicar os conceitos da Quimiometria. Aplicações do R na Quimiometria O R, uma linguagem de programação e ambiente de software para estatística, é amplamente utilizado na quimiometria devido à sua robustez em análise de dados e visualização. Começar a usar o R para aplicações quimiométricas traz uma série de benefícios. Primeiramente, o R é uma linguagem open-source, o que significa que é acessível para qualquer pessoa, sem custos de licenciamento. Para a quimiometria, que envolve análise estatística avançada, modelos preditivos e tratamento de grandes conjuntos de dados, o R oferece uma vasta coleção de pacotes dedicados especificamente a essas tarefas, como o chemometrics e pls, permitindo a implementação de métodos clássicos, como a regressão por mínimos quadrados parciais (PLS) e análise de componentes principais (PCA). A flexibilidade do R também se destaca, uma vez que ele permite customização quase ilimitada dos modelos e análises, o que é crucial em quimiometria, onde ajustes finos e experimentos com diferentes abordagens são comuns. Além disso, a capacidade de visualização do R, especialmente com pacotes como ggplot2 e plotly, facilita a interpretação dos resultados de maneira clara e interativa, ajudando a explorar os dados de forma mais eficiente. Outro ponto forte é a comunidade ativa e colaborativa. No campo da quimiometria, onde as técnicas estão em constante evolução, a presença de uma comunidade que desenvolve e compartilha novos pacotes e soluções é uma vantagem significativa. O R também tem integração com outras linguagens, como Python e C++, e compatibilidade com bancos de dados e ferramentas de big data, o que é interessante para projetos maiores ou com necessidades de alta performance. Por outro lado, há algumas desvantagens. O R pode ter uma curva de aprendizado mais acentuada, especialmente para quem não tem familiaridade com programação ou estatística. Apesar de ser uma linguagem poderosa, o R pode ser mais lento em comparação a alternativas como Python quando se trata de manipulação de grandes volumes de dados, embora isso possa ser mitigado com o uso de pacotes específicos ou integração com ferramentas de alta performance. Além disso, a quantidade de pacotes pode gerar uma certa confusão para os iniciantes, uma vez que, para o mesmo tipo de análise, pode haver múltiplos pacotes disponíveis, cada um com suas peculiaridades. Por fim, para quem já está acostumado com softwares proprietários específicos para quimiometria, como o Unscrambler ou SIMCA, a mudança para o R pode exigir um esforço maior de adaptação, pois o R, sendo uma linguagem de programação, exige uma maior responsabilidade no controle do fluxo de trabalho e nos parâmetros dos modelos. Apesar desses desafios, o que se ganha com o R em termos de versatilidade, custo-benefício e inovação compensa os possíveis obstáculos iniciais. Para quem se habitua a usar o R, a flexibilidade e a vasta gama de pacotes disponíveis tornam-no uma ferramenta poderosa e versátil para a quimiometria, permitindo que os químicos analistas realizem análises detalhadas e complexas com eficácia. Abaixo seguem exemplos de pacotes sofisticados, totalmente desenvolvidos em R e dedicados à quimiometria. A maior parte deles possui uma vinheta trazendo explicações e exemplos típicos do uso do pacote. A vinheta é vista usando o comando: browseVignettes('nome_do_pacote') mdatools é um pacote R para pré-processamento, exploração e análise de dados multivariados. O pacote fornece métodos mais comuns para quimiometria. URL: https://mdatools.com/ O pacote prospectr reúne algoritmos comumente usados em espectroscopia para pré-tratamento de espectros e amostras de calibração selecionadas. URL: http://antoinestevens.github.io/prospectr/ ChemoSpec é uma coleção de funções para análise exploratória de dados espectrais: NMR, IR, XRF e outros tipos similares de espcetróscopias. URL: https://bryanhanson.github.io/ChemoSpec/ hyperSpec é um pacote R que permite o manuseio conveniente de conjuntos de dados hiperespectrais, ou seja, conjuntos de dados que combinam espectros com dados adicionais por espectro. URL: https://r-hyperspec.github.io/hyperSpec/ chromatographR é um pacote para análise reproduzível de dados HPLC-DAD em R. URL: https://ethanbass.github.io/chromatographR O pacote RGCxGC apresenta um pipeline ponta a ponta para a análise de dados abrangentes de cromatografia gasosa bidimensional (GCxGC-MS). URL: https://github.com/DanielQuiroz97/RGCxGC O chromConverter visa facilitar a conversão de dados de cromatografia de vários formatos proprietários para que possam ser facilmente lidos em R para análises posteriores. URL: https://github.com/ethanbass/chromConverter O pacote ChromR traz algumas funções para auxiliar analistas, Ph.D. estudantes e técnicos para planejar e organizar análises de LC. URL: https://github.com/ebruginski/ChromR/ O pacote R Rnmr1D tem como objetivo realizar o processamento completo de um conjunto de espectros de RMN 1D pelos dados brutos do FID. URL: https://github.com/INRA/Rnmr1D speaq: Ferramentas para alinhamento de espectros de ressonância magnética nuclear (NMR), processamento baseado em picos, análise quantitativa e visualizações. URL: https://github.com/Beirnaert/speaq GCalignR: fornece funções para alinhar listas de picos. O pacote suporta qualquer outra técnica de cromatografia unidimensional que permita ao usuário criar uma lista de picos com pelo menos uma coluna especificando tempos de retenção. Use: browseVignettes(\"GCalignR\"). URL: https://github.com/mottensmann/GCalignR Estudar o software R é uma excelente escolha, pois essa ferramenta consagrada oferece um poder inigualável na análise quimiométricas, preparando você para enfrentar desafios complexos com confiança e precisão. Para Aprender Familiarize-se com a linguagem R, suas sintaxes e estruturas básicas. Preocupe-se em aprender a importar e exportar diferentes tipos de arquivos. Em seguida, explore pacotes específicos de R usados em Quimiometria, como chemometrics, mdatools, pls, class, cluster, que fornecem funções e ferramentas voltadas para análise quimiométrica. Consulte o CRAN Task Views3! O Talk Views visa fornecer orientação sobre quais pacotes no CRAN são relevantes para tarefas relacionadas a um determinado tópico. Eles fornecem uma breve visão geral dos pacotes incluídos que também podem ser instalados automaticamente usando o pacote ctv. As opiniões pretendem ter um foco nítido para que fique suficientemente claro quais pacotes devem ser incluídos (ou excluídos) - e não pretendem endossar os “melhores” pacotes para uma determinada tarefa. Para a Quimiometria, há o Task View: Chemometrics and Computational Physics, que lista os pacotes úteis para a área de Quimiometria e Física Computacional. URL: https://CRAN.R-project.org/view=ChemPhys Pratique aplicando o R a conjuntos de dados químicos, disponíveis em pacotes de R ou em sites especializados. Dica: Várias são as Dicas pensadas e deixadas ao longo do texto do Caderno! Execute as recomendações para adquirir conhecimento sobre os temas de maneira gradativa. Contribuições do ChatGPT O ChatGPT tem sido útil em fornecer explicações e respostas rápidas sobre a aplicação de métodos quimiométricos e o uso do R. Exemplos incluem esclarecimentos sobre técnicas específicas e orientações sobre boas práticas na análise de dados químicos. Além disso, pode fornecer orientações sobre visualização de dados para identificar padrões, análise de qualidade dos dados para identificar e tratar outliers, e a manipulação de dados para preparação antes da análise. O ChatGPT pode reproduzir e explicar algoritmos simples e até alguns sendo mais complexos (desde que se tenha cautela), demonstrar como utilizá-los, e ajudar a evidenciar suas aplicações. Para testes estatísticos, o ChatGPT pode explicar como elaborar e avaliar modelos, além de aplicar técnicas para melhorar previsões. Em aplicações específicas de quimiometria, pode oferecer orientações sobre a análise de dados cromatográficos e espectroscópicos. Finalmente, o ChatGPT pode ajudar a automatizar análises e garantir a reprodutibilidade dos resultados, assegurando eficiência e precisão no processos analítico sobre os dados. Tenha o endereço https://chatgpt.com/ em mente para te ajudar. Resumindo… Combinando o poder analítico do R e o suporte informativo do ChatGPT, você terá ajuda em como abordar e resolver problemas quimiométricos de forma mais eficaz e precisa. Esperamos que estes apontamentos forneça uma visão sobre como utilizar essas ferramentas para realizar suas análises de dados químicos. https://pt.wikipedia.org/wiki/Quimiometria↩︎ https://en.wikipedia.org/wiki/Chemometrics↩︎ https://cran-r.c3sl.ufpr.br/web/views/↩︎ "],["comece-a-usar-o-r.html", "&#8474;&#8477; 1 Comece a Usar o R 1.1 Instale o R e RStudio 1.2 Conheça o Básico do R 1.3 Realize Análises Simples 1.4 A Conveniência do help() 1.5 Manuseie Arquivos 1.6 Salve e Restaure seu Workspace 1.7 Instale Pacotes R 1.8 Aprenda Mais e Pratique 1.9 Participar de Comunidades", " &#8474;&#8477; 1 Comece a Usar o R Começar a usar o R pode ser uma experiência empolgante e enriquecedora, especialmente se você está interessado em análise de dados e estatística. Aqui está um guia para você iniciar com o R: 1.1 Instale o R e RStudio Siga as instruções: Instale o R: Acesse o site do CRAN (Comprehensive R Archive Network)4. Selecione o link para o seu sistema operacional (Windows, macOS, Linux). Baixe e execute o instalador, seguindo as instruções na tela. Instale o RStudio (opcional, mas fortemente recomendado): Vá para o site do RStudio5. Baixe a versão gratuita (RStudio Desktop Open Source License). Instale o RStudio após a instalação do R. RStudio fornece um ambiente de desenvolvimento integrado (IDE) que facilita o uso do R. Abra o RStudio: Ao abrir o RStudio, você verá várias janelas, incluindo o console, o editor de scripts, o ambiente de trabalho e o painel de arquivos. Você já poderá executar comandos no console. Dica: Não se esqueça de visitar o site oficial do R: https://www.r-project.org/. 1.2 Conheça o Básico do R Quando você usa uma função nativa do R, você segue uma sintaxe que envolve chamar a função pelo seu nome e passar argumentos entre parênteses. Sintaxe geral: nome_da_funcao(argumento1, argumento2, ...) Parênteses: Envolvem os argumentos da função. Argumentos: Valores ou variáveis que a função precisa para executar sua tarefa. Veja um exemplo: Executar Códigos Simples: Experimente comandos básicos no console do RStudio: # Criar um vetor x &lt;- c(1, 2, 3, 4, 5) # Calcular a média mean(x) # Calcular o desvio padrão sd(x) Aqui está a descrição dessa estrutura básica: #: o sinal (chamado cerquilha) indica que a linha contém um comentário explicativo x: é um nome que o usuário escolhe para receber o resultado &lt;-: é o operação de atribuição (que é um caso muito particular de função!) c(): é a função que combina valores em um vetor 1, 2, 3, 4, 5 são os dados digitados pelo usuário mean(): é O nome da função que calcula o valor médio. sd(): é O nome da função que calcula o valor do desvio padrão.   Copie o código acima e cole-o no console do R. Você obterá a seguinte saída como resultado: &gt; # Criar um vetor &gt; x &lt;- c(1, 2, 3, 4, 5) &gt; &gt; # Calcular a média &gt; mean(x) [1] 3 &gt; &gt; # Calcular o desvio padrão &gt; sd(x) [1] 1.581139 &gt; O valor da média dos dados é igual a 3 e o valor do desvio padrão é igual a 1,58.   Pronto! Você acabou de realizar sua primeira análise no R. Parabéns! Continue assim e explore ainda mais as possibilidades que o R oferece. 1.3 Realize Análises Simples Faça seus primeiros testes executando comandos simples de análises básicas. Estatísticas Descritivas: # Gera 20 pontos de um distribuição Gaussiana padrão x &lt;- rnorm( 20 ) # Estatísticas básicas summary(x) mean(x) var(x) sd(x) Gráfico de dispersão: # Gera 20 pontos de um distribuição Gaussiana padrão x &lt;- rnorm( 20 ) y &lt;- rnorm( 20 ) # Gráfico simples de dispersão de pontos plot( x, y ) Dica: Descubra detalhes sobre o que faz a função rnorm() com o comando help( rnorm ). Estude cada uma das 7 funções usadas anteriormente com help( nome_da_ função ). 1.4 A Conveniência do help() O comando help() no R é uma ferramenta fundamental para obter informações detalhadas sobre funções, pacotes, e outros objetos do R. Utilizar help() pode melhorar significativamente a eficiência e a eficácia na utilização do R, pois fornece detalhes cruciais sobre como usar as funções e interpretar seus resultados. Dica: Caso você queira os ler os manuais em português, pergunte ao ChatGPT: “Como ajustar o R para ter os manuais em pt_BR?” O ChatGPT explicará muitos detalhes sobre o comando abaixo: Sys.setenv(LANG = &quot;pt_BR.UTF-8&quot;) Como Usar o Comando help(): Obter Ajuda para uma Função Específica Para obter informações sobre uma função específica, use help() seguido do nome da função. Por exemplo, para obter ajuda sobre a função mean(), você pode usar: help(mean) Ou, você também pode usar o símbolo de interrogação como uma alternativa: ?mean Obter Ajuda para um Pacote Se você deseja obter uma lista de funções e informações sobre um pacote específico, você pode usar: help(package = &quot;cluster&quot;) Isso exibe uma lista de todas as funções e datasets disponíveis no pacote cluster, além de um link para a documentação do pacote. Pesquisar por Funções e Conceitos Para buscar uma função ou conceito sem saber o nome exato, você pode usar help.search(). Por exemplo, para procurar funções relacionadas a “linear regression”, você pode usar: help.search(&quot;linear regression&quot;) Alternativamente, o comando ?? faz a mesma busca e pode ser usado como: ?? linear regression Acessar a Documentação do Pacote Muitos pacotes têm um arquivo de documentação que pode ser acessado diretamente com: browseVignettes(package = &quot;ggplot2&quot;) Isso abre a página de vignettes do pacote no navegador, fornecendo documentação detalhada e exemplos. Metainformação E evidentemente, execute o comando no R: help( help ) para mais detalhes. Conveniência da Prática Acesso Imediato à Documentação: Utilizando help(), você pode acessar rapidamente a documentação das funções e pacotes sem sair do ambiente de programação. Isso economiza tempo e melhora a eficiência na escrita de código. Compreensão dos Argumentos e Uso: A documentação fornece detalhes sobre os argumentos que uma função aceita, seu propósito, e exemplos de uso. Isso ajuda a evitar erros e a entender como usar as funções corretamente. Exploração de Pacotes: Quando você instala novos pacotes, help() permite explorar as funções disponíveis e entender o que cada pacote oferece, facilitando a integração de novos recursos em seu trabalho. Resolução de Problemas: Se você encontrar problemas ao usar uma função, a documentação geralmente inclui informações sobre erros comuns e como resolvê-los, o que pode ajudar a depurar o código. Aprendizado Contínuo: Consultar a documentação regularmente ajuda a aprender mais sobre as funcionalidades do R e a manter-se atualizado sobre novos pacotes e métodos. Documentação Completa: O uso de help() proporciona acesso a documentação completa, incluindo descrições, argumentos, detalhes e exemplos, que pode ser mais abrangente do que uma simples busca na web. Exemplo de Uso Vamos dizer que você está trabalhando com a função lm() para regressão linear e deseja entender mais sobre seus argumentos e exemplos. Você pode digitar: ?lm Isso abrirá a página de ajuda da função lm(), onde você encontrará uma descrição completa da função, os parâmetros que ela aceita, exemplos de uso e detalhes sobre a saída. Dica: Pergunte ao ChatGPT como se estrutura uma página de ajuda sobre função R.   Em resumo, o comando help() no R é uma ferramenta essencial para a exploração e utilização eficiente das funcionalidades do R, oferecendo acesso direto à documentação e facilitando o desenvolvimento e análise no ambiente R. 1.5 Manuseie Arquivos Saber manusear arquivos em R é fundamental para qualquer análise de dados, pois permite a importação e exportação de dados de e para diversos formatos. Importação com read.table(): Essa função é essencial para carregar dados de arquivos de texto (como CSV ou TSV) para dentro do R. Sem a capacidade de importar dados, seria impossível realizar análises com informações que estão armazenadas em arquivos externos. Exportação com write.table(): Depois de realizar análises ou manipulações, a função write.table() permite salvar os resultados em arquivos de texto. Isso é importante para compartilhar resultados, criar relatórios ou armazenar dados para uso futuro. Essas funções facilitam a integração de R com outros sistemas e garantem que dados possam ser facilmente movidos e utilizados entre diferentes plataformas. A função recebe como argumento o caminho do arquivo, o qual deve conter os dados como texto puro e em formato tabular e retorna um data.frame a partir dele, com casos correspondentes a linhas e variáveis a campos da tabela. read.table() Descrição: A função read.table() é utilizada para importar dados de arquivos de texto contendo dados organizados em formato tabular delimitados por tabulação, espaços, ou outros delimitadores. Ela recebe como argumento o caminho do arquivo e retorna um objeto do tipo data.frame, onde cada linha do arquivo corresponde a uma observação e cada coluna a uma variável. A função é flexível e permite ajustar os parâmetros delimitadores de campo, nomes de colunas e tipos de dados para adequar-se à estrutura do arquivo de entrada. Sintaxe: read.table(file, header = FALSE, sep = &quot;&quot;, ...) file: O nome do arquivo ou URL. header: Indica se o primeiro registro contém os nomes das colunas (TRUE ou FALSE). sep: O delimitador dos campos, como “,” para CSV, “\\t” para tabulação, ou ” ” para espaço. …: Outros argumentos como colClasses, nrows, skip, etc. Exemplo: data &lt;- read.table(&quot;dados.txt&quot;, header = TRUE, sep = &quot;\\t&quot;) write.table() Descrição: A função write.table() é utilizada para exportar um objeto data.frame para um arquivo de texto em formato tabular. É frequentemente usada para salvar resultados de análises ou manipulações de dados. Ela recebe como argumento o caminho do arquivo de destino e o data.frame a ser salvo, e permite ajustar parâmetros como o delimitador de campo, a inclusão de cabeçalhos de coluna e amostras e o formato dos dados. O resultado é um arquivo de texto onde cada linha representa uma observação e cada coluna representa uma variável, estruturado de acordo com as especificações fornecidas. Sintaxe: write.table(x, file, sep = &quot; &quot;, row.names = TRUE, col.names = TRUE, ...) x: O objeto a ser exportado, geralmente um data frame ou matriz. file: O nome do arquivo para o qual os dados serão exportados. sep: O delimitador a ser usado entre os campos no arquivo de saída. row.names: Indica se os nomes das linhas devem ser incluídos. col.names: Indica se os nomes das colunas devem ser incluídos. Exemplo: write.table(data, &quot;dados_exportados.txt&quot;, sep = &quot;,&quot;, row.names = FALSE) Dica: Essas funções possuem variações como read.csv(), write.csv() e scan() para formatos específicos e leitura mais controlada. A leitura com atenção do manual sobre essas funções é obrigatória para qualquer analista! 1.6 Salve e Restaure seu Workspace Para garantir que todo o seu trabalho no R seja preservado e que você possa retomar suas atividades sem perder dados ou configurações, é importante salvar seu ambiente de trabalho - Workspace - regularmente. Aqui estão os passos para salvar seus dados, o histórico de comandos e a sessão de trabalho no R: Salve sua Sessão de Trabalho (Workspace): Para salvar tanto o workspace quanto o histórico de comandos, isto é, o ambiente completo, você pode usar a função save.image, que cria um arquivo .RData com todos os objetos e dados atuais: save.image(file = &quot;sessao_trabalho.RData&quot;) Salvar Objetos em um Arquivo RData: Para salvar todos os objetos do seu workspace (variáveis, funções, etc.) de uma vez, utilize a função save: save(list = ls(), file = &quot;meu_workspace.RData&quot;) Você também pode usar save para salvar objetos individualmente se necessário. Salve o Histórico de Comandos: O R mantém um histórico dos comandos que você executou durante sua sessão. Para salvar este histórico, você pode usar a função savehistory: savehistory(file = &quot;meu_historico.Rhistory&quot;) Isso criará um arquivo .Rhistory contendo todos os comandos que você digitou. Restaurar a Sessão: Quando você quiser retomar seu trabalho, carregue o arquivo .RData com a função load: load(&quot;sessao_trabalho.RData&quot;) Certifique-se de salvar seu trabalho frequentemente para evitar perdas e garantir que você possa continuar de onde parou. Isso é especialmente importante quando você está lidando com análises complexas e dados valiosos. 1.7 Instale Pacotes R Instalar Pacotes: Pacotes são conjuntos de ferramentas adicionais que você pode instalar no R para expandir suas funcionalidades. Eles são essenciais para tornar o R uma ferramenta poderosa, pois permitem que você adicione funções e procedimentos específicos para diferentes áreas da ciência e da análise de dados. Com pacotes, você pode personalizar o R para atender às suas necessidades específicas, tornando-o ainda mais eficiente e versátil.” No console do RStudio, você pode instalar pacotes com o comando install.packages(). Por exemplo: # Instala o pacote mdatools para preprocessamento, # explorar e analisar dados multivariados. install.packages(&quot;mdatools&quot;) # Instala o pacote ggplot2 para visualização de dados install.packages(&quot;ggplot2&quot;) Carregar Pacotes: Após instalar um pacote no R, ele não estará disponível automaticamente para uso. Para começar a usar as funções e recursos do pacote, você precisa carregá-lo na sessão atual com o comando library(). Por exemplo, para carregar o pacote chamado ‘cluster’, você deve usar o comando library(cluster). Lembre-se de fazer isso sempre que iniciar uma nova sessão do R e quiser usar as funcionalidades do pacote.” # Carrega o pacote &quot;cluster&quot; library(cluster) # Carrega o pacote &quot;class&quot; library(class) 1.8 Aprenda Mais e Pratique Considere explorar recursos gratuitos e disponíveis online como: ESTATÍSTICA BÁSICA COM USO DO SOFTWARE R6, Adilson dos Anjos, Departamento de Estatística, UFPR. Introdução à Análise de Dados em R7, Bruno Lucian &amp; Claudia Jakelline, Escola de Gestão &amp; Politicas Públicas da Fundação CEPERJ. Fundamentos de Ecologia Numérica8, Disciplina de Ecologia Numérica do Curso de Ciências Biológicas do Campus V da UEPB, Prof. Elvio S. F. Medeiros, Laboratório de Ecologia, Universidade Estadual da Paraíba, Campus V, João Pessoa, PB; 2024-06-13. Análises de Dados com R9, Ricardo Lehtonen R. de Souza, 2023-08-11. Curso de Estatística com R Project10, Emerson Joucoski, 2024-02-22. Estatística Computacional com R11, Mayer, Bonat, Zeviani, Krainski, Ribeiro Jr, LEG-DEST-UFPR. Conceitos e análises estatísticas com R e JASP12, Luis Anunciação (PUC-Rio), PhD. Computação em R: Análise experimental13, Eric Bastos Gorgens, Marcio Leles Romarco de Oliveira, 2021-02-09. Ecologia numérica 2020/2021: Trabalho 414, Tiago A. Marques, Susana França, Ana Sofia Reboleira e os alunos de Ecologia Numérica 2020/2021, 2020-12-30. A plataforma Coursera15 possui curso gratuito e online sobre R 1.9 Participar de Comunidades Fóruns: R-br16: é a lista Brasileira oficial de discussão do programa R. A [R-br] tem o propósito de permitir a troca de informações entre os usuários de R do Brasil (em português). R-help17: Lista de discussão principal do R Stack Overflow18: Pergunte e responda a perguntas sobre R e programação em geral. RStudio Community19: Uma comunidade dedicada ao R e ao RStudio onde você pode encontrar discussões, perguntas e dicas. Participar dessas comunidades pode ajudar você a resolver problemas, aprender com os outros e se manter atualizado com as melhores práticas.   Praticar regularmente e explorar novos pacotes e técnicas são fundamentais para se tornar proficiente no R. Boa sorte e divirta-se com suas análises! https://cran.r-project.org/↩︎ https://rstudio.com/products/rstudio/download/↩︎ https://docs.ufpr.br/~aanjos/CE002A/estdescritiva.pdf↩︎ https://bookdown.org/bruno_lucian_costa/CursoBasicoR/↩︎ https://bookdown.org/elviomedeiros/fundamentos_de_ecologia_numerica/↩︎ https://bookdown.org/ricardo_lehtonen/anlise_de_dados_com_r/↩︎ https://bookdown.org/joucoski/curso_R_book/↩︎ http://cursos.leg.ufpr.br/ecr/index.html↩︎ https://bookdown.org/luisfca/docs/↩︎ https://gorgens.github.io/compR-experimental/↩︎ https://bookdown.org/tiagoandremarques/trabalho4book/↩︎ https://www.coursera.org/↩︎ http://www.leg.ufpr.br/doku.php/software:rbr↩︎ https://stat.ethz.ch/mailman/listinfo/r-help↩︎ https://stackoverflow.com/questions/tagged/r↩︎ https://community.rstudio.com/↩︎ "],["conjunto-de-dados.html", "&#8474;&#8477; 2 Conjunto de Dados 2.1 Organização e Notação 2.2 Valores faltantes: NA 2.3 Dados Disponíveis", " &#8474;&#8477; 2 Conjunto de Dados Um conjunto de dados é uma coleção estruturada de informações organizadas em linhas e colunas, onde cada linha representa uma observação e cada coluna representa uma variável. Os dados devem ser convenientes para: Análise: Facilitar a análise estatística e identificação de padrões. Modelagem: Servir como base para desenvolver e testar modelos preditivos. Decisão: Apoiar a tomada de decisões informadas em diversos contextos. Relatórios: Permitir a criação de relatórios e a comunicação de insights. Armazenamento: Dados devem ser organizados e armazenados para fácil acesso. Conjuntos de dados são fundamentais para pesquisa, negócios, e tomada de decisões baseadas em evidências. Dica: Pergunte ao ChatGPT O que é um conjunto de dados? Qual objetivo de se ter um? Pra que serve? Caso tenha uma resposta sucinta, repita a pergunta pedindo riqueza de detalhes! 2.1 Organização e Notação O conjunto de dados é organizado em uma estrutura tabular, onde os dados são dispostos em linhas e colunas. A matriz que representa o conjunto de dados é sempre denotada por uma letra maiúscula em negrito, por exemplo, \\(\\mathbf{X}\\). Uma matriz \\(\\mathbf{X}\\) que representa o conjunto de dados pode ser formalmente descrita da seguinte maneira: \\[ \\mathbf{X} = \\begin{bmatrix} x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1j} \\\\ x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2j} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{i1} &amp; x_{i2} &amp; \\cdots &amp; x_{ij} \\end{bmatrix} \\] onde: \\(\\mathbf{X}\\) é a matriz que representa o conjunto de dados. \\(x_{ij}\\): é o elemento localizado na i-ésima linha e j-ésima coluna da matriz \\(\\mathbf{X}\\). Isto é, representa o valor da j-ésima variável para o i-ésimo objeto. Linhas: Cada linha i corresponde a um objeto individual, e todos os valores na linha i representam as medições ou características desse objeto em relação a todas as variáveis. Também é possível interpretar i como sendo o número total de objetos (amostras) no conjunto de dados. Colunas: Cada coluna j representa uma variável específica, e todos os valores na coluna j representam as medições dessa variável em todos os objetos. Também é possível interpretar j como sendo o número total de variáveis (dimensões) medidas. Notações variantes, mais compactas e menos formais, também são comumentes encontrada na literatura como: \\(\\mathbf{X}(I,J)\\). Aqui, temos \\(I\\) linhas e \\(J\\) colunas na matriz \\(\\mathbf{X}\\). Exemplificando… Se tivermos um conjunto de dados com 3 objetos e 4 variáveis, a matriz \\(\\mathbf{X}\\) pode ser representada como: \\[ \\mathbf{X} = \\begin{bmatrix} x_{11} &amp; x_{12} &amp; x_{13} &amp; x_{14} \\\\ x_{21} &amp; x_{22} &amp; x_{23} &amp; x_{24} \\\\ x_{31} &amp; x_{32} &amp; x_{33} &amp; x_{34} \\end{bmatrix} \\] onde \\(x_{ij}\\) representa o valor da j-ésima variável para o i-ésimo objeto. Para as 3 primeiras linhas das 4 variáveis do cojunto de dados Iris, temos a matriz: \\[ \\mathbf{X} = \\begin{bmatrix} 5.10 &amp; 3.50 &amp; 1.40 &amp; 0.20 \\\\ 4.90 &amp; 3.00 &amp; 1.40 &amp; 0.20 \\\\ 4.70 &amp; 3.20 &amp; 1.30 &amp; 0.20 \\\\ \\end{bmatrix} \\]   Aproveitando para Explicar data.frame e matrix no R! No R, os conjuntos de dados ou matrizes de dados são comumente representados por data frames (uma estrutura de dados computacional que muitas linguagens modernas também adotaram). Os data frames são estruturas de dados muito convenientes, pois suportam diferentes tipos de variáveis. Isso contrasta com as matrizes, que são restritas a um único tipo de dado. Ambas estruturas têm a capacidade de armazenar nomes de variáveis (colunas) e nomes de amostras (linhas) no R. A percepção de um data.frame é de uma estrutura muito mais semelhante com a de uma tabela convencional, como ilustrado a seguir: Table 2.1: Parte do dataset Iris Sepal.Length Sepal.Width Petal.Length Petal.Width 1 5.1 3.5 1.4 0.2 2 4.9 3.0 1.4 0.2 3 4.7 3.2 1.3 0.2 Para maior clareza, a seguir apresentamos a tabela implementada usando duas estruturas diferentes: data.frame e matrix. Visualmente, ambas as estruturas são indistinguíveis. &gt; X &lt;- head( iris[,-5], 3 ) &gt; X Sepal.Length Sepal.Width Petal.Length Petal.Width 1 5.1 3.5 1.4 0.2 2 4.9 3.0 1.4 0.2 3 4.7 3.2 1.3 0.2 &gt; class( X ) [1] &quot;data.frame&quot; &gt; Y &lt;- as.matrix( X ) &gt; Y Sepal.Length Sepal.Width Petal.Length Petal.Width 1 5.1 3.5 1.4 0.2 2 4.9 3.0 1.4 0.2 3 4.7 3.2 1.3 0.2 &gt; class( Y ) [1] &quot;matrix&quot; &quot;array&quot; &gt; Aqui, X é um objeto da classe data.frame e Y é um objeto da classe matrix. A seguir, fornecemos uma explicação para cada comando. O comando X &lt;- head( iris[,-5], 3 ) atribui a X(3,4) parte de iris (iris é 150x5). Digitar somente X, imprime seus valores na tela. O comando class(X) mostra que a classe do objeto X é data.frame. Y &lt;- as.matrix( X ) cria um objeto Y a partir de X e o força a ser da classe matrix Digitar somente Y, imprime seus valores na tela. O comando class(Y) foi somente para confirmar que Y é uma matriz.   Algumas propriedades da álgebra matricial são aplicáveis a ambas as classes. No entanto, quando há potencial para ambiguidades, é necessário realizar conversões explícitas entre as classes (como feito anteriormente). Além disso, é importante estar atento a conversões implícitas de tipos (ou classes), que podem ocorrer automaticamente. O exemplo a seguir ilustra esse comportamento: &gt; X &lt;- head( iris[,-5], 3 ) &gt; class(X) [1] &quot;data.frame&quot; &gt; Y &lt;- t( X ) &gt; class(Y) [1] &quot;matrix&quot; &quot;array&quot; A operação de transposição em X é permitida, mas o resultado é um objeto da classe matrix. Esteja atento a esses detalhes. O comando t(X) recebe uma matriz ou um data.frame X como argumento e retorna a transposta de X   Para mais alguns detalhes interessantes sobre Conjunto de dados, consulte a Wikipédia: Link: https://pt.wikipedia.org/wiki/Conjunto_de_dados Para saber mais sobre os dados de Iris: Link: https://pt.wikipedia.org/wiki/Conjunto_de_dados_flor_Iris No R, digite: ?iris 2.2 Valores faltantes: NA No R, NA, sigla de Not Available, é usado para representar valores faltantes em um conjunto de dados. A indicação é necessária para que o analista decida se haverá imputação de dados ou se haverá remoção das amostras que contenha parte dos dados perdidos. Aqui estão algumas maneiras comuns de lidar com NA no R: Identificação de NA Você pode identificar NA usando a função is.na(). x &lt;- c(1, 2, NA, 4, NA) is.na(x) # Verifica posições onde há algum NA # Alternativa any(is.na(x)) # Verifica se há algum NA Remoção de NA Para remover NA de um vetor ou dataframe, você pode usar a função na.omit() ou o argumento na.rm em funções como mean(), sum(), etc. Para vetores: x &lt;- c(1, 2, NA, 4, NA) x_clean &lt;- na.omit(x) Para dataframes: df &lt;- data.frame(a = c(1, 2, NA), b = c(NA, 3, 4)) df_clean &lt;- na.omit(df) Substituindo NA Substituir NA por um valor específico. x &lt;- c(1, 2, NA, 4, NA) x[is.na(x)] &lt;- 0 # Substituindo NA por 0 Análise Estatística com NA Algumas funções permitem que você especifique como lidar com NA usando o argumento na.rm. x &lt;- c(1, 2, NA, 4) mean(x, na.rm = TRUE) # Calcula a média ignorando NA Para detalhes adicionais sobre Dados Faltantes, consulte a Wikipédia: Link: https://pt.wikipedia.org/wiki/Não-resposta Imputação Imputação de dados é uma técnica utilizada na Estatística e em Ciência de Dados para lidar com dados faltantes ou ausentes em um conjunto de dados. Em vez de simplesmente excluir registros com dados faltantes, a imputação visa preencher esses valores ausentes com estimativas baseadas em informações disponíveis no conjunto de dados. O objetivo é melhorar a qualidade dos dados e permitir análises mais precisas e completas. Entre as principais técnicas estão: Imputação Simples: Substitui valores faltantes por uma constante, como a média ou mediana dos valores existentes. Imputação por Regressão: Usa um modelo de regressão para prever e preencher valores faltantes com base em variáveis preditoras. Imputação por Métodos de Machine Learning: Emprega algoritmos como K-Nearest Neighbors (KNN) ou Random Forests para prever valores faltantes com base em padrões encontrados nos dados. Imputação por Métodos Estatísticos: Inclui técnicas como múltiplas imputações, que criam vários conjuntos de dados imputados e combinam os resultados para uma análise mais robusta. Considerações Qualidade dos Dados: A escolha da técnica de imputação depende da natureza dos dados e do mecanismo de ausência. Técnicas mais complexas podem ser necessárias para conjuntos de dados grandes ou para variáveis que têm relações não lineares. Impacto na Análise: A imputação pode introduzir vieses ou variabilidade adicional nos dados, especialmente se as estimativas imputadas não forem precisas. A imputação de dados é uma técnica importante para preparar os dados para análise, e a escolha da técnica deve levar em conta a estrutura dos dados e o objetivo da análise. No entanto, neste caderno, não vamos abordar a imputação de dados.   Para detalhes adicionais sobre Imputação, consulte a Wikipédia: Link: https://en.wikipedia.org/wiki/Imputation_(statistics) 2.3 Dados Disponíveis Para aprender e praticar análise de dados, é essencial ter acesso a conjuntos de dados variados. No R, o pacote datasets oferece uma coleção de dados embutidos que vem com a instalação padrão do software. Esse pacote abrange uma ampla gama de conjuntos de dados, cobrindo diversas áreas como biologia, economia e ciências sociais. Com esses dados, os usuários podem praticar técnicas de análise, visualização e modelagem, facilitando o aprendizado prático e a aplicação de conceitos estatísticos. Para visualizar a lista completa de conjuntos de dados disponíveis, use o comando: help( package='datasets' ). Para carregar um conjunto de dados em sua sessão, utilize o comando data() seguido do nome do conjunto. Por exemplo: data( 'iris' ). Além disso, a maioria dos pacotes no R inclui seus próprios conjuntos de dados. Para acessá-los, basta usar o comando data( 'nome_do_dataset', package='nome_do_pacote' ) após a instalação do pacote.” Para facilitar o acesso aos leitores do Caderno de Quimiometria com R, selecionamos alguns conjuntos de dados específicos que podem ser especialmente úteis para iniciantes em análises quimiométricas. Apresentamos uma lista de conjuntos de dados recomendados. No pacote chemometrics: NIR: derivada de espectros NIR e concentração de glicose e etanol para 166 mostos de fermentação alcoólica de diferentes matérias-primas (centeio, trigo e milho). PAC: 209 objetos sendo um conjunto de dados X (467 variáveis = descritores moleculares) e um conjunto de dados y (1 variável = índices de retenção de GC de Compostos Aromáticos Policíclicos - PAC). Phenyl: espectros de massa de 600 compostos químicos, onde 300 contêm uma subestrutura fenil (grupo 1) e 300 compostos não contêm esta subestrutura (grupo 2). ash: Dados de 99 amostras de cinzas provenientes de diferentes biomassas, medidas em 9 variáveis (e mais 8 variáveis obtidas por transformação logarítmica). cereal: um conjunto de dados X e Y, medidos nos mesmos objetos (cereais). Os dados X são 145 espectros infravermelhos e os dados Y são 6 propriedades químicas (valor de aquecimento, conteúdo de C, H, N, Amido, Cinza). Os cereais vêm de 5 grupos B=Cevada, M=Milho, R=Centeio, T=Triticale, W=Trigo. glass e glass.grp: Uma matriz de dados com 180 objetos e 13 variáveis (13 medidas diferentes, em geral contéudo de óxidos, para 180 vasos arqueológicos de vidro de diferentes grupos). No pacote pls: gasoline: espectros NIR e números de octanas de 60 amostras de gasolina. mayonnaise: espectro NIR não processado (351 comprimentos de onda, 1100-2500 nm em passos de 4 nm) feitas em 54 amostras de maionese com base em seis tipos diferentes de óleo (soja, girassol, canola, azeitona, milho e semente de uva). As 54 amostras resultantes foram medidas em triplicado, resultando em 54 x 3 = 162 espectros diferentes (120/42 treinamento/teste). oliveoil: pontuações em 6 atributos de um painel sensorial e medições de 5 parâmetros de qualidade físico-químicos em 16 amostras de azeite. yarn: 21 espectros NIR de fios PET, medidos em 268 comprimentos de onda e 21 densidades correspondentes. No pacote kohonen: degelder: Padrões de raios X de 131 estruturas cristalográficas (em pó). Os dados estão em uma lista com três componentes: o primeiro componente, “patterns”, é uma matriz de 131 linhas e 441 variáveis, contendo os padrões de pó; o segundo componente é “thetas”, os valores 2theta nos quais as intensidades foram medidas. O componente final, “properties”, fornece informações sobre as propriedades cristalográficas das estruturas. nir: Um objeto de dados contendo 95 espectros NIR de misturas ternárias de etanol, água e isopropanol, medidos em cinco temperaturas diferentes. wines: Um dataframe contendo 177 linhas e treze colunas; o objeto vintages contém os rótulos da classe. Os dados são resultados de análises químicas de vinhos cultivados na mesma região da Itália (Piemonte), mas derivados de três cultivares diferentes: uvas Nebbiolo, Barberas e Grignolino. O vinho da uva Nebbiolo chama-se Barolo. Os dados contêm as quantidades dos vários constituintes encontrados em cada um dos três tipos de vinhos. Pacote doBy: NIRmilk: Medições de luz infravermelha próxima (NIR) feitas em 152 comprimentos de onda em 17 amostras de leite. Cada amostra de leite foi adicionalmente analisada quanto a gordura, lactose, proteína e matéria seca. codstom: Dados de conteúdo estomacal para bacalhau do Atlântico (Gadus morhua) no Golfo de St.Lawrence, leste do Canadá. São 10.000 observações em 10 variáveis. cropyield: Rendimento da produção agrícola dinamarquesa de cereais e tubérculos. Um dataframe com 97 linhas e 7 colunas. prostate: conjunto de dados de expressão genética de tumor de próstata. O conjunto de dados consiste em 52 amostras com tumores de próstata e 50 amostras normais. As classes normais e tumorais são codificadas em 0 e 1. milkman: Dados de produção de leite para vacas ordenhadas manualmente. São 161836 observações e 12 variáveis. Pacote cluster: animals: conjunto de dados coom 6 atributos binários para 20 animais. flower: conjunto de dados coom 8 atributos para 18 observações. plantTraits: Este conjunto de dados constitui uma descrição de 136 espécies de plantas de acordo com atributos biológicos (morfológicos ou reprodutivos - com 31 variáveis). Pacote carData: Pottery: Os dados, 26x6, fornecem a composição química da cerâmica antiga. Salaries: O salário acadêmico para professores em uma faculdade nos EUA. Os dados foram coletados para monitorar as diferenças salariais entre docentes do sexo masculino e feminino. São 397 observações em 6 variáveis. Soils: Composições de características físicas e químicas do solo. São 48 observações em 14 variáveis. Pacote ropls: cellulose: Conjunto de dados de exemplo NIR para ilustrar a calibração multivariada usando PLS. Viscosidade é a propriedade estudada de 180 amostras. cornell: Medição de octanagem de 12 misturas de 7 componentes de gasolina. foods: Consumo relativo de 20 itens alimentares compilado para 16 países. Os valores variam entre 0 e 100 por cento e um valor alto corresponde a um consumo elevado (O conjunto de dados contém 3 dados ausentes). Pacote boot: urine: 79 amostras de urina num esforço para determinar se certas características físicas (7) da urina poderiam estar relacionadas com a formação de cristais de oxalato de cálcio. cane: O dataframe, 45x7, representa um delineamento em blocos casualizados com variedades de cana-de-açúcar e 4 blocos. Pacote caret: oil: concentrações de ácidos graxos de óleos comerciais (“Palmitic”, “Stearic”, “Oleic”, “Linoleic”, “Linolenic”, “Eicosanoic”, “Eicosenoic”, medidas por cromatografia gasosa). Os dados são usados para prever o tipo de óleo: abóbora (A), girassol (B), amendoim (C), azeitona (D), soja (E), colza (F) e milho (G). tecator: dados registrados em um analisador de alimentos e rações Tecator Infratec trabalhando na faixa de comprimento de onda de 850 a 1050 nm pelo princípio de transmissão no infravermelho próximo (NIT). As amostras continham carne pura finamente picada com diferentes teores de umidade, gordura e proteína. Veja os detalhes de permissão caso queira utilizar esses dados! Pacote ISLR: Auto: 392 observações em 9 variáveis sobre automóveis. OJ: dados de 1070 amostras e 18 variáveis sobre compra de suco de laranja. Pacote kknn: glass: dados com 214 observações, onde o problema é prever o tipo de vidro em termos de seu teor de óxido (ou seja, Na, Fe, K, etc). O estudo da classificação dos tipos de vidros foi motivado pela investigação criminológica. São 214 observações sobre 11 variáveis. Pacote multigroup: wine: dados com 21 linhas (o número de vinhos) e 31 colunas (a primeira coluna corresponde ao rótulo de origem, a segunda coluna corresponde ao solo e as restantes correspondem aos descritores sensoriais). Pacote speaq: Winedata: dados de 1H-NMR de 40 vinhos, de diferentes origens. Pacote baseline: milk: Matriz de 45 espectros de valores de 21451 m/z de MALDI-TOF em amostras de leite misto. E vetor com a concentração de leite de vaca em amostras mistas de leite de vaca, cabra e ovelha. Pacote ade4: rhone: Este conjunto de dados fornece para 39 amostras de água uma descrição de 15 variáveis físico-química com o número de datas de amostragem e as vazões de três afluentes. Pacote mixexp: fishp: Dados, do Cornell, do famoso experimento sobre patê de peixes envolvendo variáveis de mistura e variáveis de processo. Executados sob um delineamento split-plot. O dataframe é 56 x 7. Pacote drc: G.aparine: Plantas (Galium aparine) foram pulverizadas com o herbicida. Após tempo de tratamento, foi medida a matéria seca. Os dados são de 240 amostras, doses, matéria seca e formulação do herbicida. Pacote GGally: flea: Estes dados, 74x7, contêm medições físicas em três espécies de besouros pulgas. Pacote hyperSpec: barbiturates: 286 Espectros de barbitúricos (arquivos de exemplo: BARBITUATES.SPC da especificação de formato de arquivo ‘*.spc’ da Thermo Galactic). Pacote ptw: gaschrom: 16 cromatogramas (GC) de calibração, medidos em 5.000 pontos no tempo. Uma versão com pico escolhido está disponível como objeto gaschrom.st. Pacote robustbase: alcohol: Características físico-químicas de 44 álcoois alifáticos. O objetivo do experimento foi a previsão da solubilidade com base em 7 descritores moleculares. Existem algumas outras fontes disponíveis que oferecem dados químicos confiáveis de forma gratuita, permitindo que pesquisadores e estudantes acessem informações essenciais sem custos, como por exemplo: DataSets - Eigenvector: Os conjuntos de dados a seguir estão disponíveis para download no Eigenvector Archive, em: https://eigenvector.com/resources/data-sets/ (Todos estão armazenados como arquivos MATLAB zipados). CGL_NIR, Predição de proteína de grãos por espectros NIR IASIM16 Challenge, imagens NIR de melamina em glúten de trigo IR Image of an Excedrin Tablet, útil para testar algoritmos de análise de imagem NIR spectra of corn samples, úteis para comparação de padronização e de pré-processamento Semiconductor metal etch, incluindo falhas conhecidas NIR spectra of diesel fuels, para testar seleção de variáveis e algoritmos de calibração Pharmaceutical tablets, o conjunto de dados “Shootout” consiste de espectros NIR de 654 comprimidos farmacêuticos de dois espectrômetros ArtImageDataA, imagens NIR de tintas a óleo Grupo de Quimiometria de Copenhague: há uma relação com mais de 50 conjuntos de dados livremente disponíveis em: https://ucphchemometrics.com/datasets/ . Nota contida no site: “Aqui estão alguns conjuntos de dados públicos que podem ser usados ​​para análise multivariada de dados. Observe que quando não há extensão de arquivo, basta adicionar ‘*.zip’ ao final do arquivo baixado.” Comentário 1: muitos desses dados estão em arquivos MATLAB; Comentário 2: Como há uma variedade de dados EEM, o pacote eemR possui algumas funções específicas para essa técnica; Comentário 3: Os pacotes pracma e matlab ajudam a migração de usuárisos do Matlab para o R pois há uma quantidade significativa de comandos matlab portados para o R; Comentário 4: Para leitura de arquivos MATLAB direto no R, use a função readMat() do pacote R.matlab, para arquivos ‘*.mat’ versões 4 e 5. Para arquivos ‘*.mat’ da versão 7.3 use os pacotes h5 e rhdf5. Periódico Data in Brief: Data in Brief é um periódico multidisciplinar, de acesso aberto e revisado por pares, que descrevem e fornecem acesso a dados de pesquisa. Data in Brief contribui para a ciência aberta e melhora a reprodutibilidade, tornando os dados e a pesquisa associada mais detectáveis, abrindo portas para a colaboração e reduzindo a duplicação de esforços. O Data in Brief apoia o FAIR Data Principles, que afirmam que os dados de pesquisa devem ser Findable, Accessible, Interoperable, and Reusable (Localizáveis, Acessíveis, Interoperáveis ​​e Reutilizáveis). Portanto, a missão do Data in Brief é tornar os dados de pesquisa acessíveis, detectáveis, citáveis, compreensíveis, revisados, reproduzíveis e reutilizáveis. Para saber mais, acesse: https://www.sciencedirect.com/journal/data-in-brief . Abaixo segue exemplos de 03 artigos dos quais fornecem os dados para download. Near-infrared spectra dataset of milk composition in transmittance mode, Jose A. Diaz-Olivares, Arnout van Nuenen, Martin J. Gote, Valeria Fonseca Díaz, Wouter Saeys, Ines Adriaens, Ben Aernouts, Data in Brief, Volume 51, 2023, https://doi.org/10.1016/j.dib.2023.109767. O conjunto de dados contém medições espectrais NIR no modo de transmitância na faixa de comprimento de onda de 960 nm a 1.690 nm de 1.224 amostras individuais de leite cru. Para estas medições espectrais são incluídos valores de referência laboratoriais correspondentes aos três principais componentes do leite cru (gordura, proteína e lactose), ureia e contagem de células somáticas (CCS). Os dados estão disponíveis para download Dataset of Near-infrared spectroscopy measurement for amylose determination using PLS algorithms P. Sampaio, A. Soares, A. Castanho, A.S. Almeida, J. Oliveira, C. Brites, Data in Brief, Volume 15, 2017, https://doi.org/10.1016/j.dib.2017.09.077. O artigo apresenta 168 amostras de arroz compreendendo 16 variedades de arroz e 11 variedades de arroz padrão. A concentração de amilose foi avaliada com base no método do iodo, e os espectros de infravermelho próximo (NIR) foram determinados. Para avaliar a vantagem da espectroscopia no infravermelho próximo, diferentes variedades de arroz e algoritmos específicos, como Standard Normal Variate (SNV), Multiple Scatter Calibration (MSC) e filtro Savitzky-Golay foram utilizados para pré-processamento de espectros NIR. Os dados estão disponíveis para download Fourier transform and near infrared dataset of dialdehyde celluloses used to determine the degree of oxidation with chemometric analysis Jonas Simon, Otgontuul Tsetsgee, Nohman Arshad Iqbal, Janak Sapkota, Matti Ristolainen, Thomas Rosenau, Antje Potthast, Data in Brief, Volume 40, 2022, https://doi.org/10.1016/j.dib.2021.107757. Este conjunto de dados está relacionado ao artigo de pesquisa intitulado: A fast method to measure the degree of oxidation of dialdehyde celluloses using multivariate calibration and infrared spectroscopy. No artigo, 74 amostras de dialdeído celulose com diferentes graus de oxidação foram preparadas por oxidação de periodato e analisadas por espectroscopia de infravermelho com transformada de Fourier (FTIR) e infravermelho próximo (NIR). Os correspondentes graus de oxidação foram determinados indiretamente pelo consumo de periodato utilizando espectroscopia UV a 222 nm e pela reação quantitativa com cloridrato de hidroxilamina seguida de titulação potenciométrica. A regressão de mínimos quadrados parciais (PLSR) foi utilizada para correlacionar os dados infravermelhos com o correspondente grau de oxidação (DO). Os modelos NIR/PLSR e FTIR/PLSR desenvolvidos podem ser facilmente implementados em outros laboratórios para prever de forma rápida e confiável o grau de oxidação das celuloses de dialdeído. Dados disponíveis na publicação prévia: A fast method to measure the degree of oxidation of dialdehyde celluloses using multivariate calibration and infrared spectroscopy, Jonas Simon, Otgontuul Tsetsgee, Nohman Arshad Iqbal, Janak Sapkota, Matti Ristolainen, Thomas Rosenau, Antje Potthast, Carbohydrate Polymers, Volume 278, 2022, https://doi.org/10.1016/j.carbpol.2021.118887. Os dados estão disponíveis para download Se você conhecer fontes confiáveis e gratuitas de dados químicos, por favor, envie um e-mail para cleber.borges@ufla.br . Sua contribuição será fundamental para enriquecer este caderno! "],["visualização-de-dados.html", "&#8474;&#8477; 3 Visualização de Dados 3.1 Gráficos com o R", " &#8474;&#8477; 3 Visualização de Dados Muitas vezes, os detalhes da informação que estamos tentando compreender, que podem estar ocultos nos dados, não são facilmente capturados apenas por análises e processos numéricos. Visualizar dados em gráficos é crucial por várias razões: Facilita a Interpretação: Gráficos tornam os dados mais compreensíveis ao transformar números em tabelas em representações visuais que, em geral, são mais fáceis de interpretar. Pois, isso ajuda a identificar padrões (tendências), anomalias (outliers) e relação entre variáveis de forma mais intuitiva. É mais fácil entender e analisar informações visuais do que interpretar grandes conjuntos de números. Facilita a Análise Exploratória: Durante a análise exploratória dos dados, gráficos ajudam a entender melhor o conjunto de dados e a formular hipóteses para testes estatísticos subsequentes. Ajuda na Comparação: Permite a comparação direta entre diferentes conjuntos de dados ou categorias. Comunicação: Facilita a comunicação quando você precisar apresentar seus resultados para outras pessoas. Em termos gerais, os gráficos são úteis para tornar os dados mais compreensíveis, detectar padrões e tendências. Quarteto de Anscomble O Quarteto de Anscomble20 é um conjunto de dados artificial criado justamente com a intenção de, forma gráfica, transmitir ao menos algumas das justificativas anteriores para obter uma compreensão completa da informação neles contida. Esse conjunto de dados foi projetado de maneira a ter algumas estatísticas descritivas númericas idênticas (média, variância, correlação, reta) mais com diferentes padrões óbvios quando visualizados graficamente. Fica evidente que dados podem ter estatísticas numéricas similares mas podem ter estruturas muito diversas. A tabela a seguir mostram esses dados.   Table 3.1: Quarteto de Anscombe   conjunto I: x1 e y1; conjunto II: x2 e y2; conjunto III: x3 e y3; conjunto IV: x4 e y4   Abaixo seguem os quatro gráficos do conjunto de dados de Anscombe. Figure 3.1: Quarteto de Anscombe A saber, as estatísiticas descritivas são apresentados a seguir no trecho de código executado: # Calcula as médias dos x&#39;s e dos y&#39;s &gt; round( apply( anscombe, 2, mean ), 2 ) x1 x2 x3 x4 y1 y2 y3 y4 9.0 9.0 9.0 9.0 7.5 7.5 7.5 7.5 &gt; # Calcula as variâncias dos x&#39;s e dos y&#39;s &gt; round( apply( anscombe, 2, var ), 2 ) x1 x2 x3 x4 y1 y2 y3 y4 11.00 11.00 11.00 11.00 4.13 4.13 4.12 4.12 &gt; &gt; attach(anscombe) # Calcula as correlações entre os x&#39;s e y&#39;s &gt; round( c( cor(x1,y1), cor(x2,y2), cor(x3,y3), cor(x4,y4) ), 2 ) [1] 0.82 0.82 0.82 0.82 &gt; # Calcula as regressões lineares entre os x&#39;s e y&#39;s &gt; reg1 &lt;- lm( y1 ~ x1 ); reg2 &lt;- lm( y2 ~ x2 ) &gt; reg3 &lt;- lm( y3 ~ x3 ); reg4 &lt;- lm( y4 ~ x4 ) &gt; # Mostra os coeficientes angulares e lineares, entre os x&#39;s e y&#39;s &gt; round( rbind( coef( reg1 ), coef( reg2 ), coef( reg3 ), coef( reg4 ) ), 2 ) (Intercept) x1 [1,] 3 0.5 [2,] 3 0.5 [3,] 3 0.5 [4,] 3 0.5 &gt; # Mostra os coeficientes de determinação, entre os x&#39;s e y&#39;s &gt; round( + c( summary( reg1 )$r.sq, summary( reg2 )$r.sq, + summary( reg3 )$r.sq, summary( reg4 )$r.sq + ), 2 ) [1] 0.67 0.67 0.67 0.67 &gt; &gt; detach( anscombe ) &gt; Do resultado observamos que: os valores das médias das variáveis x’s são iguais a 9. os valores das médias das variáveis y’s são iguais a 7,5. os valores das variâncias das variáveis x’s são iguais a 11. os valores das variâncias das variáveis y’s são iguais a 4,1. os valores das correlações entre as variáveis x’s e y’s são iguais a 0,82. os valores dos coeficientes dos parâmetros lineares e angulares são, respectivamente, iguais a 3 e 0,5. Considere os gráficos anteriores e tente argumentar contra sua importância! Será um um desafio significativo.   Para saber mais, consulte a Wikipédia: Link: https://pt.wikipedia.org/wiki/Quarteto_de_Anscombe No R, digite: ?anscombe   Exercício Para entender passo a passo o último trecho de código apresentado, no ChatGPT, digite: “Explique o seguinte código:” 8&lt;recorte e cole aqui o trecho de código executado acima&gt;8  Resposta Demonstração pelo ChatGPT   Construindo bons Gráficos… Um bom gráfico deve comunicar informações de maneira clara e direta. Para alcançar esse objetivo, é fundamental que ele possua certas características essenciais. A seguir, apresentamos as principais características que contribuem para a eficácia de um gráfico. Clareza: As informações devem ser apresentadas de maneira que sejam facilmente compreendidas pelo público. Evite a sobrecarga de elementos no gráfico, como excessos de dados, linhas ou legendas, pois isso pode confundir o espectador. Um design limpo e focado ajuda a transmitir a mensagem de forma mais eficaz. Título Descritivo: Um título deve ser claro e conciso, refletindo precisamente o conteúdo e o propósito do gráfico. Ele deve fornecer uma visão imediata sobre o que está sendo apresentado, ajudando o espectador a entender rapidamente o tema abordado e o contexto dos dados. Eixos Bem Definidos: Os eixos do gráfico devem ter rótulos claros e informativos, que identifiquem claramente o que cada um representa. É essencial incluir as unidades de medida quando aplicável, pois isso ajuda o espectador a interpretar os dados com precisão e a entender a escala e o contexto das informações apresentadas. Legibilidade: Escolha fontes que sejam claras e legíveis, utilizando tamanhos apropriados para garantir que todos os elementos do gráfico sejam facilmente compreensíveis. A legibilidade é fundamental para que o público possa absorver as informações rapidamente, sem esforço adicional para decifrar texto ou números. Esteja ciente de que um gráfico bonito é tão ruim quanto um gráfico feio pois elementos de estética não devem nunca se sobreporem aos elementos informativos! Então evite qualquer fonte decorativa que possa prejudicar a clareza. Cores e Estilos: Utilize cores que contrastem bem entre si para facilitar a distinção entre diferentes elementos do gráfico. Quando o gráfico precisar ser impresso em cores e em escala de cinza, escolha uma paleta que mantenha a clareza das informações em ambas as versões. Isso garante que os dados sejam legíveis e facilmente interpretáveis, independentemente do formato de impressão. Existem paletas de cores projetadas especificamente para minimizar a perda de informação ao converter o gráfico do espaço de cores para a escala de cinza. Essas paletas são cuidadosamente elaboradas para garantir que as diferenças e nuances importantes entre os dados sejam preservadas, mesmo quando visualizadas em preto e branco. Dessa forma, a legibilidade e a clareza do gráfico são mantidas, independentemente do formato de apresentação (Pesquise sobre a paleta de cores Viridis21 para saber mais).  Paletas e Escala de Cinza Paletas tim.colors (pacote fields) e viridis Suas respectivas projeções para escala de cinza Escala Apropriada: A escala do gráfico deve ser adequada ao tipo de dados apresentados, permitindo uma interpretação precisa e facilitando a comparação entre diferentes elementos. Escolher uma escala que represente adequadamente a variação dos dados ajuda a evitar distorções e a garantir que as diferenças sejam facilmente visíveis. Isso contribui para uma análise mais eficaz e intuitiva das informações. Dados Precisos: Garanta que os dados representados no gráfico sejam corretos, confiáveis e atualizados. A precisão das informações é fundamental para a credibilidade do gráfico e para a tomada de decisões informadas. Verificar as fontes e a integridade dos dados antes de apresentá-los ajuda a evitar mal-entendidos e a transmitir uma mensagem clara e consistente. Anotações e Destaques: Utilize anotações para realçar pontos-chave e informações relevantes no gráfico, facilitando a compreensão do espectador. No entanto, é importante evitar excessos; muitas anotações podem criar confusão e desviar a atenção do principal. O equilíbrio é essencial: destaque o que é realmente significativo para que a mensagem central permaneça clara e impactante. Interatividade (se aplicável): Em gráficos digitais, a inclusão de elementos interativos pode enriquecer a experiência do usuário, permitindo uma exploração mais profunda dos dados. Funcionalidades como zoom, filtragem e visualizações dinâmicas permitem que o público interaja com as informações, descubra padrões e personalize a visualização de acordo com suas necessidades. Isso não só torna a análise mais envolvente, mas também ajuda na compreensão de insights complexos. Essas características são essenciais para garantir que o público compreenda rapidamente a mensagem que o gráfico busca transmitir. Ao priorizar clareza, precisão e design eficaz, facilitamos a interpretação dos dados e potencializamos o impacto da informação, tornando-a mais acessível e significativa para todos. 3.1 Gráficos com o R R oferece um rico ecossistema de funções e pacotes para a criação de vários tipos de gráficos que permite desde visualizações simples até representações sofisticadas para necessidades específicas. Desde o ínicio, nas próprias palavras do R Core Team, a definição para o R foi22: “R é uma linguagem e ambiente para computação estatística e gráficos”. E o cuidado com o sistema gráfico foi planejado desde a origem do ambiente: “Um dos pontos fortes do R é a facilidade com que gráficos bem projetados com qualidade de publicação podem ser produzidos, incluindo símbolos matemáticos e fórmulas quando necessário. Foi tomado muito cuidado com os padrões para as pequenas opções de design em gráficos, mas o usuário mantém o controle total”. A adoção e desenvenvolvimento posterior foi consequência direta dessa preocupação em se ter um software livre com tais capacidades também em quesitos de produção de boas imagens de gráficos estatísticos. O site The R Graph Gallery23 possui uma coleção extensa de gráficos gerados por R na web. São centenas de gráficos são exibidos em diversas seções, sempre com seu código reproduzível disponível. H Há também R Gallery Book24, um guia completo baseado no site da R Graph Gallery. Ele extrai informações diretamente da galeria, oferecendo detalhes minuciosos na reprodução e na elaboração de ideias. 3.1.1 Funções nativas As funções nativas servem para criar gráficos diversos, incluindo gráficos de dispersão, linhas, barras, boxplots e histogramas, entre outros. Essas funções nativas são de propósito geral e bastante versáteis. (Existem muitas outras funções específicas disponíveis para atender a necessidades mais especializadas e que são fornecidas por pacotes). Algumas dessas funções gerais e suas respectivas finalidades estão listadas a seguir: plot(): usada para criar gráficos de dispersão básicos25, permitindo visualizar a relação entre duas variáveis.  Demonstração pelo ChatGPT Resultados Exemplo 1: Gráfico de Dispersão (Scatter Plot) Exemplo 2: Gráfico de Linhas (Line Plot) Exemplo 3: Gráfico de Barras (Bar Plot) Exemplo 4: Gráfico de Função Matemática (Mathematical Function Plot) Exemplo 5: Gráfico de Densidade (Density Plot) Exemplo 6: Gráfico com Anotações (Para o exemplo 6, foi necessário alterar as coordenadas) text(3, 0, “Ponto Médio”, col = “red”, cex = 1.2) arrows(1, -0.8, 3, 0, col = “gray”, length = 0.1, lwd=2) matplot(): é utilizada para plotar várias séries26 de dados em um único gráfico, facilitando a comparação entre múltiplas variáveis ou conjuntos de dados.  Demonstração pelo ChatGPT Resultados Exemplo 1: Gráfico Simples de Múltiplas Linhas Exemplo 2: Gráfico de Funções Trigonométricas Exemplo 3: Gráfico de Múltiplas Séries de Dados Aleatórios Exemplo 4: Gráfico com Diferentes Escalas de Eixo Y Exemplo 5: Gráfico de Dispersão de Multiplas Variáveis barplot(): é utilizada para criar gráficos de barras27, utiliza barras retangulares para representar dados. O comprimento de cada barra é proporcional ao valor que ela representa, e as barras podem ser orientadas vertical ou horizontalmente.  Demonstração pelo ChatGPT Resultados Exemplo 1: Exemplo Básico de barplot Exemplo 2: Gráfico de Barras com Várias Categorias Exemplo 3: Gráfico de Barras Empilhadas Exemplo 4: Gráfico de Barras com Barras Horizontais Exemplo 5: Gráfico de Barras com Personalização de Eixos e Rótulos Exemplo 6: Gráfico de Barras com Erros (Bar Error) Exemplo 7: Gráfico de Barras com Anotação de Texto (Para o exemplo 7, foi necessário acrescentar a opção:) ylim=c(0,23) pie(): é usada para criar gráfico de setores ou gráfico circular, tradicionalmente chamado gráficos de pizza28, que mostram a proporção de cada categoria em relação ao total, facilitando a visualização das partes que compõem um todo. Recomenda-se não usar esse tipo de gráfico!  Demonstração pelo ChatGPT Resultados Exemplo 1: Exemplo Simples de Gráfico de Pizza Exemplo 2: Gráfico de Pizza com Percentuais Exemplo 3: Gráfico de Pizza Colorido Exemplo 4: Gráfico de Pizza com Explosão de Fatias (Para o exemplo 4, foi necessário retirar a opção:) ‘explode’, pois não existe a opção na função pie nativa Exemplo 5: Gráfico de Pizza com Legenda Personalizada Exemplo 6: Gráfico de Pizza com Rótulos Personalizados Exemplo 7: Gráfico de Pizza 3D (Usando Pacote plotrix) (Para o exemplo 7, foi necessário acrescentar a opção:) title(sub=“Que horror! Nunca faça um monstro desses…”) boxplot(): é utilizada para criar gráficos de caixa29, que visualizam a distribuição de dados através de quartis, mediana e possíveis outliers, ajudando a identificar a dispersão e a simetria dos dados.  Demonstração pelo ChatGPT Resultados Exemplo 1: Boxplot Simples de Um Conjunto de Dados Exemplo 2: Comparação de Múltiplos Grupos Exemplo 3: Boxplot com Outliers e Linha de Referência Exemplo 4: Boxplot Horizontal Exemplo 5: Boxplot por Fatores em um DataFrame Exemplo 6: Boxplot com Notch hist(): é usada para criar histogramas30, que mostram a distribuição de uma variável contínua, dividindo os dados em intervalos (bins) e exibindo a frequência ou a densidade de cada intervalo.  Demonstração pelo ChatGPT Resultados Exemplo 1: Histograma simples com dados gerados aleatoriamente Exemplo 2: Ajustando o número de intervalos (breaks) Exemplo 3: Comparando distribuições (dois histogramas sobrepostos) (Para o exemplo 3, foi necessário acrescentar a opção:) ylim=c(0,150) Exemplo 4: Histograma com densidade (suavização) Exemplo 5: Histograma com cores graduadas e sem bordas Exemplo 6: Customizando eixos e rótulos Exemplo 7: Histograma de dados com assimetria (distribuição exponencial) Exemplo 8: Adicionando média e mediana ao histograma image(): cria gráficos de matriz ou mapas de calor31, representando a estrutura de dados em uma matriz através de cores, permitindo a visualização de padrões e intensidades em dados bidimensionais.  Demonstração pelo ChatGPT Resultados Exemplo 1: Visualização de uma matriz simples Exemplo 2: Visualização de Funções Matemáticas (Função Seno) Exemplo 3: Mapa de Calor de Dados Aleatórios Exemplo 4: Visualizando uma Distribuição Normal Bivariada Exemplo 5: Customizando o Eixo e a Escala de Cores Exemplo 6: Visualizando uma Imagem de Rastreio (Contornos) contour(): ou curvas de nível32, é utilizada para criar gráficos de contorno, que representam linhas de igual valor em um conjunto de dados bidimensionais, ajudando a visualizar a forma e a distribuição de variáveis contínuas em um espaço.  Demonstração pelo ChatGPT Resultados Exemplo 1: Contorno simples de uma função matemática Exemplo 2: Adicionando mais níveis de contorno Exemplo 3: Contorno de uma superfície quadrática Exemplo 4: Contorno com cores customizadas Exemplo 5: Adicionando uma imagem de fundo (grayscale) Exemplo 6: Contornos de uma função bidimensional com múltiplas funções Exemplo 7: Contorno com títulos e rótulos customizados mosaicplot(): usada para criar gráficos de mosaico33, que visualizam a associação entre duas ou mais variáveis categóricas, mostrando a proporção de cada combinação de categorias através de áreas proporcionais em um gráfico.  Demonstração pelo ChatGPT Resultados Exemplo 1: Gráfico de Mosaico Simples Exemplo 2: Gráfico de Mosaico com Dados de Frequência Exemplo 3: Gráfico de Mosaico com Bordas e Cores Personalizadas Exemplo 4: Gráfico de Mosaico para Mais de Duas Variáveis Exemplo 5: Gráfico de Mosaico com Formato Horizontal e Vertical (Para o exemplo 5, não existe a opção direction:) o R ignora e gera uma mensagem de atenção. Exemplo 6: Gráfico de Mosaico para Dados Faltantes dotchart(): usada para criar gráficos de pontos34, que visualizam a distribuição de dados em uma única variável categórica, com pontos dispostos ao longo de um eixo, facilitando a comparação de magnitudes entre categorias.  Demonstração pelo ChatGPT Resultados Exemplo 1: Gráfico de Pontos Simples Exemplo 2: Gráfico de Pontos com Nomes de Categorias Exemplo 3: Gráfico de Pontos com Agrupamento de Dados (Para o exemplo 3, foi feita a alteração:) groups = as.factor(grupos). Exemplo 4: Gráfico de Pontos com Valores Ordenados Exemplo 5: Gráfico de Pontos com Múltiplos Grupos e Cores Personalizadas (Para o exemplo 5, foi feita a alteração:) groups = as.factor(grupos). stripchart(): é usada para criar gráficos de dispersão unidimensionais35, representando a distribuição de dados de uma variável contínua ao longo de um eixo, com pontos posicionados horizontalmente ou verticalmente, facilitando a visualização da dispersão e concentração dos dados. Esse tipo de gráfico é uma boa alternativa aos boxplots quando os tamanhos de amostra são pequenos. É similar ao dotchart.  Demonstração pelo ChatGPT Resultados Exemplo 1: Stripchart básico com um conjunto de dados Exemplo 2: Comparação entre grupos (vetores múltiplos) Exemplo 3: Stripchart com sobreposição de boxplot (Para o exemplo 3, foi feita a alteração:) groups = as.factor(grupos). Exemplo 4: Stripchart de dados categorizados Exemplo 5: Stripchart com distribuição empilhada Exemplo 6: Stripchart de vários grupos em linhas diferentes persp(): é utilizada para criar gráficos em perspectiva 3D, que visualizam superfícies tridimensionais a partir de dados de uma matriz, ajudando a representar e explorar a estrutura espacial dos dados.  Demonstração pelo ChatGPT Resultados Exemplo 1: Superfície de uma Função Quadrática Simples Exemplo 2: Função Seno-Cosseno (Ondas) Exemplo 3: Gráfico de uma Função de Pico Gaussiano Exemplo 4: Superfície de Função Exponencial com Parâmetros Modificados Exemplo 5: Função de Ondas Cosenoidal Multiplicada por um Exponencial Exemplo 6: Função de Picos Aleatórios (Uso de Ruído) Uma curiosidade é que o R não possui uma função nativa específica para criar gráficos de dispersão em 3D. Embora seja possível realizar isso com criatividade usando as funções persp e trans3d, gráficos de dispersão em 3D são frequentemente desaconselhados devido ao fato de não transmitir informações claras e úteis ao usuário (baixo poder informativo). Como há pessoas que insistem no uso nesse tipo de gráfico, e visando à completude, o pacote scatterplot3d será utilizado para complementar a lista de tipos de gráficos.  Demonstração pelo ChatGPT Resultados Exemplo 1: Gráfico Básico 3D Exemplo 2: Gráfico com Diferentes Cores Exemplo 3: Gráfico com Tamanho dos Pontos Variável (Para o exemplo 3, foi feita a alteração:) de cex = sizes para cex.symbols = sizes*0.15. Exemplo 4: Gráfico com Superfície de Regressão (Para o exemplo 4, foi feita a alteração:) draw_lines = TRUE Exemplo 5: Gráfico com Anotações (Para o exemplo 4, foi feita a alteração:) text( s3d$xyz.convert( cbind(x,y,z) ), … Para aprender, consulte o help() de cada função para saber sobre o uso típico. Leia atentamente todos os detalhes e execute os exemplos fornecidos (hiperlink ‘Run examples’ das páginas de ajuda das funções). Com tempo e prática, você se familiarizará gradualmente com cada uma. Essa listagem está aqui para futuras consultas. O sistema gráfico nativo é ideal para gráficos rápidos e quando se busca simplicidade preservando qualidade editorial (ou seja, quando é necessário ter alto nível de clareza, precisão, legibilidade, consistência e profissionalismo para que o gráfico seja apresentado para fins de comunicação e publicação. Você pode salvar esses gráficos em diversos formatos raster (bitmap) e também em alguns formatos vetoriais, dependendo das suas necessidades de qualidade (Veja: ?png e ?pdf). 3.1.2 Pacote animation Com o uso do pacote animation é possível ir além dos gráficos estáticos, permitindo criar animações, proporcionando um modo dinâmico de visualizar os dados quando pertinente. É particularmente útil quando você deseja visualizar mudanças ou interações complexas ao longo do tempo ou ângulos, como simulações, evoluções temporais ou rotações de gráficos 3D. O principal propósito do pacote animation é facilitar a criação de GIFs animados, vídeos e apresentações interativas diretamente a partir de gráficos gerados no R. Ele simplifica a criação de animações de maneira iterativa e automática, combinando vários gráficos em uma sequência que pode ser visualizada como um filme. Funcionalidades e Vantagens: Criação de GIFs e Vídeos: O animation permite salvar animações em diferentes formatos, como GIF, vídeo (MP4, AVI) e até slideshows em HTML. É multi-formato: Suporta vários formatos de saída, como GIF, MP4, HTML, e AVI. Funções como saveGIF(), saveVideo(), e saveHTML() facilitam o processo de salvar as animações em diversos formatos de mídia. Simulações Dinâmicas: É ideal para criar animações de simulações estatísticas, como métodos de Monte Carlo, simulações de cadeia de Markov, evolução de dados ao longo do tempo ou visualizações rotativas de gráficos 3D. Controle Total da Animação: O usuário pode controlar a velocidade, número de quadros e até personalizar os intervalos de tempo entre os quadros, proporcionando flexibilidade para criar animações simples ou avançadas. Exemplos: ani.options(interval = 0.1) define o intervalo de tempo entre os quadros da animação. Facilidade de Uso: A estrutura é intuitiva, mesmo para iniciantes. Para criar uma animação, basta colocar o código de criação dos gráficos dentro de um loop, e a função saveGIF() ou outra função correspondente cuidará do resto. É simples e rápido pois mesmo com pouco código é possível criar animações sofisticadas. Integração com Gráficos 3D e Superfícies: Funciona muito bem com gráficos tridimensionais, permitindo criar animações que rotacionam gráficos em torno de seus eixos ou mostram como eles evoluem em várias dimensões. Interatividade e Visualização de Modelos: A animação pode ajudar a entender visualmente conceitos complexos, como a convergência de estimativas em modelos estatísticos, a simulação de distribuições probabilísticas, ou a visualização de resíduos em modelos de regressão.  i Demonstração pelo ChatGPT Resultados Exemplo 1: Animação Básica com Função saveGIF() Exemplo 2: Animação com Função saveVideo() MP4 video MP4 video Exemplo 3: Animação Interativa em HTML com saveHTML() Exemplo 4: Animação com ani.options() para Controlar Parâmetros  ii Demonstração pelo ChatGPT Animações dos Gráficos anteriormente obtidos com a função persp() Resultados Exemplo 1: Exemplo 2: Exemplo 3: Exemplo 4: Exemplo 5: Exemplo 6:  iii Demonstração pelo ChatGPT Animações dos Gráficos anteriormente obtidos com a função scatterplot3d() Resultados Exemplo 1: Exemplo 2: Exemplo 3: Exemplo 4: Exemplo 5: 3.1.3 Pacote ggplot2 O ggplot236, 37 é um dos pacotes mais renomados para visualização de dados em R, desenvolvido e mantido por Hadley Wickham. Baseado na Gramática dos Gráficos (Grammar of Graphics38), ele oferece uma abordagem estruturada e intuitiva para criar representações visuais sofisticadas e informativas. Ao combinar diferentes camadas, como pontos, linhas e barras, o ggplot2 proporciona grande flexibilidade na personalização dos gráficos, permitindo criar visualizações esteticamente agradáveis e adaptadas às suas necessidades. O pacote ggplot2, por ter sua estrutura baseada em camadas, é altamente extensível. Essas extensões são desenvolvidas por uma comunidade ativa de usuários e desenvolvedores e são distribuídas como pacotes adicionais no ecossistema R. Elas podem adicionar novas funcionalidades, melhorar a estética dos gráficos ou facilitar o uso de ggplot2 em contextos específicos. O site: https://exts.ggplot2.tidyverse.org/ rastreia e lista extensões do pacote ggplot2 desenvolvidas por usuários R na comunidade. O objetivo do site é facilitar aos usuários do R a localização dessas extensões desenvolvidas. A galeria39 de demonstração possui mais de 130 extensões, todas acompanhadas de exemplos para ajudar na sua usabilidade.   Demonstração pelo ChatGPT Resultados Exemplo 1: Gráfico de Dispersão (Scatter Plot) Exemplo 2: Gráfico de Linhas (Line Plot) Exemplo 3: Histograma Exemplo 4: Boxplot Exemplo 5: Gráfico de Barras Empilhadas (Stacked Bar Chart) Exemplo 6: Gráfico de Barras Agrupadas (Grouped Bar Chart) Exemplo 7: Gráfico de Área Exemplo 8: Gráfico de Densidade (Density Plot) Exemplo 9: Gráfico de Linhas com Múltiplas Séries Gramática dos Gráficos A Gramática dos Gráficos é um framework teórico para a construção de gráficos por meio de camadas. Aqui está uma visão geral dessas camadas: Conceitos Fundamentais Componentes Básicos: Dados: O conjunto de dados a ser visualizado. Estéticas (Aesthetics): Mapeamento de variáveis para propriedades visuais, como a posição, cor, tamanho e forma dos elementos do gráfico. Geometria (Geoms): Define os tipos de gráficos e formas visuais representadas que você quer criar, como pontos (geom_point()), linhas (geom_line()), barras (geom_bar()), etc. Facetas (Facets): Divisão dos dados em subconjuntos para criar múltiplos gráficos, dividindo os dados com base em uma ou mais variáveis para facilitar a comparação entre grupos (usando facet_wrap() ou facet_grid()). Estatísticas (Stats): Transformações e cálculos realizados sobre os dados antes de serem visualizados, como a adição de linhas de tendência (usando stat_smooth()). Coordenação (Coords): Sistema de coordenadas usado para posicionar os elementos no gráfico, como coordenadas cartesianas (coord_cartesian()) ou polares (coord_polar()). Temas (Themes): Personalização da aparência estética do gráfico, incluindo a formatação de texto, linhas e elementos de fundo. Camadas (Layers): Em vez de criar um gráfico de uma vez, a Gramática dos Gráficos permite construir gráficos adicionando camadas. Cada camada representa uma parte do gráfico e pode ser ajustada e combinada de maneira modular. Gráficos Combináveis: Graças à abordagem modular, você pode combinar diferentes tipos de gráficos e camadas em uma única visualização. Por exemplo, adicionar uma linha de regressão a um gráfico de dispersão ou combinar gráficos de barras com gráficos de linha. Sistema de Mapeamento: A Gramática dos Gráficos usa mapeamentos estéticos para associar variáveis dos dados a propriedades visuais dos elementos do gráfico. Isso facilita a criação de visualizações que são tanto informativas quanto visualmente impactantes. Benefícios da Gramática dos Gráficos Flexibilidade: Permite criar visualizações complexas combinando diferentes elementos. Modularidade: Facilita a construção e modificação de gráficos por meio da adição ou remoção de camadas. Consistência: Oferece uma abordagem consistente para a criação de gráficos, tornando mais fácil entender e interpretar a visualização. O objetivo da Gramática dos Gráficos é facilitar a criação de representações visuais de maneira intuitiva e estruturada, promovendo uma análise mais direta e uma comunicação mais eficaz dos resultados. 3.1.4 Pacote plotly O pacote plotly cria gráficos interativos e dinâmicos. Ele é baseado na biblioteca JavaScript Plotly.js, que é amplamente utilizada para visualizações de dados na WEB. O objetivo do plotly é facilitar a criação de gráficos interativos diretamente a partir do R, sem a necessidade de programar em JavaScript. Ele é muito popular em aplicações de ciência de dados e dashboards. O plotly é construído sobre a tecnologia D3.js (Data-Driven Documents) e o WebGL, que permitem a renderização de gráficos diretamente em navegadores com alto desempenho e interação. Ele é altamente compatível com outras ferramentas de visualização no R, como o ggplot2, o que permite transformar gráficos estáticos em gráficos interativos com facilidade. É altamente versátil e pode ser utilizado tanto para visualizações simples como para gráficos complexos e tridimensionais, tudo com um alto nível de interatividade e personalização. Principais Recursos do plotly Interatividade: Gráficos podem ser manipulados pelos usuários, incluindo zoom, rotação (para gráficos 3D), seleção de dados, e adição de anotações. Suporte a gráficos 3D: Permite a criação de gráficos tridimensionais como superfícies, dispersões e malhas (meshes) que podem ser rotacionados e explorados. Facilidade de uso: Simples de integrar com gráficos criados pelo ggplot2 e também com gráficos nativos do R, como os criados pela função plot_ly. Gráficos interativos e responsivos: Os gráficos se ajustam automaticamente ao tamanho da tela, o que os torna ideais para aplicações WEB e dashboards. Exportação: Permite exportar gráficos em formatos estáticos como PNG, SVG, PDF, além de integrá-los diretamente em documentos HTML interativos. Suporte a vários tipos de gráficos: Gráficos de dispersão (scatter plots) Gráficos de linhas Gráficos de barras Histogramas Superfícies 3D Gráficos de calor (heatmaps) Gráficos de violino e boxplots Gráficos de radar, etc. Visite o site: https://plot.ly/r/ para consultar o ‘Guia de Referência’ e uma variedade de exemplos ilustrando os recursos do pacote. O livro: ‘Interactive web-based data visualization with R, plotly, and shiny’ está disponível gratuitamente em https://plotly-r.com/ . A obra é dedicada à criação de visualizações interativas baseadas na WEB utilizando o R, focando principalmente no pacote plotly e na integração com o Shiny. Exemplo típico do plotly Gráfico simples feito com plotly para fins de ilustração: Figure 3.2: Exemplo típico do plotly A toolbar (barra de ferramentas) nos gráficos criados com plotly oferece uma série de funcionalidades interativas que melhoram a experiência do usuário ao explorar os dados de maneira dinâmica. Esses recursos tornam a visualização não apenas mais atraente, mas também mais funcional, facilitando a análise de dados complexos de forma acessível. O usuário também pode personalizar a toolbar escolhendo quais ferramentas e recursos devem ser exibidos, utilizando o parâmetro config() na função plot_ly() (por exemplo, esconder a toolbar ou desativar certas funções). Aqui estão os principais recursos e botões: Zoom: O botão de zoom permite que o usuário selecione uma área do gráfico para ampliar. Após clicar no botão, você pode clicar e arrastar para criar um retângulo em torno da área que deseja ampliar. Pan: O botão de pan permite que o usuário mova o gráfico. Isso é útil após usar o zoom, permitindo que você explore diferentes partes do gráfico sem ter que redefinir o zoom. Reset Axes: Este botão redefine o gráfico para suas escalas e limites originais, permitindo que você retorne rapidamente à visualização padrão. Download: O botão de download permite exportar o gráfico em diferentes formatos, como PNG, JPEG, PDF ou SVG. Essa funcionalidade é especialmente útil para incluir gráficos em relatórios ou apresentações. Select: O recurso de seleção permite que o usuário selecione pontos ou uma área específica do gráfico. Dependendo do tipo de gráfico, essa seleção pode ser usada para destacar dados ou filtrar informações. Hover Information: Ao passar o mouse sobre os pontos do gráfico, informações adicionais sobre os dados são exibidas. Isso pode incluir valores numéricos, rótulos e outras informações contextuais que ajudam na interpretação dos dados. Legend (Legenda): A legenda permite ao usuário ativar ou desativar a visibilidade de diferentes séries de dados no gráfico, facilitando a comparação entre categorias ou grupos.  + Demonstração pelo ChatGPT Mais exemplos usando o plotly Exemplo 1: Exemplo 2: Exemplo 3: Exemplo 4: Exemplo 5: Exemplo 6: Produzido pelo estatístico Francis John Anscombe↩︎ https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html↩︎ https://www.r-project.org/about.html↩︎ https://r-graph-gallery.com/↩︎ https://bookdown.org/content/b298e479-b1ab-49fa-b83d-a57c2b034d49/↩︎ https://pt.wikipedia.org/wiki/Gr%C3%A1fico↩︎ https://pt.wikipedia.org/wiki/Gr%C3%A1fico_de_linha↩︎ https://pt.wikipedia.org/wiki/Gr%C3%A1fico_de_barras↩︎ https://pt.wikipedia.org/wiki/Gr%C3%A1fico_de_setores↩︎ https://pt.wikipedia.org/wiki/Diagrama_de_caixa↩︎ https://pt.wikipedia.org/wiki/Histograma↩︎ https://pt.wikipedia.org/wiki/Mapa_de_calor↩︎ https://pt.wikipedia.org/wiki/Isolinha↩︎ https://en.wikipedia.org/wiki/Mosaic_plot↩︎ https://en.wikipedia.org/wiki/Dot_plot_(statistics)↩︎ https://en.wikipedia.org/wiki/Dot_plot_(statistics)↩︎ https://ggplot2.tidyverse.org/↩︎ https://en.wikipedia.org/wiki/Ggplot2↩︎ A Gramática dos Gráficos foi desenvolvida por Leland Wilkinson↩︎ https://exts.ggplot2.tidyverse.org/gallery/↩︎ "],["parte-ii---tratamento-dos-dados.html", "PARTE II - Tratamento dos Dados", " PARTE II - Tratamento dos Dados No contexto de quimiometria, o tratamento dos dados é uma etapa crítica que visa preparar os dados químicos e espectroscópicos para análise e modelagem. O pré-processamento dos dados ajuda a melhorar a qualidade dos dados e a eficácia das análises subsequentes. São raros os casos em que se aplica uma técnica de análise sem antes realizar a etapa de pré-processamento dos dados. O pré-processamento é essencial porque dados brutos frequentemente contêm ruído, variabilidade indesejada e outras imperfeições que podem comprometer a qualidade dos modelos analíticos. Técnicas adequadas de pré-processamento melhoram a precisão dos modelos, facilitam a interpretação dos resultados e ajudam a obter conclusões mais confiáveis a partir dos dados químicos e espectroscópicos. A seguir estão listadas algumas das principais técnicas de pré-processamento usadas em quimiometria. Pré-Processamento de Variáveis Autoescalamento O autoescalamento visa transformar os dados de modo que suas características (como escala e variância) sejam ajustadas para que diferentes variáveis possam ser comparadas de maneira mais uniforme. A normalização pelo Z-score, também conhecida como padronização, é uma técnica específica de autoescalamento. A padronização transforma os dados para ter uma média de zero e um desvio padrão de um. Isso é útil quando os dados com diferentes distribuições precisam ser comparados ou quando a distribuição das variáveis devam ser mais uniformizadas, o que é essencial para evitar que variáveis com diferentes magnitudes dominem a análise. É comum os termos: Autoescalamento, Padronização e Normalização Z-score serem usados de forma intercambiáveis Normalização Mínimo-Máximo: A normalização ‘Min-Max’ escala os dados para um intervalo específico, como por exemplo [0, 1]. Isto é, O procedimento assegura que todos os valores estejam dentro do mesmo intervalo. Pré-Processamento de Amostras Smoothing (Suavização) A suavização é usada para reduzir o ruído em dados espectroscópicos e melhorar a qualidade dos sinais. Métodos comuns incluem o filtro de Savitzky-Golay e a suavização por média móvel. Correção de Base Correção de base ajusta os dados espectroscópicos para remover qualquer linha de base ou fundo que possa ter sido introduzido durante a coleta dos dados. Isso é crucial para obter um espectro limpo e preciso. Remoção de Ruído Reduz o ruído nos dados para melhorar a clareza dos sinais químicos. Técnicas incluem filtros de mediana e algoritmos de redução de ruído baseados em wavelets. Transformações Matemáticas Transformações de Fourier: Convertem os dados do domínio do tempo para o domínio da frequência, frequentemente usadas em espectroscopia. Transformações de Derivada: Calculam derivadas dos dados espectroscópicos para melhorar a resolução de picos e identificar características importantes. Correção de Deslocamento Espectral Ajusta os dados para corrigir deslocamentos ou variações sistemáticas no espectro, garantindo que comparações entre diferentes amostras ou experimentos sejam precisas. Interpolação e Amostragem Interpolação é usada para preencher lacunas nos dados espectroscópicos, enquanto a amostragem pode ser ajustada para uniformizar os pontos de dados e melhorar a resolução. Eliminação de Outliers Identifica e remove outliers que podem distorcer a análise. Isso é feito através de métodos estatísticos ou gráficos de dispersão. Seleção de Características Escolhe as variáveis mais relevantes para a análise, reduzindo a dimensionalidade dos dados e focando apenas nas características que contribuem significativamente para os modelos. "],["pré-processamento-de-variáveis.html", "&#8474;&#8477; 4 Pré-Processamento de Variáveis 4.1 Autoescalamento 4.2 Normalização Min-Max: 4.3 Transformações", " &#8474;&#8477; 4 Pré-Processamento de Variáveis 4.1 Autoescalamento O autoescalamento é um processo de pré-processamento de dados amplamente utilizado em estatística, aprendizado de máquina e quimiometria para transformar os dados para uma escala comum. Isso facilita a análise e modelagem, especialmente quando diferentes variáveis possuem magnitudes e unidades diferentes. O autoescalamento é fundamental para garantir que todas as variáveis contribuam igualmente para a análise, evitando que variáveis com escalas maiores dominem o modelo. Esta transformação ajusta a escala dos dados e a centraliza em torno de zero, facilitando comparações e análises subsequentes. Fórmula do Autoescalamento Para uma variável \\(x\\) com média μ e desvio padrão σ, o autoescalamento é realizado pela transformação:   \\[ x_{autoescalado} = (x − μ ) / σ \\] Onde: \\(x\\) representa os valores da variável original. μ é a média da variável. σ é o desvio padrão da variável. \\(x_{autoescalado}\\) são os valores transformados.   Propriedades do Autoescalamento Uniformização da Escala: Garante que todas as variáveis estejam na mesma escala, o que é especialmente útil em algoritmos que são sensíveis à escala dos dados, como algoritmos de clustering (e.g., K-means) e métodos de aprendizado de máquina (e.g., regressão linear e máquinas de vetor de suporte). Melhoria da Convergência: Pode acelerar a convergência de algoritmos de otimização que utilizam gradientes, pois elimina a influência desproporcional de variáveis com escalas maiores. Interpretação Consistente: Facilita a interpretação dos coeficientes de modelos, especialmente em regressão, onde variáveis com diferentes escalas podem obscurecer a relação entre variáveis e respostas. Comparação e Visualização: Facilita a comparação entre variáveis e a visualização dos dados em gráficos, pois todas as variáveis têm a mesma escala. (Comumente, os softwares que criam gráficos autoescalam as variáveis para melhorar a clareza das dispersões dos pontos, mas mantêm os rótulos das variáveis originais. Esta é uma das aplicações do autoescalamento).   Aplicações Comuns Aprendizado de Máquina: Antes de aplicar algoritmos de aprendizado de máquina, como K-means, PCA (Análise de Componentes Principais), e regressão, os dados são frequentemente autoescalados para garantir um desempenho eficaz e resultados confiáveis. Análise Estatística: Em análises estatísticas multivariadas, como a Análise de Componentes Principais (PCA) e Análise Discriminante Linear (LDA), o autoescalamento é usado para garantir que cada variável contribua de forma equitativa para as análises. Quimiometria: Na análise de dados espectroscópicos e quimiométricos, o autoescalamento ajuda a normalizar os dados para melhorar a interpretação dos modelos e a precisão das previsões.   Exemplo Prático em R Abaixo segue um exemplo de como realizar o autoescalamento de dados em R, usando a função scale(): # Carregar pacotes necessários library(ggplot2) # Definir o seed para reprodutibilidade set.seed(42) # Criar um dataframe com 2 variáveis e 25 amostras df &lt;- data.frame( Variavel1 = rnorm(25, mean = 50, sd = 10), Variavel2 = rnorm(25, mean = 100, sd = 20) ) # Exibir o dataframe original print(&quot;Dataframe Original:&quot;) print(df) # Aplicar autoescalamento df_autoescalado &lt;- as.data.frame( scale(df) ) # Exibir o dataframe autoescalado print(&quot;Dataframe Autoescalado:&quot;) print(df_autoescalado) # Gráfico dos dados originais p1 &lt;- ggplot(df, aes(x = Variavel1, y = Variavel2)) + geom_point(color = &quot;blue&quot;, size=2) + labs(title = &quot;Dados Originais&quot;, x = &quot;Variável 1&quot;, y = &quot;Variável 2&quot;) # Gráfico dos dados autoescalados p2 &lt;- ggplot(df_autoescalado, aes(x = Variavel1, y = Variavel2)) + geom_point(color = &quot;red&quot;, size=2) + labs(title = &quot;Dados Autoescalados&quot;, x = &quot;Variável 1 (Autoescalado)&quot;, y = &quot;Variável 2 (Autoescalado)&quot;) # Mostrar os gráficos library(gridExtra) grid.arrange(p1, p2, ncol = 2) Você pode copiar e colar esse código em um script R para gerar e visualizar os gráficos dos dados originais e autoescalados.   Explicação do Código Criação do Dataframe: Utilizamos rnorm() para gerar 25 amostras para duas variáveis com médias e desvios padrão diferentes. Autoescalamento: Aplicamos a função scale() para normalizar os dados, transformando-os para ter média zero e desvio padrão um. Gráficos: Dados Originais: Exibimos um gráfico de dispersão dos dados originais. Dados Autoescalados: Exibimos um gráfico de dispersão dos dados após o autoescalamento. Visualização: Usamos gridExtra para mostrar os gráficos lado a lado. O autoescalamento é uma técnica essencial no pré-processamento de dados, garantindo que todos os dados sejam tratados de maneira justa e adequada para análises e modelagens subsequentes.   Saída Gráfica Na janela de saída gráfica do R, deve aparecer o gráfico seguinte.   Figure 4.1: Dados originais e autoescalados.   O que deve ser notado? A mudança de escala de cada variável individualmente deve ser óbvia, e a distância relativa entre as variáveis não se altera! É justamente essa propriedade que nos garante que a informação química que pode estar contida nos dados não se perde.   4.2 Normalização Min-Max: Embora a normalização pelos valores mínimos e máximos não seja completamente resistente a outliers, ela pode reduzir o impacto que valores extremos têm na análise. Outliers podem distorcer a escala e a interpretação dos dados, e a normalização ajuda a mitigar esse efeito ao transformar os dados para um intervalo padrão.   Fórmula para a normalização \\[ x_{normalizado} = ( x − x_{min} ) / ( x_{max} - x_{min} ) \\] onde: \\(x\\) representa os dados originais \\(x_{min}\\) é o valor mínimo da variável \\(x_{min}\\) é o valor máximo da variável \\(x_{normalizado}\\) são os dados normalizados   Passos para Normalização Min-Max Determine os Valores Mínimo e Máximo: Encontre os valores mínimo e máximo da variável que você deseja normalizar. Aplicar a Fórmula: Para cada variável \\(x\\), aplique a fórmula para transformar o valor para o intervalo [0, 1].   Exemplo Prático em R # Criar um vetor de dados dados &lt;- c(10, 20, 30, 40, 50) # Normalizar os dados para o intervalo [0, 1] normalizado &lt;- (dados - min(dados)) / (max(dados) - min(dados)) # Exibir os dados normalizados print(normalizado) # Para normalizar os dados contidos em um dataframe ou uma matriz # Definir o seed para reprodutibilidade set.seed(42) # Criar um dataframe com 2 variáveis e 25 amostras df &lt;- data.frame( Variavel1 = rnorm(25, mean = 50, sd = 10), Variavel2 = rnorm(25, mean = 100, sd = 20) ) # Aplicar a normalização Min-Max df_normalizado &lt;- apply( df, 2, function(df)(df - min(df)) / (max(df) - min(df)) ) #Exibir os dados normalizados print( df_normalizado)   Explicação do Trecho do Código # Aplicar a normalização Min-Max df_normalizado &lt;- apply( df, 2, function(df)(df - min(df)) / (max(df) - min(df)) ) Passo a Passo: Função apply(): apply() é uma função em R que aplica uma função especificada a margens de uma matriz ou data frame. A sintaxe básica é apply(X, MARGIN, FUN, ...), onde X é a matriz ou data frame, MARGIN indica a margem sobre a qual a função deve ser aplicada (1 para linhas e 2 para colunas), e FUN é a função a ser aplicada. (Estes detalhes estão no help da função apply). Argumentos do apply(): df: Este é o data frame ou matriz ao qual a função será aplicada. 2: Este argumento indica que a função deve ser aplicada a cada coluna do data frame (MARGIN = 2 significa aplicar a função por coluna). function(df)(df - min(df)) / (max(df) - min(df)): Este é o código da função que será aplicada a cada coluna. Função anônima: function(df) (df - min(df)) / (max(df) - min(df)) é uma função anônima (ou seja, uma função definida sem nome). Essa função realiza a normalização Min-Max para cada coluna do data frame. A normalização Min-Max transforma os valores para uma faixa de 0 a 1. Destrinchando a Função Anônima: df - min(df): Para cada valor na coluna, subtrai o valor mínimo dessa coluna. Isso é feito para ajustar os valores, começando do zero. max(df) - min(df): Calcula a diferença entre o valor máximo e o valor mínimo da coluna (também chamado de amplitude). Isso é usado para dimensionar os valores ajustados para a faixa de 0 a 1. (df - min(df)) / (max(df) - min(df)): Divide o ajuste realizado na primeira etapa pela faixa (a diferença entre o valor máximo e o valor mínimo). O resultado é que os valores da coluna são escalados para o intervalo de 0 a 1. Resultado: df_normalizado: O resultado do apply() é um novo data frame (ou matriz, dependendo da estrutura original de df) onde cada coluna é normalizada usando a técnica Min-Max. Os valores em cada coluna vão variar entre 0 e 1. Considerações Adicionais: Estrutura do Data Frame: apply() pode transformar um data frame em uma matriz, o que pode causar problemas se o data frame original contiver tipos de dados mistos (por exemplo, números e caracteres). Se for o caso, considere converter para uma matriz ou usar funções específicas para data frames, como lapply(). 4.3 Transformações É muito comum, na construção de modelos preditivos, o uso de transformações40 de variáveis para abordar e corrigir vários problemas corriqueiros como assimetria na distribuição, heterocedasticidade, não linearidade e outliers. O uso adequado dessas transformações pode melhorar significativamente a eficácia das previsões, isto é, resultar em modelos mais precisos, robustos e interpretáveis. As transformações têm como objetivo abordar os seguintes aspectos: 1. Correção da Assimetria: Dados assimétricos (ou seja, que apresentam uma cauda mais longa de um lado do que do outro) podem levar a ajustes imprecisos dos modelos prejudicando o desempenho preditivo, pois muitos modelos estatísticos assumem que os dados seguem uma distribuição normal. Transformações como a logarítmica, a raiz quadrada ou a Box-Cox podem ajudar a suavizar a distribuição dos dados, tornando-a mais simétrica e, assim, melhorando a precisão do modelo. 2. Estabilização da Variância: Em muitos casos, a variância dos dados não é constante em diferentes níveis da variável preditora, um fenômeno conhecido como heterocedasticidade. Isso pode levar a previsões imprecisas e inferências incorretas. Transformações como a Box-Cox ou a logarítmica podem estabilizar a variância, fazendo com que a variabilidade dos dados seja mais uniforme e adequada para modelos que assumem homocedasticidade. 3. Melhora na Linearidade: Muitos modelos preditivos, como a regressão linear, pressupõem uma relação linear entre as variáveis independentes e a variável dependente. No entanto, a relação real pode ser não linear. Transformações podem transformar essas relações não lineares em lineares, facilitando a modelagem e a interpretação dos dados. Por exemplo, uma transformação logarítmica pode transformar uma relação exponencial em uma relação linear, melhorando a adequação do modelo. 4. Redução do Impacto de Outliers: Transformações podem reduzir o impacto de valores extremos ou outliers, que podem distorcer a análise e a construção do modelo. Por exemplo, a transformação logarítmica pode minimizar o efeito de valores muito altos, tornando-os menos influentes no ajuste do modelo. 5. Melhoria na Interpretabilidade: Em alguns casos, a transformação das variáveis pode facilitar a interpretação dos coeficientes do modelo. Por exemplo, em uma transformação logarítmica, o coeficiente da variável pode ser interpretado como a taxa de crescimento percentual, o que pode ser mais intuitivo e informativo para decisões práticas. 6. Ajuste para Problemas Específicos de Dados: Alguns tipos de dados, como contagens ou dados que variam em ordens de magnitude, podem se beneficiar de transformações específicas que são mais adequadas para o tipo de dados em questão. Transformações apropriadas podem levar a modelos mais precisos e robustos. 4.3.1 Logarítmica e Raiz Quadrada As transformações por aplicação da operação logarítmica e da operação raiz quadrada são amplamente empregadas quando a variável apresenta uma distribuição assimétrica ou uma variância elevada. São monotônicas, o que significa que preserva a ordem dos dados. Isso é crucial para modelos preditivos, pois assegura que a transformação não altere a capacidade do modelo de capturar a relação entre as variáveis. Também são não-paramétricas.   Transformação logarítmica: \\[ y_{transformada} = log( y ) \\] Transformação raiz quadrada: \\[ y_{transformada} = \\sqrt y \\] onde: \\(y\\) é a variável original a ser transformada. \\(y_{transformada}\\) é a variável transformada.   A diferença básica é entre as duas operações é o quão rapidamente conseguem contrair a escala original. Graficamente é mais intuitivo essa percepção. Figure 4.2: Transformações Logarítmica e Raiz Quadarada   4.3.2 Transformação Box-Cox A Transformação Box-Cox41 é uma técnica estatística utilizada para estabilizar a variância e tornar os dados mais próximos de uma distribuição normal, o que pode ser útil para melhorar a qualidade de modelos estatísticos e de machine learning. Aqui estão alguns pontos principais sobre a Transformação Box-Cox: Objetivo: A transformação visa lidar com problemas de heterocedasticidade (variação não constante) e não-normalidade nos dados. A normalidade e a homocedasticidade são frequentemente pressupostos em muitos testes estatísticos e modelos de regressão. Formulação Matemática: A fórmula da Transformação Box-Cox é dada por: \\[ y(\\lambda) = \\begin{cases} \\frac{y^\\lambda - 1}{\\lambda} &amp; \\text{se } \\lambda \\neq 0 \\\\ \\log(y) &amp; \\text{se } \\lambda = 0 \\end{cases} \\] onde: \\(y\\) é a variável original a ser transformada. \\(\\lambda\\) é o parâmetro de transformação, que pode assumir qualquer valor real. Escolha de \\(\\lambda\\): O parâmetro \\(\\lambda\\) é ajustado para encontrar a melhor transformação possível dos dados. A escolha de \\(\\lambda\\) é geralmente feita através de métodos de da maximização da função de verossimilhança, que busca o valor \\(\\lambda\\) que melhor estabiliza a variância maximiza e melhor proximidade dos dados transformados a uma distribuição normal. Em alguns softwares estatísticos, você pode encontrar o melhor \\(\\lambda\\) automaticamente. A transformação ajuda a reduzir a heterocedasticidade, ou seja, a variação não constante dos resíduos em um modelo de regressão. O melhor ajuste dos dados a uma distribuição normal pode melhorar a adequação dos modelos estatísticos e facilitar a interpretação dos resultados. A transformação é uma forma generalizada de várias outras transformações comuns, como a transformação logarítmica (para \\(\\lambda = 0\\)) e a raiz quadrada (\\(\\lambda = 0.5\\)). Restrições: A Transformação Box-Cox só pode ser aplicada a dados positivos (ou seja, \\(y &gt; 0\\)). Se seus dados contiverem valores não positivos, pode ser necessário adicionar uma constante aos dados antes da transformação ou utilizar outras transformações. A escolha de \\(\\lambda\\) pode ser sensível e pode variar dependendo dos dados. Uma transformação que funciona bem para um conjunto de dados pode não ser ideal para outro (Existem outras transformações, como a transformação logarítmica simples ou a transformação de raiz quadrada, que podem ser mais apropriadas dependendo do contexto dos dados e dos objetivos da análise). Aplicações: A Transformação Box-Cox é amplamente utilizada em análise de regressão, ANOVA e outros modelos estatísticos onde os pressupostos de normalidade e homocedasticidade são importantes. Ela pode melhorar a validade dos resultados e a eficácia dos modelos. Modelagem Estatística: Em modelos de regressão e ANOVA, onde pressupostos de normalidade e homocedasticidade são importantes, a Transformação Box-Cox pode melhorar o ajuste do modelo e a validade dos testes. Previsão: Em modelos de séries temporais e de previsão, a transformação pode estabilizar a variância das observações ao longo do tempo, tornando os modelos mais robustos. Análise Exploratória: A transformação pode ser usada para explorar a natureza dos dados e identificar a melhor abordagem para análise e modelagem. Interpretação: Após aplicar a transformação, interpretar os resultados pode ser menos intuitivo exigir cuidados, pois a transformação altera a escala dos dados. Dependendo de \\(\\lambda\\), a interpretação dos coeficientes e das previsões pode exigir uma adaptação. Em casos particulares, pode ser necessário traduzir os resultados transformados de volta para a escala original. Em síntese, a Transformação Box-Cox é utilizada para ajustar dados e melhorar a performance dos modelos estatísticos, ajudando a lidar com problemas comuns como heterocedasticidade e não-normalidade. No entanto, como qualquer ferramenta estatística, deve ser usada com compreensão de suas limitações e com cuidado na interpretação dos resultados.   No R, o pacote MASS possui a função boxcox que implementa essa transformação. 4.3.3 Logística e Logit Função Logística A função logística42 é a curva com um formato de “S” quando representada graficamente, sendo conhecidad como sigmoidal. O contradomínio da função começa em \\(0\\), sobe rapidamente, e depois se aproxima de \\(1\\), criando uma transição suave entre esses dois limites. É definida pela fórmula: \\[ z = \\left(\\frac{1}{1+e^{-y}}\\right) \\] À medida que \\(y\\) tende a \\(-\\infty\\), a função se aproxima de \\(0\\). À medida que \\(y\\) tende a \\(\\infty\\), a função se aproxima de \\(1\\). O ponto de inflexão da função logística ocorre em \\(y = 0\\), onde a função assume o valor \\(0,5\\). Esse ponto é onde a função muda de crescer lentamente para crescer rapidamente. A função pode ser generalizada facilmente para contemplar os seus ajustes a outros limites. Uso da Função Logística Modelagem de Probabilidades: A função logística é usada para mapear qualquer valor real \\(z\\) para um intervalo entre \\(0\\) e \\(1\\), tornando-a útil para modelar probabilidades. Isso é especialmente importante em modelos de regressão logística, onde a saída do modelo deve ser uma probabilidade. Modelos de Crescimento: Em biologia e outras ciências, a função logística é usada para descrever o crescimento populacional limitado por fatores de capacidade de carga. Redes Neurais: Na aprendizagem de máquinas e redes neurais, a função logística pode ser usada como uma função de ativação para introduzir não-linearidade nos modelos. Relação com a Função Logit A função logística é a inversa da função logit. Enquanto a função logit transforma uma probabilidade em uma escala não restrita, a função logística faz a transformação inversa, convertendo um valor em uma probabilidade. Se você tem um valor \\(z\\) (que pode ser o resultado de uma combinação linear de variáveis independentes em um modelo de regressão logística), a função logística é usada para obter a probabilidade \\(p\\). Dessa forma, a função logística é essencial para transformar as previsões de um modelo de regressão logística em probabilidades compreensíveis e interpretáveis. Função logit A função logit43 é uma função matemática que é frequentemente usada em modelos estatísticos, especialmente na regressão logística. A função logit é definida como o logaritmo da razão de chances (odds) e é dada pela fórmula: \\[ \\text{logit}(p) = \\log \\left(\\frac{p}{1-p}\\right) \\] onde \\(p\\) é a probabilidade de um evento ocorrer, e \\(\\frac{p}{1-p}\\) é a razão de chances do evento. A função logit transforma uma probabilidade \\(p\\) (que varia entre \\(0\\) e \\(1\\)) em uma escala que vai de \\(-\\infty\\) a \\(\\infty\\). Isso é útil em modelos de regressão logística porque a probabilidade \\(p\\) é restrita ao intervalo \\([0,1]\\), mas a função logit permite que a relação entre variáveis preditoras e a probabilidade de um evento seja modelada em uma escala não restrita. A função logit é a função inversa da função logística. Figure 4.3: Curvas Logística e logit https://en.wikipedia.org/wiki/Data_transformation_(statistics)↩︎ https://en.wikipedia.org/wiki/Power_transform#Box%E2%80%93Cox_transformation↩︎ https://pt.wikipedia.org/wiki/Fun%C3%A7%C3%A3o_log%C3%ADstica↩︎ https://en.wikipedia.org/wiki/Logit↩︎ "],["pré-processamento-de-amostras.html", "&#8474;&#8477; 5 Pré-Processamento de Amostras 5.1 TODO", " &#8474;&#8477; 5 Pré-Processamento de Amostras 5.1 TODO Pré-Processamento de Amostras Smoothing (Suavização) Remoção de Ruído TODO Correção de Linha Base TODO Transformações Matemáticas Primeira Derivada Segunda Derivada TODO Correção de Deslocamento Espectral Spectral Shift Correction TODO Correção Multiplicativa de Sinal (Multiplicative Scatter Correction - MSC) TODO Alinhamento de sinais TODO Padrão Normal de Variação (Standard Normal Variate- SNV) TODO Interpolação e Amostragem TODO Eliminação de Outliers TODO Seleção de Características (Variáveis) TODO "],["parte-iii---análise-exploratória-de-dados.html", "PARTE III - Análise Exploratória de Dados", " PARTE III - Análise Exploratória de Dados A Análise Exploratória de Dados (AED)44 é uma etapa fundamental que visa explorar e interpretar grandes volumes de dados químicos de maneira intuitiva e visual. Ela é usada principalmente para entender a estrutura de conjuntos de dados multivariados, como aqueles gerados em espectroscopia, cromatografia, e outros métodos instrumentais. A AED busca resumir as principais características dos dados e revelar padrões, tendências e relações entre variáveis sem se preocupar em fazer suposições rigorosas sobre eles. Quando se trata de dados multivariadas, comumente sendo o caso na quimiometria, os dados são geralmente de alta dimensionalidade (com muitas variáveis e observações), o que torna a AED essencial para entender e reduzir a complexidade dos dados antes de aplicar técnicas mais sofisticadas, como Análise de Componentes Principais (PCA), regressão multivariada, ou análises discriminantes. AED na Quimiometria A AED serve para vários propósitos, tais como: Compreender a estrutura dos dados: Em dados multivariados, a AED permite identificar as variáveis que mais influenciam a variabilidade do conjunto de dados, destacando aquelas que possuem maior poder explicativo ou relevância no comportamento geral de todas as amostras. Isso é realizado por meio da análise da magnitude da contribuição de cada variável para os padrões observados, como variações entre grupos e correlações entre variáveis, permitindo uma compreensão mais clara dos fatores predominantes que governam o comportamento do sistema estudado. Visualizar padrões e agrupamentos: Na AED, técnicas como PCA (Análise de Componentes Principais) e HCA (Análise Hierárquica de Agrupamentos) são amplamente utilizadas para identificar padrões e clusters (grupos) de amostras. O PCA reduz a dimensionalidade dos dados, permitindo a visualização de possíveis agrupamentos e relações subjacentes entre as variáveis, enquanto o HCA cria dendrogramas que destacam hierarquias e conexões entre as amostras. Essa combinação facilita uma análise inicial das interações potenciais entre as variáveis, proporcionando uma visão mais estruturada e profunda dos padrões complexos que dificilmente seriam detectados ao analisar as variáveis de forma isolada. Detectar outliers: Valores extremos ou pontos fora do padrão podem distorcer os resultados em análises multivariadas, afetando precisão de modelos e interpretações. A AED auxilia na identificação desses outliers, permitindo uma avaliação cuidadosa de sua origem e impacto, o que é fundamental para garantir a robustez e confiabilidade das análises subsequentes. Avaliar a correlação entre variáveis: É comum a necessidade de investigar as relações entre variáveis químicas em estudos quimiométricos. A AED facilita essa tarefa ao identificar correlações significativas, permitindo que os pesquisadores compreendam como as variáveis se influenciam mutuamente. Essa análise inicial é crucial para orientar sobre escolha de métodos mais adequados para explorar essas interações.” Preparar dados para análises posteriores: A AED desempenha um papel crucial na identificação de dados ausentes e variáveis redundantes, permitindo que os pesquisadores reconheçam e tratem problemas de qualidade nos dados antes de realizar análises mais complexas. Além disso, ela fornece orientações para a seleção dos métodos mais adequados de normalização ou transformação dos dados, garantindo que estejam devidamente preparados para a aplicação em modelos de regressão ou classificação multivariada. Esse processo é fundamental para assegurar a qualidade e a precisão das análises subsequentes. Na Quimiometria, as técnicas mais utilizadas para explorar, visualizar e fornecer informações para interpretações básicas incluem: Análise de Componentes Principais (PCA): Reduz a dimensionalidade dos dados, destacando as variáveis que mais contribuem para a variabilidade. Facilita a visualização de padrões e relações entre amostras e variáveis. Análise Hierárquica de Agrupamentos (HCA): Cria dendrogramas que representam hierarquias de agrupamentos, permitindo a identificação de relações entre amostras com base em similaridades. Em combinação com as técnicas mencionadas acima, é recomendável utilizar ferramentas gráficas desde o início da exploração dos dados, pois elas podem evidenciar se há informações relevantes em variáveis específicas de interesse. Algumas das ferramentas gráficas mais comuns incluem: Gráficos de Dispersão: Utilizados para visualizar a relação entre duas variáveis, ajudando a identificar padrões, tendências e possíveis outliers. Boxplots: Representam a distribuição de uma variável e ajudam a visualizar a presença de outliers, a mediana e a variação dos dados. Histogramas: Exibem a distribuição de frequências de variáveis, permitindo uma análise inicial da sua forma e dispersão. Mapas de Calor (Heatmaps): Usados para visualizar a intensidade de dados em uma matriz, facilitando a identificação de padrões e correlações entre variáveis. Histórico A Análise Exploratória de Dados (AED) é uma abordagem que ganhou destaque significativo através das contribuições de John W. Tukey, um estadístico americano cujas ideias influenciaram profundamente a análise de dados em várias disciplinas, incluindo a quimiometria. Contribuições de John W. Tukey Contexto e Filosofia: Em meados do século XX, Tukey começou a enfatizar a importância de entender os dados antes de aplicar métodos estatísticos mais rigorosos. Ele acreditava que os analistas deveriam explorar os dados para descobrir padrões e insights, ao invés de confiar cegamente em modelos estatísticos tradicionais. Publicação de “Exploratory Data Analysis”45: Em 1977, Tukey publicou o livro “Exploratory Data Analysis”, que se tornou um marco na estatística. Nesse trabalho, ele apresentou uma variedade de métodos gráficos e técnicas estatísticas que ajudavam a visualizar e compreender os dados de forma intuitiva. O livro introduziu conceitos como boxplots, gráficos de dispersão e histogramas, que ainda são amplamente utilizados hoje. Ênfase na Visualização: Tukey defendia que a visualização dos dados era uma parte essencial da análise estatística. Ele argumentava que gráficos e representações visuais poderiam revelar informações que poderiam passar despercebidas em análises numéricas puras. Essa ênfase na visualização ajudou a moldar a forma como os analistas lidam com dados complexos. Mudança de Paradigma: A abordagem de Tukey representou uma mudança de paradigma na estatística. Em vez de focar apenas em testes de hipóteses e modelagem formal, ele incentivou a exploração e a descrição dos dados. Essa mudança de foco ajudou a tornar a estatística mais acessível e prática para cientistas e profissionais de diversas áreas. Impacto Duradouro: As ideias de Tukey sobre AED influenciaram não apenas a estatística, mas também campos como a ciência de dados, a quimiometria e a análise multivariada. Seu trabalho continua a ser fundamental para a formação de estatísticos e analistas de dados, reforçando a importância da exploração e visualização antes de qualquer análise formal. Em suma, a contribuição de John W. Tukey à AED estabeleceu um fundamento sólido para a análise de dados, promovendo uma abordagem mais intuitiva e visual que continua a impactar a prática estatística e a ciência de dados até os dias de hoje. https://pt.wikipedia.org/wiki/An%C3%A1lise_explorat%C3%B3ria_de_dados↩︎ Tukey, John W.: Exploratory Data Analysis, Addison-Wesley Publishing Company Reading, Mass, 1977, XVI, 688 pages.↩︎ "],["pca---análise-de-componentes-principais.html", "&#8474;&#8477; 6 PCA - Análise de Componentes Principais 6.1 Desenvolvimento histórico 6.2 Definição 6.3 Interpretação Geométrica 6.4 Objetivo 6.5 PCA como Decomposição Matricial 6.6 Variância por PC 6.7 Aplicação Típica", " &#8474;&#8477; 6 PCA - Análise de Componentes Principais A Análise de Componentes Principais (PCA)46 é uma técnica estatística multivariada amplamente utilizada para reduzir a dimensionalidade de conjuntos de dados, preservando a maior quantidade possível de variabilidade presente nos dados originais. Essa abordagem transforma um grande número de variáveis correlacionadas em um conjunto menor de variáveis não correlacionadas, chamadas de componentes principais. Os componentes principais são combinações lineares das variáveis originais e são ordenados de forma que o primeiro componente captura a maior parte da variabilidade dos dados, seguido pelo segundo, e assim por diante. 6.1 Desenvolvimento histórico A Análise de Componentes Principais tem seu histórico que remonta ao início do século XX. Aqui estão os principais marcos na evolução dessa técnica: Princípios Iniciais: Os princípios da PCA podem ser traçados até o trabalho de Karl Pearson47, um estatístico britânico. Em 1901, ele publicou um artigo intitulado “On Lines and Planes of Closest Fit to Systems of Points in Space”48, onde apresentou o conceito de “análise de fatores” e a ideia de encontrar uma linha de melhor ajuste para um conjunto de pontos em um espaço multidimensional. Este trabalho é considerado uma das primeiras referências à ideia subjacente da PCA. Desenvolvimentos Posteriores: Em 1933, Harold Hotelling49, um estatístico americano, expandiu as ideias de Pearson e formalizou a técnica da PCA em seu trabalho “Analysis of a Complex of Statistical Variables into Principal Components”50. Neste artigo, Hotelling descreveu um método para reduzir a dimensionalidade de dados multivariados, mantendo a maior parte da variabilidade nos dados originais. Ele introduziu a terminologia e as fórmulas que são fundamentais para a PCA moderna. Adoção e Popularização: Ao longo do século XX, a PCA começou a ser adotada em várias disciplinas, incluindo psicologia, biologia, e, mais tarde, em áreas como ciências sociais e, mais recentemente, em ciências de dados e quimiometria. O conceito foi utilizado para análises de dados complexos, onde a visualização e a interpretação de múltiplas variáveis se tornaram essenciais. Avanços Computacionais: Com o advento dos computadores e o aumento do poder computacional nas décadas de 1970 e 1980, a PCA tornou-se ainda mais acessível e prática. Softwares estatísticos começaram a incorporar a PCA como uma ferramenta padrão para análise de dados, permitindo que pesquisadores e profissionais realizassem análises multivariadas com maior facilidade. Aplicações Modernas: Nos anos 2000 e além, a PCA passou a ser uma técnica fundamental na análise de grandes volumes de dados, especialmente na era do Big Data. Sua aplicação se estendeu para áreas como aprendizado de máquina etc, onde a redução de dimensionalidade e a identificação de padrões são cruciais. Em síntese, o método PCA evoluiu de um conceito inicial apresentado por Pearson para uma técnica robusta e amplamente utilizada, graças ao trabalho de Hotelling e ao avanço da computacão. Sua relevância na análise de dados contemporâneos atesta sua eficácia na extração de informações significativas, contexto do qual a quimiometria está inserida. 6.2 Definição A explicação apresentada anteriormente pode ainda não ser satisfatória e suficiente para um entendimento completo do PCA, especialmente para aqueles que estão iniciando seus estudos. Assim, vamos retomar e destacar alguns pontos-chave de forma mais direta para esclarecer o conceito de maneira mais eficaz. Vamos analisar cada ponto com atenção e com melhores definições:   O que são essas Componentes Principais? São novas variáveis produzidas a partir das variáveis originais   Como são criadas essas novas variáveis chamadas de Componentes Principais? São combinações lineares das variáveis originais.   Considerando que as combinações lineares desempenham um papel crucial para a definição do PCA, vamos explorar esse aspecto em mais detalhes e compreender como elas geram as componentes principais. Combinações Lineares Considere que temos nossos dados numa matrix \\(\\mathbf{X}\\) com \\(n\\) amostras (linhas) e \\(m\\) variáveis (colunas). Podemos combinar linearmente as \\(m\\) variáveis para formar \\(k\\) componentes principais da seguinte forma: \\[ \\mathbf{t}_1 = p_{1,1} \\cdot \\mathbf{x}_{\\cdot,1} + p_{1,2} \\cdot \\mathbf{x}_{\\cdot,2} + p_{1,3} \\cdot \\mathbf{x}_{\\cdot,3} + \\ldots + p_{1,m} \\cdot \\mathbf{x}_{\\cdot, m} \\\\ \\\\ \\mathbf{t}_2 = p_{2,1} \\cdot \\mathbf{x}_{\\cdot,1} + p_{2,2} \\cdot \\mathbf{x}_{\\cdot,2} + p_{2,3} \\cdot \\mathbf{x}_{\\cdot,3} + \\ldots + p_{2,m} \\cdot \\mathbf{x}_{\\cdot, m} \\\\ \\\\ \\mathbf{t}_3 = p_{3,1} \\cdot \\mathbf{x}_{\\cdot,1} + p_{3,2} \\cdot \\mathbf{x}_{\\cdot,2} + p_{3,3} \\cdot \\mathbf{x}_{\\cdot,3} + \\ldots + p_{3,m} \\cdot \\mathbf{x}_{\\cdot, m} \\\\ \\\\ \\vdots \\\\ \\\\ \\mathbf{t}_k = p_{k,1} \\cdot \\mathbf{x}_{\\cdot,1} + p_{k,2} \\cdot \\mathbf{x}_{\\cdot,2} + p_{k,3} \\cdot \\mathbf{x}_{\\cdot,3} + \\ldots + p_{k,m} \\cdot \\mathbf{x}_{\\cdot, m} \\]        onde: \\(\\mathbf{t}_1\\), \\(\\mathbf{t}_2\\), \\(\\mathbf{t}_3\\), …, \\(\\mathbf{t}_k\\) são vetores. São chamados de scores. \\(p_{1,1}\\), \\(p_{1,2}\\), \\(p_{1,3}\\), …, \\(p_{k,m}\\) são valores escalares e são chamados de pesos para cada variável. São chamados de loadings. \\(\\mathbf{x}_{\\cdot,1}\\), \\(\\mathbf{x}_{\\cdot,2}\\), \\(\\mathbf{x}_{\\cdot,3}\\), …, \\(\\mathbf{x}_{\\cdot,m}\\) são as colunas da matriz \\(\\mathbf{X}\\). \\(k\\) pode variar de 1 até \\(w=min(n,m)\\) (o menor valor entre \\(n\\) e \\(m\\)). Uma notação mais compacta para as equações pode ser expressa como: \\[ \\mathbf{t}_k = \\sum_{k=1}^{w} \\sum_{j=1}^{m} p_{k,j} \\cdot \\mathbf{x}_{\\cdot,j} \\] Essa notação resume e deixa explicíto as combinações lineares que resultam em cada vetor \\(\\mathbf{t}_k\\) em termos dos coeficientes \\(p_{k,j}\\) e dos vetores \\(\\mathbf{x}_{\\cdot,j}\\).   É fundamental destacar que as combinações lineares apresentadas até agora permitem infinitas possibilidades, pois ainda não impusemos restrições quanto à variabilidade dos dados que elas devem descrever. Para que as combinações lineares se tornem as componentes principais, como mencionamos no início, é essencial e necessário definir um critério que as caracterize. Esse critério se refere à forma como será a representação da variabilidade dos dados. O próximo ponto abordará essa condição específica.   Então como são feitas essas combinações lineares considerando a descrição da variabilidade dos dados? A primeira componente principal captura, obrigatoriamente, a maior parte de informação possível contida dos dados. Cada outra nova componente principal, ao ser definida, deve ser obrigatoriamente ortogonal às anteriores e capturar a maior variação possível dos dados que ainda não foi explicada.   Por ortogonalidade entenda-se que não há correlação entre eles, ou seja, cada componente principal representa uma direção distinta na variabilidade dos dados. Em termos estritamente matemáticos, duas componentes principais (vetores, direções etc) são ortogonais se o produto interno entre elas é zero. No sentido sobre capturar informações dos dados, diz-se que as componentes são independentes entre si. Essa independência é crucial no PCA, pois permite que cada componente capture informações únicas sobre a variação nos dados, sem redundância. Em outras palavras, a primeira componente principal explica a maior parte da variância dos dados, a segunda componente, que é ortogonal à primeira, explica a maior parte da variância restante (que necessariamente não foi capturada na primeira), e assim por diante. A intenção dos três pontos abordados é esclarecer as condições necessárias para a definição completa de PCA. Compreendendo esses aspectos, teremos uma base sólida sobre o que é PCA, em termos formais. No entanto, é igualmente importante explorar mais dois outros pontos, que são, a interpretação geométrica do PCA e os objetivos para os quais o PCA é utilizado. 6.3 Interpretação Geométrica A explicação anterior tratou do PCA sob uma perspectiva algébrica. No entanto, também podemos compreendê-lo geometricamente, destacando que a transformação resultante da combinação linear pode ser vista como uma rotação dos eixos originais. Essa rotação é realizada de forma a maximizar a captura da variabilidade dos dados, mantendo a ortogonalidade das novas direções, conforme já definido. No caso de duas variáveis, essa rotação se torna facilmente visualizável e imediatamente reconhecível, permitindo uma compreensão intuitiva do processo. Para facilitar a ilustração e a visualização, consideraremos uma matriz \\(\\mathbf{X}_{100,2}\\)​ composta por duas variáveis, \\(\\mathbf{x}_1\\)​ e \\(\\mathbf{x}_{2}\\)​, contendo 100 linhas de valores aleatórios. O processo de transformar as variáveis originais em duas novas componentes principais pode ser ilustrado no gráfico a seguir: Figure 6.1: PCA como rotação dos dados  + Detalhes Suplemento: Mais detalhes sobre a matriz X[100,2] Outpu do R para: Correlação de \\(\\mathbf{X}\\) Dados originais Resumo sobre PCA, de \\(\\mathbf{X}\\) # correlação &gt; cov( X ) x1 x2 x1 1.0000000 0.8303746 x2 0.8303746 1.0000000 &gt; # Dados Originais &gt; round(as.numeric( X[,1]),3) [1] 0.305 0.416 -0.318 -0.627 0.324 -1.851 0.345 -0.468 -0.475 -1.045 [11] -0.324 1.410 0.113 0.247 -0.893 0.513 -1.015 -0.517 0.785 0.048 [21] 0.479 1.086 -0.798 -1.613 -1.653 1.399 -0.652 0.337 0.329 -0.366 [31] 0.508 1.751 1.618 1.244 0.008 0.221 -0.511 -1.711 1.366 -0.197 [41] 0.792 -2.355 -1.171 0.621 0.547 1.090 -1.488 0.289 0.303 -1.392 [51] -0.705 1.527 -0.172 0.096 -0.822 0.029 0.400 0.519 1.704 -2.325 [61] -0.086 -1.424 0.276 1.206 -0.746 -1.864 0.577 1.210 0.244 -1.382 [71] -0.171 -0.924 -1.161 1.871 1.038 0.626 0.521 -0.948 0.207 0.696 [81] 0.359 0.716 -0.493 2.002 0.651 1.455 0.383 -0.496 0.261 0.520 [91] -1.085 -0.987 1.473 -0.571 -1.100 0.397 -0.673 1.716 -0.758 -0.843 &gt; &gt; round(as.numeric( X[,2]),3) [1] 0.335 -0.344 -0.037 -1.243 0.303 -1.751 0.069 0.476 -0.640 -0.654 [11] -0.957 0.330 0.761 0.896 -0.759 0.091 -0.838 -0.076 1.789 -0.026 [21] 1.201 1.545 -0.463 -0.422 -1.535 0.413 -1.551 0.876 -0.947 -0.970 [31] 1.277 1.816 1.515 0.279 1.453 -0.435 -0.980 -0.975 1.101 -0.411 [41] -0.045 -2.560 -1.259 -0.377 0.406 1.116 -2.031 0.669 0.705 -1.023 [51] -1.466 1.458 -0.050 -0.398 -0.720 -0.141 1.225 0.185 1.328 -2.181 [61] -0.361 -0.957 0.054 0.639 -1.151 -1.619 0.771 0.222 1.141 -1.105 [71] -0.046 0.008 -0.902 1.876 -0.167 0.724 1.753 -0.535 0.254 -0.494 [81] -0.253 0.500 -0.630 2.147 0.930 1.141 1.076 -0.201 0.082 0.098 [91] -0.561 -1.067 1.548 -0.299 -0.148 0.723 -0.073 1.054 -0.858 -0.668 &gt; # PCA &gt; summary( prcomp( X ) ) Importance of components: PC1 PC2 Standard deviation 1.3529 0.41186 Proportion of Variance 0.9152 0.08481 Cumulative Proportion 0.9152 1.00000 &gt;   Gráfico interativo △   A figura 6.1 mostra 4 gráficos, com o primeiro mostrando apenas a dispersão dos pontos nas variáveis originais. No segundo, a reta em azul representa a direção da primeira componente principal capturando a maior variabilidade dos dados. Ou seja, está na direção onde há a maior dispersão da informação contida nos dados. A reta vermelha no terceiro gráfico indica a segunda componente principal, que é perpendicular à primeira e por sua vez é a direção que captura a maior parte da dispersão que a primeira componente não consegue representar. O quarto gráfico usa as duas primeiras componentes principais como sistema de coordenadas e assim é possível visualizar e entender que essa transformação consistiu na rotação do sistema dos eixos originais. É importante destacar que a relação entre os pontos permanece inalterada, independentemente do sistema de eixos utilizado como mostrado pela figura. Em outras palavras, a distância relativa entre os pontos não muda com a rotação dos eixos. Essa propriedade é fundamental no contexto do PCA, pois significa que, ao aplicar a transformação para encontrar as componentes principais, a estrutura dos dados em termos de proximidade e dispersão é preservada. Assim, mesmo após a rotação, as relações espaciais entre os pontos continuam a refletir a mesma informação sobre a variabilidade dos dados, permitindo que a análise se concentre nas direções de maior variância sem distorcer a interpretação dos dados originais. 6.4 Objetivo Após uma apresentação formal da definição de PCA, a pergunta que se levanta é: qual é a motivação para aplicar essa técnica aos dados? O ponto crucial para compreender suas aplicações reside intrinsecamente ligada na interpretação dos dados nas novas variáveis geradas. O objetivo central do PCA é permitir a redução a dimensionalidade de um conjunto de dados, preservando ao máximo a variabilidade e a informação contida neles. E como se dá essa redução? A chave está na interpretação: o pesquisador assume que, a partir de certo ponto de acúmulo de informação, o que excede pode ser considerado apenas resíduo ou erro estatístico inerente aos dados. Ou seja, parte da informação pode ser descartada sem comprometer a descrição que os dados propõem. Para ser mais claro, a técnica de PCA não reduz a dimensionalidade por si só necessariamente; é o pesquisador que, na maior parte das vezes, por meio de uma interpretação cuidadosa das componentes principais, que decide descartar as de ordem mais alta, levando à redução efetiva da dimensionalidade dos dados. A técnica de PCA reduz a dimensionalidade somente em situações de dependência linear entre variáveis. Quando uma variável pode ser expressa como uma combinação linear exata de outras, isso resulta em uma correlação perfeita, com coeficiente igual a 1. Nesses casos, o número de componentes principais gerados será menor que o menor valor entre o número de variáveis (\\(n\\)) e o número de observações (\\(m\\)), pois a redundância das informações é representada em uma única componente. Essa característica permite que o PCA retenha apenas as combinações de variáveis totalmente correlacionadas, simplificando a representação dos dados em termos de número de variáveis. Por exemplo didático, crie uma imagem mental da figura 6.1 e reflita sobre a aplicação da PCA nos dados \\(\\mathbf{x}_1 = \\{1;2;3\\}\\) e \\(\\mathbf{x}_2 = \\{1;2;3\\}\\). Nos casos em que a correlação entre as variáveis é inferior a 1, mas ainda assim altamente significativa, a técnica de PCA resulta em \\(k\\) componentes principais, sendo \\(k\\) igual ao menor valor entre o número de variáveis (\\(n\\)) e o número de observações (\\(m\\)). Nesses cenários, algumas das componentes principais podem explicar muito pouca variabilidade, tornando-se candidatas a serem descartadas. Isso acontece porque as informações redundantes são capturadas em uma componente e apenas a informação restante que é independente, é representada em outra componentes mas com baixa descrição de variabilidade. Dessa forma, essas componentes não terá peso substancial para a descrição da variabilidade dos dados. Por exemplo, construa uma imagem mental da figura 6.1 e pense na PCA dos dados \\(\\mathbf{x}_1 = \\{1;2;3\\}\\) e \\(\\mathbf{x}_2 = \\{1;2;3,3\\}\\). Para o caso em que as variáveis são inerentemente não correlacionadas, todas as componentes principais terão magnitudes semelhantes, tornando a aplicação do PCA desnecessária. Por exemplo, visualize a figura 6.1 e considere a PCA aplicada aos dados \\(\\mathbf{x}_1 = \\{1, 0\\}\\) e \\(\\mathbf{x}_2 = \\{0, 1\\}\\). Nessa situação, a falta de correlação entre as variáveis implica que a técnica não trará benefícios, já que as informações já estão distribuídas de forma independente. É nesses termos que se afirma sobre a possibilidade de redução de dimensionalidade: A PCA permite simplificar conjuntos de dados de alta dimensão, ou seja, eliminar colinearidades quando estas estiverem presentes. Assim, permite que analistas visualizem e interpretem as informações de forma mais clara. Ao reduzir a dimensionalidade dos dados, a técnica preserva a maior parte da variabilidade, facilitando a identificação de padrões (caso exista) e a tomada de decisões. Essa representação mais compacta dos dados torna a análise mais acessível e eficiente, podendo revelar tendências que potencialmente passariam despercebidos em um espaço dimensional maior. Aproveitando o exemplo já mostrado pela figura 6.1 com as variáveis \\(x_1\\) e \\(x_2\\), observamos que a correlação é \\(\\sim 83\\%\\). A primeira componente principal retém \\(\\sim 91,5\\%\\), da variância dos dados, enquanto a segunda componente retém apenas \\(\\sim 8,5\\%\\). Se considerarmos que uma retenção de variância acima de \\(90\\%\\) é satisfatória, podemos concluir que a segunda componente é dispensável. Isso nos permite simplificar os dados, mantendo a essência das informações. Ao descartar a segunda componente, assumimos que ela representa principalmente ruído. Dessa forma, estamos utilizando a análise de forma mais eficiente, interpretando os dados com o emprego da análise de PCA servindo como um filtro, destacando as características mais relevantes e eliminando variações indesejadas. Mais alguns Detalhes A consequência da redução de dimensionalidade em dados altamente correlacionados, como os obtidos em várias técnicas espectroscópicas, resulta em que a variância acumulada nas duas primeiras componentes principais geralmente representa uma parte significativa da variância total. Isso permite a criação de gráficos bidimensionais, que facilitam a exploração visual das componentes e se há identificação de padrões e tendências no comportamento dos dados. Essa ferramenta se torna muito útil para análises e interpretações. Outro aspecto está relacionado quanto a decisão que precisamos tomar de quantas componentes principais devemos manter. No entanto, não existe uma regra fixa ou um número mágico para isso. A razão é que a escolha do \\(k\\) depende de vários fatores, como o objetivo da análise e a natureza dos dados. Por exemplo, se queremos uma representação menos rigorosa dos dados, podemos optar por menos componentes. Mas se buscamos capturar a maior parte da variação nos dados, talvez precisemos de mais componentes. Além disso, cada conjunto de dados é único, então o que funciona bem para um conjunto pode não ser adequado para outro. É por isso que, em geral, utilizamos a variação explicada por cada componente, mostrada em gráfico, ou soma acumulada da variância para nos ajudar a decidir quantas componentes manter, mas no final, a escolha sempre envolve um certo grau de julgamento e análise do contexto. Outra característica intrínseca ao uso do PCA é que conseguir uma interpretação fácil e direta das componentes principais em termos físicos pode ser desafiador. Especialmente quando os dados têm muitas variáveis, a interpretação física é praticamente impossível e é melhor não tentar fazê-la. Isso acontece porque as componentes principais são combinações lineares das variáveis originais, e essas combinações podem não ter um significado físico claro. Em vez disso, é muito mais eficaz focar nas variáveis com maior influência nas componentes, e assim sendo, é possível obter alguma interpretação ao analisar os coeficientes (ou pesos) que cada variável tem nas componentes. Se uma variável tem um peso alto em uma componente principal, isso indica que ela é importante para essa nova dimensão. Assim, você pode olhar para essas variáveis mais influentes e tentar entender como elas se relacionam com a variação dos dados. Por fim, ter entendimento sobre PCA é importante também pois ela serve de base e suporte para outros métodos, como PCR (Regressão em Componentes Principais) e o método de classificação SIMCA (Soft Independent Modeling of Class Analogy). Além disso, a Análise Discriminante e vários métodos de agrupamento (clustering) podem ter um desempenho melhor quando aplicam a PCA para eliminar ruídos nos dados. Para qualquer outro método que enfrente desafios relacionados à multicolinearidade nas variáveis, a aplicação da PCA como etapa prévia se torna conveniente, pois permite eliminar redundâncias. Assim, a PCA atua como uma técnica de pré-processamento que aprimora a eficácia e a precisão desses métodos.   Resumindo: A PCA serve a vários propósitos no contexto da quimiometria e da análise multivariada de dados: Redução de Dimensionalidade: Ao simplificar conjuntos de dados de alta dimensão, a PCA permite que os analistas visualizem e interpretem os dados mais facilmente. Identificação de Padrões: A técnica ajuda a revelar padrões ocultos, tendências e estruturas nos dados, facilitando a identificação de grupos ou clusters de amostras. Eliminação de Redundância: A PCA pode ajudar a eliminar variáveis redundantes, concentrando-se nas variáveis que mais contribuem para a variabilidade dos dados. Pré-processamento para Modelagem: A PCA é frequentemente utilizada como um passo de pré-processamento antes de aplicar modelos de regressão, classificação ou outras análises multivariadas, melhorando a eficiência e a precisão dos resultados.   Recomendação para quando usar: A PCA é recomendada em várias situações dentro da quimiometria, especialmente quando: Dados de Alta Dimensionalidade: Quando se trabalha com conjuntos de dados que contêm um grande número de variáveis em relação ao número de amostras, a PCA é útil para reduzir a complexidade. Correlações Entre Variáveis: A PCA é especialmente eficaz quando há alta correlação entre as variáveis, pois a técnica ajuda a identificar as direções principais da variação nos dados. Visualização de Dados: Quando é necessário visualizar a estrutura dos dados, a PCA pode ser utilizada para criar gráficos bidimensionais ou tridimensionais que facilitam a interpretação e a comunicação dos resultados. Detecção de Outliers: A PCA pode ajudar a identificar pontos de dados que se afastam do padrão esperado, facilitando a detecção de outliers. Modelos Previstos: A PCA é útil como etapa inicial na modelagem, onde as variáveis transformadas em componentes principais podem ser usadas como entradas para modelos de regressão ou classificação, garantindo que a variabilidade significativa dos dados seja capturada. 6.5 PCA como Decomposição Matricial As combinações lineares das quais discutimos anteriormente podem ser reestruturadas em formato matricial, tornando a notação mais compacta. \\[ \\mathbf{T} = \\mathbf{X} \\cdot \\mathbf{P} \\]        onde: \\(\\mathbf{T}\\) é a matriz de scores \\(\\mathbf{X}\\) é a matriz de dados \\(\\mathbf{P}\\) é a matriz de loadings   E decorre, após manipulações, que PCA pode igualmente ser entendido e abordado como uma decomposição da matriz \\(\\mathbf{X}\\) nessas outras duas novas matrizes:   \\[ \\mathbf{X} = \\mathbf{T} \\cdot \\mathbf{P^T} \\]   Os scores referem-se às coordenadas das observações nos novos eixos criados pelos componentes principais. Em outras palavras, eles representam a projeção das observações originais na nova base de componentes principais. Cada score indica como uma observação se posiciona em relação aos novos eixos. Os loadings, por outro lado, representam a contribuição de cada variável original para os componentes principais. Eles indicam a correlação entre as variáveis originais e os componentes principais. Os loadings ajudam a entender como as variáveis influenciam a formação dos novos eixos e, consequentemente, os scores. Em geral, um loading alto (positivo ou negativo) para uma variável em um componente principal sugere que essa variável tem uma forte influência nesse componente.   Equivalência de Notação As duas formas são equivalentes, pois podemos monstrar que \\(\\mathbf{T} = \\mathbf{X} \\cdot \\mathbf{P}\\) pode ser expresso como \\(\\mathbf{X} = \\mathbf{T} \\cdot \\mathbf{P^T}\\). Só precisamos manipular a equação de forma a isolarmos \\(\\mathbf{X}\\). Começamos com a equação original: \\[ \\mathbf{T} = \\mathbf{X} \\cdot \\mathbf{P} \\] Multiplicamos ambos os lados pela inversa de \\(\\mathbf{P}\\) à direita. Como \\(\\mathbf{P}\\) é ortogonal, usamos a relação \\(\\mathbf{P^{-1}} = \\mathbf{P^T}\\): \\[ \\mathbf{T} \\cdot \\mathbf{P^{-1}} = ( \\mathbf{X} \\cdot \\mathbf{P} ) \\cdot \\mathbf{P^{-1}} \\] O lado direito simplifica-se como: \\[ \\mathbf{T} \\cdot \\mathbf{P^T} = \\mathbf{X} \\cdot \\mathbf{I} = \\mathbf{X} \\] Portanto, podemos reescrever a equação como: \\[ \\mathbf{X} = \\mathbf{T} \\cdot \\mathbf{P^T} \\] Assim, está demonstrado que, sob a condição de que \\(\\mathbf{P}\\) sendo uma matriz ortogonal, a relação \\(\\mathbf{T} = \\mathbf{X} \\cdot \\mathbf{P}\\) implica em \\(\\mathbf{X} = \\mathbf{T} \\cdot \\mathbf{P^T}\\) e que o ponto de vista sobre PCA ser uma decomposição matricial é factível. 6.5.1 Principais Algoritmos Adotando a perpectiva da decomposição matricial, conseguimos estabelecer uma conexão natural para explorar outras decomposições matriciais já existentes e suas interconexões metodológicas. Assim sendo, ao menos os métodos mais usuais utilizados para realizar a PCA aproveitam abordagens matemáticas distintas já consolidadas. Cada uma dessas abordagens oferece suas particularidades e aplicações específicas, proporcionando vantagens em diferentes contextos e tamanhos de conjuntos de dados. A seguir, apresentamos três formatos diferentes, cada um implementado por meio das seguintes abordagens: 6.5.1.1 Decomposição Espectral A Decomposição Espectral51,52 também conhecida como Decomposição em Autovetores e Autovalores53,54 decompõe uma matriz quadrada \\(\\mathbf{A}\\) de dimensão \\(m \\times m\\) na forma: \\[ \\mathbf{A} = \\mathbf{V} \\cdot \\mathbf{\\Sigma} \\cdot \\mathbf{V^{-1}} \\]        onde: \\(\\mathbf{A}\\) é a matriz de covariância55 de \\(\\mathbf{X}\\). \\(\\mathbf{V}\\) é uma matriz \\(m \\times m\\) cujas colunas são os autovetores. \\(\\mathbf{V^{-1}}\\) é a inversa de \\(\\mathbf{V}\\). \\(\\mathbf{\\Sigma}\\) é uma matriz diagonal de dimensão \\(m \\times m\\) contendo os autovalores correspondentes.   As matrizes \\(\\mathbf{P}\\) (loadings) e \\(\\mathbf{T}\\) (scores), da Análise de Componentes Principais (PCA), podem ser obtidas por:   \\[ \\mathbf{P} = \\mathbf{V} \\]   \\[ \\mathbf{T} = \\mathbf{X} \\cdot \\mathbf{V} \\]   Uma das características intrísecas a esse método de implementação de PCA é que exige cálculo explícito tanto de \\(\\mathbf{A} = \\mathbf{X^T} \\cdot \\mathbf{X}\\) quanto de \\(\\mathbf{T} = \\mathbf{X} \\cdot \\mathbf{V}\\) , que são custosas computacionalmente em conjuntos de dados muito grandes (caso das conhecidas ciências “ômicas” e da área de Big Data) devido ao número de operação. O que por sua vez, acarreta em maior propagação dos erros inerentes à representação numérica na computação em ponto flutuante56 quanto comparável à implementação a ser mostrada em sequência por SVD. Algoritmos que implementam essa decomposição não permitem dados faltantes. Caso exista na matriz de dados, o usuário deve tratar os dados perdidos previamente. No R, existe a função princomp que se utiliza justamente dessa forma de abordagem para PCA. Porém a leitura do manual da função declara que a mesma existe somente por compatibilidade com o software S-PLUS e que deve ser um método preterido. Para saber mais detalhes, digite : ?princomp no console do R. O leitor que se sentir satisfeito ao saber que, uma vez obtida a solução da Decomposição Espectral da matriz \\(\\mathbf{A}\\), a solução de PCA será automaticamente alcançada, pode pular os detalhes e prosseguir para o próximo algoritmo.    + Detalhes Uma Solução para a Decomposição Espectral Para realizar a Decomposição Espectral podemos seguir os seguintes passos: Encontrar os autovalores: Calcular o polinômio característico de \\(\\mathbf{A}\\), que é dado por \\(\\det(\\mathbf{A} - \\lambda \\mathbf{I}) = 0\\), onde \\(\\lambda\\) são os autovalores, \\(\\mathbf{I}\\) é a matriz identidade e \\(\\det()\\) é a função matricial determinante57. Resolver a equação para encontrar os autovalores \\(\\lambda_1, \\lambda_2, \\ldots, \\lambda_n\\). Encontrar os autovetores: Para cada autovalor \\(\\lambda_i\\), resolva o sistema \\((\\mathbf{A} - \\lambda_i \\mathbf{I})\\mathbf{v}_i = 0\\) para encontrar o autovetor correspondente \\(\\mathbf{v}_i\\). Repita isso para todos os autovalores. Formar as matrizes \\(\\mathbf{V}\\) e \\(\\mathbf{\\Sigma}\\): A matriz \\(\\mathbf{V}\\) é formada pelos autovetores como colunas: \\(\\mathbf{V} = [\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n]\\). A matriz \\(\\mathbf{\\Sigma}\\) é uma matriz diagonal cujos elementos na diagonal são os autovalores correspondentes: \\(\\mathbf{\\Sigma} = \\text{diag}(\\lambda_1, \\lambda_2, \\ldots, \\lambda_n)\\). Verificar a decomposição: Você pode verificar a decomposição calculando \\(\\mathbf{V} \\cdot \\mathbf{\\Sigma} \\cdot \\mathbf{V^{-1}}\\) e confirmando que resulta na matriz original \\(\\mathbf{A}\\). Observação: A decomposição espectral é aplicável a matrizes que são diagonalisáveis58   Resolução usando o Polinômio Característico Aqui, para o leitor que deseja acompanhar um exemplo de solução passo a passo, exploraremos com mais alguns detalhes o caso ilustrado na figura 6.1. Aqui, mais uma vez, faremos uso explícito do ChatGPT para demonstrar como utilizá-lo como ferramenta de suporte. Podemos iniciar com uma pergunta ao ChatGPT da seguinte forma:   Encontre os autovalores e autovetores para a matriz abaixo: &gt; cov(data) x1 x2 x1 1.0000000 0.8303746 x2 0.8303746 1.0000000 &gt; Use somente elementos de álgebra. Não use funções prontas do R.   A resposta fornecida pelo ChatGPT foi a seguinte:   Para encontrar os autovalores e autovetores da matriz de covariância \\[ A = \\begin{bmatrix} 1.0000000 &amp; 0.8303746 \\\\ 0.8303746 &amp; 1.0000000 \\end{bmatrix} \\] podemos proceder usando métodos da álgebra linear. O primeiro passo é encontrar os autovalores \\(\\lambda\\) resolvendo o polinômio característico59. Passo 1: Determinar o polinômio característico O polinômio característico é dado por: \\[ \\text{det}( \\mathbf{A} - \\mathbf{\\lambda} \\mathbf{I}) = 0 \\] onde \\(\\mathbf{I}\\) é a matriz identidade de mesma ordem que \\(\\mathbf{A}\\), e \\(\\mathbf{\\lambda}\\) são os autovalores. Assim, temos: \\[ \\text{det} \\left( \\begin{bmatrix} 1.0000000 &amp; 0.8303746 \\\\ 0.8303746 &amp; 1.0000000 \\end{bmatrix} - \\lambda \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix} \\right) = 0 \\] Isto se traduz na seguinte equação: \\[ \\text{det} \\left( \\begin{bmatrix} 1.0000000 - \\lambda &amp; 0.8303746 \\\\ 0.8303746 &amp; 1.0000000 - \\lambda \\end{bmatrix} \\right) = 0 \\] O determinante de uma matriz \\(2 \\times 2\\) é dado por: \\[ \\text{det}(\\mathbf{A}) = (a_{11} \\cdot a_{22}) - (a_{12} \\cdot a_{21}) \\] Aplicando isso à matriz \\(\\mathbf{A} - \\mathbf{\\lambda} \\mathbf{I}\\): \\[ (1.0000000 - \\lambda)(1.0000000 - \\lambda) - (0.8303746 \\cdot 0.8303746) = 0 \\] Simplificando: \\[ (1.0000000 - \\lambda)^2 - 0.8303746^2 = 0 \\] Expanda os termos: \\[ (1.0000000 - \\lambda)^2 = 1 - 2.0000000\\lambda + \\lambda^2 \\] \\[ 0.8303746^2 = 0.6895219 \\] Agora temos a equação: \\[ 1 - 2.0000000\\lambda + \\lambda^2 - 0.6895219 = 0 \\] Simplifique para chegar na forma canônica do polinômio característico: \\[ \\lambda^2 - 2.0000000\\lambda + 0.3104781 = 0 \\] Passo 2: Resolver a equação quadrática Agora, podemos resolver essa equação quadrática usando a fórmula de Bhaskara60: \\[ \\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} \\] Onde \\(a = 1\\), \\(b = -2.0000000\\) e \\(c = 0.3104781\\). Calculando o discriminante: \\[ \\Delta = (-2.0000000)^2 - 4(1)(0.3104781) \\] \\[ \\Delta = 4.0000000 - 1.2419124 = 2.7580876 \\] Agora, extraímos a raiz quadrada do discriminante: \\[ \\sqrt{\\Delta} = \\sqrt{2.7580876} \\approx 1.660211 \\] Agora, podemos calcular os autovalores: \\[ \\lambda_1 = \\frac{2.0000000 + 1.660211}{2} = \\frac{3.660211}{2} \\approx 1.830106 \\] \\[ \\lambda_2 = \\frac{2.0000000 - 1.660211}{2} = \\frac{0.339789}{2} \\approx 0.169894 \\] Passo 3: Encontrar os autovetores Agora que temos os autovalores \\(\\lambda_1 = 1.830106\\) e \\(\\lambda_2 = 0.169894\\), podemos encontrar os autovetores resolvendo o sistema \\((A - \\lambda I)v = 0\\) para cada autovalor. Para \\(\\lambda_1 = 1.830106\\): A equação \\(( \\mathbf{A} - \\lambda_1 \\mathbf{I})\\mathbf{v} = 0\\) é: \\[ \\begin{bmatrix} 1.0000000 - 1.830106 &amp; 0.8303746 \\\\ 0.8303746 &amp; 1.0000000 - 1.830106 \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} = 0 \\] Isso resulta no sistema: \\[ \\begin{bmatrix} -0.830106 &amp; 0.8303746 \\\\ 0.8303746 &amp; -0.830106 \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} = 0 \\] Resolvendo, obtemos uma relação entre \\(v_1\\) e \\(v_2\\). Para simplificação, o autovetor associado será proporcional a: \\[ v_1 \\approx v_2 \\] Para \\(\\lambda_2 = 0.169894\\): Similarmente, para \\(\\lambda_2\\): \\[ \\begin{bmatrix} 1.0000000 - 0.169894 &amp; 0.8303746 \\\\ 0.8303746 &amp; 1.0000000 - 0.169894 \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} = 0 \\] Isso resulta no sistema: \\[ \\begin{bmatrix} 0.830106 &amp; 0.8303746 \\\\ 0.8303746 &amp; 0.830106 \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} = 0 \\] Resolvendo, temos que: \\[ v_1 \\approx -v_2 \\] Resumo dos autovalores e autovetores (não normalizados): Autovalores: \\(\\lambda_1 \\approx 1.830106\\) e \\(\\lambda_2 \\approx 0.169894\\) Autovetores (não normalizados): Para \\(\\lambda_1\\): \\(v_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\) Para \\(\\lambda_2\\): \\(v_2 = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\\) Passo 4: Normalizar os autovetores Para normalizar os autovetores, devemos dividir cada autovetor pela sua norma61 (ou módulo). A norma de um vetor \\(\\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix}\\) é dada por: \\[ \\|\\mathbf{v}\\| = \\sqrt{v_1^2 + v_2^2} \\] Agora, vamos calcular a norma de cada um deles e dividi-los por essa norma. Para o autovetor \\(\\mathbf{v_1} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\): \\[ \\|\\mathbf{v_1}\\| = \\sqrt{1^2 + 1^2} = \\sqrt{2} \\] O autovetor normalizado é: \\[ \\mathbf{\\hat{v_1}} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{bmatrix} \\] Ou, de forma aproximada: \\[ \\mathbf{\\hat{v_1}} = \\begin{bmatrix} 0.707 \\\\ 0.707 \\end{bmatrix} \\] Para o autovetor \\(\\mathbf{v_2} = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\\): \\[ \\|\\mathbf{v_2}\\| = \\sqrt{1^2 + (-1)^2} = \\sqrt{2} \\] O autovetor normalizado é: \\[ \\mathbf{\\hat{v_2}} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{-1}{\\sqrt{2}} \\end{bmatrix} \\] Ou, de forma aproximada: \\[ \\mathbf{\\hat{v_2}} = \\begin{bmatrix} 0.707 \\\\ -0.707 \\end{bmatrix} \\] Resumo dos autovetores normalizados: Autovetor normalizado para \\(\\lambda_1 = 1.830106\\): \\[ \\mathbf{\\hat{v_1}} = \\begin{bmatrix} 0.707 \\\\ 0.707 \\end{bmatrix} \\] Autovetor normalizado para \\(\\lambda_2 = 0.169894\\): \\[ \\mathbf{\\hat{v_2}} = \\begin{bmatrix} 0.707 \\\\ -0.707 \\end{bmatrix} \\] Considerações Atenção: Não assuma que o resultado fornecido pelo ChatGPT esteja 100% correto! As respostas devem ser vistas como um ponto de partida ou guia de estudo. Embora possam oferecer informações valiosas, é comum que contenham erros ou imprecisões. Por isso, é essencial revisar e ajustar o conteúdo. Essa busca por refinamento se torna um ótimo exercício de aprendizado, permitindo que você desenvolva habilidades críticas e analíticas. Se o leitor acredita que a saída fornecida pelo ChatGPT não foi alterada, engana-se. Várias versões de perguntas e mudanças no texto foram feitas até esse resultado final. Dedique um tempo para analisar as informações recebidas, identificando inconsistências ou áreas que podem ser aprimoradas. Essa prática não só fortalecerá seu entendimento, mas também o ajudará a se tornar um leitor mais crítico. Dado o aconselhamento, realizaremos uma verificação dos resultados em comparação com os obtidos no R. Utilizando a função nativa eigen(), podemos calcular os autovalores e autovetores: &gt; eigen( cov( X ) ) eigen() decomposition $values [1] 1.8303746 0.1696254 $vectors [,1] [,2] [1,] 0.7071068 -0.7071068 [2,] 0.7071068 0.7071068 &gt; Aqui, podemos observar que nossa solução utilizando o polinômio característico está alinhada com o output fornecido pelo console do R, com uma pequena exceção! O segundo autovetor apresenta sinais invertidos, o que é uma característica inerente à Análise de Componentes Principais (PCA). Não é possível prever a direção exata dada pelos sinais gerados, mas, se necessário para um contexto específico, podemos multiplicar esse autovetor por -1 sem que isso altere a interpretação da análise. Quanto aos autovalores, os mesmos estão com plena concordânica a menos de pequeno erro de aproximações numéricas. A inversão dos sinais é corroborada pelo uso de outra função do R, a princomp. Veja abaixo: &gt; princomp( X )$loadings Loadings: Comp.1 Comp.2 x1 0.707 0.707 x2 0.707 -0.707 &gt; Por fim, devemos verificar se conseguimos restaurar a matriz de covariância original por meio da multiplicação das nossas matrizes de autovalores e autovetores. Mais uma vez, salvo por possíveis erros de aproximação numérica, alcançamos a conferência desejada. &gt; v1 &lt;- c( 0.707, 0.707 ) &gt; v2 &lt;- c( 0.707, -0.707 ) &gt; V &lt;- cbind( v1, v2 ) &gt; Sigma &lt;- diag( c( 1.8301, 0.169894 ) ) &gt; round( V %*% Sig %*% solve(V), 4 ) [,1] [,2] [1,] 1.0000 0.8301 [2,] 0.8301 1.0000 &gt; Como detalhe adicional, é importante ressaltar para os iniciantes que a abordagem adotada nesta resolução é puramente didática. O desenvolvimento de algoritmos numéricos e eficientes envolve técnicas complexas e avançadas, que vão muito além do escopo deste texto introdutório. O objetivo aqui é proporcionar uma compreensão básica dos conceitos, preparando o terreno para estudos mais aprofundados no futuro. Os algoritmos utilizados em softwares especializados são significativamente mais avançados e sofisticados. Eles são projetados para lidar com uma ampla variedade de situações, levando em consideração fatores como eficiência computacional e precisão dos resultados. △   6.5.1.2 Decomposição em Valores Singulares (SVD) A Decomposição em Valores Singulares (Singular Value Decomposition - SVD)62 é considerada uma generalização da Decomposição Espectral para casos gerais de matrizes retangulares. É uma técnica que fatoriza uma matriz \\(\\mathbf{X}\\) de dimensões \\(n \\times m\\) em três matrizes:   \\[ \\mathbf{X} = \\mathbf{U} \\cdot \\mathbf{\\Sigma} \\cdot \\mathbf{V^T} \\]        onde: \\(\\mathbf{U}\\) é uma matriz ortonormal\\(\\dagger\\) de dimensão \\(n \\times n\\). É a matriz de vetores singulares a esquerda. \\(\\mathbf{V}\\) é uma matriz ortonormal\\(\\dagger\\) de dimensão \\(m \\times m\\). É a matriz de vetores singulares a direita. \\(\\mathbf{\\Sigma}\\) é uma matriz de dimensão \\(n \\times m\\) onde os valores singulares estão efetivamente organizados em uma submatriz diagonal \\(r \\times r\\), onde \\(r\\) é o posto da matriz \\(\\mathbf{X}\\) (o número de valores singulares não nulos e não-negativos). O valor de \\(r\\) pode ser no máximo igual a \\(min(n,m)\\). Os valores singulares em \\(\\mathbf{\\Sigma}\\) indicam a importância das direções representadas pelas colunas de \\(\\mathbf{U}\\) e \\(\\mathbf{V}\\). \\(\\dagger\\)Nota: Uma matriz \\(\\mathbf{M}\\) ser ortonormal significa que suas colunas são vetores ortogonais entre si e têm norma unitária63. Isso garante que \\(\\mathbf{M^T} \\cdot \\mathbf{M} = \\mathbf{I}\\), onde \\(\\mathbf{I}\\) é a matriz identidade.   Na Análise de Componentes Principais (PCA), utiliza-se da decomposição da SVD pois as matrizes \\(\\mathbf{P}\\) (loadings) e \\(\\mathbf{T}\\) (scores) podem ser diretamente obtidas pelas relações:   \\[ \\mathbf{P} = \\mathbf{V} \\]   \\[ \\mathbf{T} = \\mathbf{U} \\cdot \\mathbf{\\Sigma} \\]   Nas implementações da decomposição SVD também não é possível ter dados faltantes. Caso exista na matriz de dados, o usuário deve tratar os dados perdidos previamente. No R, existe a função prcomp que implementa PCA via SVD. Deve ser o método de preferência pela melhor precisão numérica conforme relatada no manual e pelo que evidencia as equações. Consulte o manual da função pelo comando: ?prcomp.    + Detalhes   Relação entre as decomposições SVD e Espectral Conforme já mencionado, a decomposição SVD pode ser vista como uma generalização da Decomposição Espectral para matrizes retangulares. Para entender a SVD, podemos conceitualmente derivá-la a partir da Decomposição Espectral, aplicando-a de maneira adequada às matrizes que não são quadradas.   Defina a matriz de dados: Considere uma matriz \\(\\mathbf{X}\\) de tamanho \\(n \\times m\\). Calcule \\(\\mathbf{X^TX}\\) e \\(\\mathbf{XX^T}\\): Compute \\(\\mathbf{X^TX}\\) (matriz \\(m \\times m\\)) e \\(\\mathbf{XX^T}\\) (matriz \\(n \\times n\\)). Encontre os autovalores: Calcule os autovalores de \\(\\mathbf{X^TX}\\) e \\(\\mathbf{XX^T}\\). Os autovalores não negativos de \\(\\mathbf{X^TX}\\) são os quadrados dos valores singulares de \\(\\mathbf{X}\\). Calcule os autovetores: Encontre os autovetores correspondentes aos autovalores de: \\(\\mathbf{X^TX}\\) (que formarão a matriz \\(\\mathbf{V}\\)) e de; \\(\\mathbf{XX^T}\\) (que formarão a matriz \\(\\mathbf{U}\\)). Organize os valores singulares: Os valores singulares \\(\\sigma_i\\) são as raízes quadradas dos autovalores. Organize-os em ordem decrescente. Construa as matrizes \\(\\mathbf{U}\\), \\(\\mathbf{\\Sigma}\\) e \\(\\mathbf{V}\\): \\(\\mathbf{U}\\) é formado pelos autovetores de \\(\\mathbf{XX^T}\\). \\(\\mathbf{V}\\) é formado pelos autovetores de \\(\\mathbf{X^TX}\\). \\(\\mathbf{\\Sigma}\\) é uma matriz diagonal onde os valores singulares \\(\\sigma_i\\) estão dispostos na diagonal. Monte a decomposição: A decomposição SVD é dada por \\(\\mathbf{X} = \\mathbf{U \\Sigma V^T}\\). Verifique a decomposição: Multiplique as matrizes \\(\\mathbf{U}\\), \\(\\mathbf{\\Sigma}\\) e \\(\\mathbf{V^T}\\) para garantir que você retorna à matriz original \\(\\mathbf{X}\\).   Entretanto, essa relação deve ser vista apenas como uma ferramenta de compreensão, e não deve ser utilizada para a implementação numérica, especialmente em matrizes de grandes dimensões.   Verificação Numérica da Equivalência # Cria uma matriz de exemplo X set.seed( 42 ) n &lt;- 5 m &lt;- 8 X &lt;- round( matrix( rnorm( n*m ), n, m ), 2 ) # Calcula a SVD de X svd_decomp &lt;- La.svd( X ) # Calcula X^T X e XX^T XtX &lt;- crossprod( X ) XXt &lt;- tcrossprod( X ) # Matriz U via eigen() U &lt;- eigen(XXt)$vectors U &lt;- abs( U ) * sign( svd_decomp$u ) # Matriz V via eigen() V &lt;- eigen(XtX)$vectors V[, 1:n ] &lt;- abs( V[, 1:n ] ) * sign( t( svd_decomp$vt ) ) # Matriz Sigma via eigen() Sigma &lt;- matrix( 0, n, m ) diag(Sigma) &lt;- sqrt(round(eigen(XtX)$values,12))[ 1:n ] # Compara ( U %*% Sigma %*% t(V) ) com X all.equal( ( U %*% Sigma %*% t(V) ), X ) ## [1] TRUE Desde que o resultado final seja TRUE, fica também demonstrado numericamente a equivalência entre a Decomposição Espectral e a Decomposição SVD. △   6.5.1.3 Algoritmo NIPALS O algoritmo NIPALS, Non-linear Iterative Partial Least Squares, Algoritmo nipals &lt;- function(x, npc = 2, tol = 1e-10, maxiter = 500, verbose=TRUE) { # Inicializa as matrizes de loadings e scores T &lt;- matrix(0, nrow = nrow(x), ncol = npc) P &lt;- matrix(0, nrow = ncol(x), ncol = npc) g &lt;- numeric(npc) for (iter in 1:npc) { # Inicializa variáveis para o loop interno d &lt;- 10 * tol niter &lt;- 0 # Escolhe a coluna com maior soma de quadrados como o primeiro t t &lt;- x[, which.max(colSums(x^2)), drop = FALSE] while (d &gt; tol &amp;&amp; niter &lt; maxiter) { # Calcula os loadings p &lt;- crossprod(x, t) / sum(t^2) p &lt;- p / norm(p, &#39;F&#39;) # Normaliza os loadings t_old &lt;- t t &lt;- x %*% p # Atualiza os scores # Calcula a convergência d &lt;- sum( abs( t_old - t ) ) # Incrementa o contador de iterações niter &lt;- niter + 1 if( verbose ){ # Imprima info cat(&quot;niter =&quot;, niter, &quot;em pc:&quot;, iter, &quot;\\n&quot;) flush.console() } } # Atualiza X e armazena os resultados x &lt;- x - tcrossprod(t, p) T[, iter] &lt;- t P[, iter] &lt;- p g[iter] &lt;- niter } # Retorna os resultados em uma lista return(list( T = T, P = P, VarExpl = colSums(T^2) / sum(x^2), g = g, tol = tol, maxiter = maxiter )) } Teste i &lt;- 7000 j &lt;- 150 X &lt;- matrix(rnorm( i*j), nrow = i, ncol = j) X &lt;- scale( X ) res &lt;- nipals1( X, npc=5, verbose=F, maxiter=2000 ) pca&lt;-prcomp(X) head( pca$x[,1:5] ) head( res$T )         O algoritmo NIPALS permite extrair componentes principais sem necessidade de imputar valores ausentes previamente (desde de que devidamente implementado) sendo uma vantagem significativa em relação a outros métodos que não lidam bem com dados faltantes. Como desvantagem, é conhecido que o algoritmo tem lenta convergência. No R, não há função nativa mas há o método em alguns pacotes. Em especial, vale mencionar o pacote nipals que implementa o algoritmo com opção padrão da execução da ortogonalização de Gram-Schmidt em cada iteração. Isso é necessário pois, devido ao acúmulo de erros de ponto flutuante, a ortogonalidade dos componentes principais é rapidamente perdida à medida que o número de componentes aumenta. A versão modificada do NIPALS, NIPALS-GS64, com o processo de Gram-Schmidt, re-ortogonaliza tanto os scores quanto os loadings de forma a estabilizá-los. Com a versão tradicional do NIPALS apenas as primeiras componentes devem ser consideradas e de matrizes não muito grandes.   PCoA - Principal Coordinates Analysis   PCA e PCoA se equivalem quando a distância a ser preservada é a euclidiana. Atualmente, o PCoA é mais comumente conhecido como Escalonamento Multidimensional Clássico65 (Classical Multidimensional Scaling)   TODO (texto a ser desenvolvido)   6.6 Variância por PC       Considerando o exemplo ilustrado na figura 6.1 &gt; &gt; 1.830106 / ( 1.830106 + 0.169894 ) [1] 0.915053 &gt; 0.169894 / ( 1.830106 + 0.169894 ) [1] 0.084947 &gt;       https://pt.wikipedia.org/wiki/Matrizes_semelhantes 6.7 Aplicação Típica           Algoritmo para SVD O algoritmo apresentado abaixo é apenas uma possibilidade dentre vários O método de Jacobi é uma abordagem iterativa que pode ser aplicada à SVD, embora não seja a mais eficiente para matrizes grandes. É apresentado para incentivar o conhecimento de se ter ao menos uma alternativa de resolução do problema com apenas conceitos básicos de álgebra. Ou seja, sem o uso de funções prontas e abordagem caixa-preta. # Função para calcular SVD usando o método de Jacobi jacobi_svd &lt;- function(A, tol = 1e-10, max_iter = 100) { # Inicialização n &lt;- nrow(A) m &lt;- ncol(A) U &lt;- diag(n) V &lt;- diag(m) # Calcular A^T A ATA &lt;- t(A) %*% A # Iterações de Jacobi for (iter in 1:max_iter) { # Encontrar o maior elemento fora da diagonal off_diag &lt;- which(abs(ATA) &gt; tol, arr.ind = TRUE) off_diag &lt;- off_diag[off_diag[, 1] != off_diag[, 2], ] if (nrow(off_diag) == 0) { break # Convergido } # Selecionar o primeiro elemento fora da diagonal p &lt;- off_diag[1, 1] q &lt;- off_diag[1, 2] # Cálculo dos ângulos de rotação if (ATA[p, p] == ATA[q, q]) { theta &lt;- pi / 4 } else { theta &lt;- atan(2 * ATA[p, q] / (ATA[p, p] - ATA[q, q])) / 2 } # Matriz de rotação c &lt;- cos(theta) s &lt;- sin(theta) # Atualizar ATA R &lt;- diag(n) R[c(p, q), c(p, q)] &lt;- matrix(c(c, -s, s, c), nrow = 2) ATA &lt;- t(R) %*% ATA %*% R V &lt;- V %*% R # Calcular os autovalores if (all(abs(ATA[off_diag]) &lt; tol)) { break } } # Valores singulares singular_values &lt;- sqrt(diag(ATA)) # Calcular U for (i in 1:n) { if (singular_values[i] &gt; 0) { U[, i] &lt;- A %*% V[, i] / singular_values[i] } } # Retornar os resultados return(list(U = U, D = diag(singular_values), V = V)) } # Matriz exemplo A &lt;- matrix(c(4, 0, 3, -5), nrow = 2, byrow = TRUE) # Chamar a função result &lt;- jacobi_svd(A) # Resultados cat(&quot;Matriz U:\\n&quot;) ## Matriz U: print(result$U) ## [,1] [,2] ## [1,] 0.8944272 -0.4472136 ## [2,] -0.4472136 -0.8944272 cat(&quot;\\nMatriz D (valores singulares):\\n&quot;) ## ## Matriz D (valores singulares): print(result$D) ## [,1] [,2] ## [1,] 3.162278 0.000000 ## [2,] 0.000000 6.324555 cat(&quot;\\nMatriz V:\\n&quot;) ## ## Matriz V: print(result$V) ## [,1] [,2] ## [1,] 0.7071068 -0.7071068 ## [2,] 0.7071068 0.7071068 Algoritmos para SVD Existem vários algoritmos para calcular a SVD, incluindo: Método de Jacobi: Uma abordagem iterativa, mas geralmente mais lenta para matrizes grandes. Algoritmo de Golub-Reinsch: Um método mais eficiente que combina a decomposição QR e o método de Jacobi. Algoritmo de Lanczos: Usado para matrizes grandes e esparsas; baseia-se em métodos de subespaço. SVD baseado em bidiagonalização: Usa decomposições bidiagonais, seguido por uma SVD da matriz bidiagonal. Algoritmos de fatoração de matrizes de baixa-rank: Técnicas como o método de potência e métodos estocásticos. Eficiência dos Algoritmos O algoritmo de Golub-Reinsch é um dos mais utilizados na prática e considerado eficiente para a maioria das aplicações. Para matrizes muito grandes ou esparsas, métodos baseados em Lanczos ou bidiagonalização tendem a ser mais eficientes. A escolha do algoritmo SVD mais eficiente depende do tamanho e da estrutura da matriz que você está tratando, bem como das necessidades específicas da aplicação. Em bibliotecas computacionais otimizadas, tais como BLAS66, LAPACK67, MKL68 em geral as funções implementadas trazem esses diversos algoritmos e alguma análise inicial da matriz determina o algoritmo mais adequado para a situação específica. △ https://pt.wikipedia.org/wiki/An%C3%A1lise_de_componentes_principais↩︎ https://pt.wikipedia.org/wiki/Karl_Pearson↩︎ Pearson, K. (1901). LIII. On lines and planes of closest fit to systems of points in space, The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 2(11), 559–572. https://doi.org/10.1080/14786440109462720↩︎ https://pt.wikipedia.org/wiki/Harold_Hotelling↩︎ Hotelling, H. (1933). Analysis of a complex of statistical variables into principal components, Journal of Educational Psychology, 24(6), 417–441. https://doi.org/10.1037/h0071325↩︎ https://pt.wikipedia.org/wiki/Teorema_espectral↩︎ https://en.wikipedia.org/wiki/Spectral_theorem↩︎ https://pt.wikipedia.org/wiki/Autovalores_e_autovetores↩︎ https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors↩︎ https://pt.wikipedia.org/wiki/Covari%C3%A2ncia↩︎ https://pt.wikipedia.org/wiki/V%C3%ADrgula_flutuante↩︎ https://pt.wikipedia.org/wiki/Determinante↩︎ https://pt.wikipedia.org/wiki/Matriz_diagonaliz%C3%A1vel↩︎ https://pt.wikipedia.org/wiki/Polin%C3%B4mio_caracter%C3%ADstico↩︎ https://pt.wikipedia.org/wiki/F%C3%B3rmula_quadr%C3%A1tica↩︎ https://pt.wikipedia.org/wiki/Norma_(matem%C3%A1tica)↩︎ https://pt.wikipedia.org/wiki/Decomposi%C3%A7%C3%A3o_em_valores_singulares↩︎ https://pt.wikipedia.org/wiki/Ortonormalidade↩︎ M. Andrecut, Parallel GPU Implementation of Iterative PCA Algorithms, Journal of Computational Biology, 2009 16:11, 1593-1599, https://www.liebertpub.com/doi/10.1089/cmb.2008.0221↩︎ https://en.wikipedia.org/wiki/Multidimensional_scaling↩︎ https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms↩︎ https://en.wikipedia.org/wiki/LAPACK↩︎ https://en.wikipedia.org/wiki/Math_Kernel_Library↩︎ "],["hca---análise-de-agrupamento-hierárquico.html", "&#8474;&#8477; 7 HCA - Análise de Agrupamento Hierárquico", " &#8474;&#8477; 7 HCA - Análise de Agrupamento Hierárquico x x x x https://en.wikipedia.org/wiki/Robert_Tryon https://en.wikipedia.org/wiki/Cluster_analysis https://en.wikipedia.org/wiki/Hierarchical_clustering https://pt.wikipedia.org/wiki/Agrupamento_hier%C3%A1rquico "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
